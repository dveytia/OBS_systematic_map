---
title: "Remove-duplicate-dois"
author: "Devi Veytia"
date: "2023-07-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
```


# INTRODUCTION

This file will compile all the relevant tables in a clean and logical way into a new sqlite file. For each table, a description will be provided. Note that where column names are repeated the descriptions for the columns will not be repeated. 

```{r format new sql database}
new_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                             here::here("data/raw-data/sql-databases/all_tables_v2.sqlite"), create=TRUE)
old_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                             here::here("data/raw-data/sql-databases/all_tables_v1.sqlite"), create=FALSE)
```

# TABLES DESCRIBING THE CORPUS

```{r allrefs -- Table for all references (before de-duplication)}

tblNameOld <- "allrefs_join" # table name in the old database
tblNameNew <- "allrefs" # table name in the new database (can be the same)
df <- tbl(old_db,tblNameOld)%>% collect()
df <- df %>%
  relocate(duplicate_id, .after = analysis_id)
dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)
```

*Description*
This table compiles all the references retrieved from the search strategy, along with automatically exported metadata, from both Web of Science and Scopus. Note that the source database is indicated in the "source_database" column. Duplicates can appear with and between databases. A unique "analysis_id" is assigned to each row (reference). 

*Column descriptions*
analysis_id : The unique record identifier for each unit of publication (i.e. when there are duplicates and before unique references are extracted, analysis_id > duplicate_id).

duplicate_id : publications identified as duplicates by title and year matching will receive the same duplicate_id

source_database : the citation indexed database the record was retrieved from. If the record had duplicates identified from 
multiple databases, these will be separated by " | " (e.g. "Web Of Science | Scopus")

search_substring : the search string used to retrieve the record. If multiple search strings returned the same record, numerous entries will be separated by " | "

database_article_id: the unique identifer given to the reference by its native source database

type: the type of publication

author: the authors of the publication

title : the text for the title of the publication

source_title : title of the source of publication (e.g. journal, book)

year: the year the publication was published

volume: the volumne of the journal (if applicable)

number: the number of the issue (if applicable)

pages: which pages in the issue the publication appeared in

abstract : the abstract of the publication

keywords : author tagged keywords

keywords_other : other keywords tagged by the database

publisher: the publisher of the publication

editor: the editor of the publication

language: the language of publication

affiliation: the affiliation (according to the protocol of the native citation indexed database, usually first author affiliation)

funding: any funding information

doi: the Digital Object Identifier

issn: the issn

isbn: the isbn

journal: the name of the publishing journal (if applicable)

research_areas: The research areas attributed to the publishing journal according to the protocol of the source database

web_of_science_categories: another form of research area classification from WOS

booktitle: if applicable

book_group_author: if applicable

book_author: if applicable

organization: The organization publishing the publication

series: series of publication (if applicable)

da: date of publication



```{r uniquerefs -- Table for all the unique references}
tblNameOld <- "uniquerefs"
tblNameNew <- "uniquerefs"
df <- tbl(old_db,tblNameOld)%>% collect()
df <- df %>%
  filter(!is.na(abstract))
dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)
```

*Description*
This table contains only unique references that have an abstract that is not NA. Note that the function "extract_unique_references" from the R revtools package was used to decide which record to retain when duplication was identified. In this case, the most complete record was retained where duplicates were identified. Therefore in this dataset, the duplicates are removed, thus each row represents a unique publication.

*Column description*
Columns remain the same as for the "allrefs" table


# TABLES DESCRIBING 'SEEN' ARTICLES 

```{r seen_screen -- Table for the reviewer screening choices}
tblNameOld <- "all-screen-results_screenExcl-codeIncl" 
tblNameNew <- "seen_screen"
df <- tbl(old_db,tblNameOld)%>% collect()
df <- df %>%
  select(-c(sysrev_id))
dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)

```

*Description*
This table provides the screening decisions for the manually screened subset of the corpus. Note that this subset contains articles that were sampled semi-randomly as well as non-randomly (for more information see the description on the sample_screen column). The include_screen column giving the include/exclude (i.e. 1/0 respectively) decisions was used to train the relevance predictions from the machine learning model.

Note here that the number of articles that are classified as "test_list" under the "sample_screen" variable totals 87 as opposed to the 94 reported in the protocol. This is because after screening, some test list articles were determined to be irrelevant after revisions made to the eligibility criteria during reviewer training.

*Column descriptions*
reviewer: The name of the primary reviewer to screen the article

reviewer_2: The name of the second reviewer if a double blind screen was done.

sample_screen: how the article was sampled in order to be screened. Some articles were test list articles, some were sampled semi-randomly for screening (by randomly sampling 1000 articles from each sub-string and then pooling the results), some were screened with priority according to a high predicted relevance as part of our active learning approach, and others were added after the coding stage in order to supplement article types that were under-sampled. 

include_screen: whether the decision was made to include (1) or exclude (0) the article


```{r seen_coding -- Table for the reviewer coding choices}
tblNameOld <- "allCodingSimplifiedVariablesMore"
tblNameNew <- "seen_coding"
df <- tbl(old_db,tblNameOld)%>% collect()
df <- df %>%
  select(-c(sysrev_id))
dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)
```

*Description*
This table provides all the reviewer coding decisions used to fit the mode. Each column represents a unique variable and label combination, and the reviewer decision is coded as 1 for being relevant and 0 for irrelevant. These decisions were used to train the relevance predictions from the machine learning model for the different metadata variables.

*Column descriptions*
coder_1: the name of the reviewer who coded the article

coder_2: the name of the second reviewer who coded the article if the article was flagged for double coding.

NB the rest of the column names following the coder_2 column represent the different metadata variables that were coded in the coding sheet. For descriptions of the variables and labels for each variable, please see the coding instructions file. The column names are in the following format: variable_name.label_name if multiple labels are used, or variable_name if just a binary 1/0 label. For example, for the variable "climate_threat" and the label "Extreme_weather" the corresponding column is named "climate_threat.Extreme_weather". Also note that the variable "ORO_type" is decomposed into two variables with increasing granularity: oro_branch which refers to whether the ORO aims to address mitigation (Mitigation), natural resilience (Nature) or societal adaptation (Societal) objectives, and oro_any which refers specifically to which type of OROs are mentioned in the article. For all multi-label variables, multiple choice is possible. 



# TABLES DESCRIBING MODEL PREDICTIONS 


```{r get unique ids to make sure that no duplicate ids get in}
dedups <- tbl(old_db, "uniquerefs") %>% select(analysis_id, abstract) %>% collect()
keepIDs <- unique(dedups$analysis_id[!is.na(dedups$abstract)])
rm(dedups)

```

```{r pred_relevance -- Table for relevance (i.e. screening)}

tblNameOld <- "predRel2"
tblNameNew <- "pred_relevance"
df <- tbl(old_db,tblNameOld) %>% collect() 
df <- df %>%
  filter(analysis_id %in% keepIDs) # this doesn't change nrow which is good

dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)

```



After realizing that uniquerefs has doi duplicates, remove from all csvs then write to sql -- might not need to do this, somehow the numbers work out. It is because the duplicated dois were from the abstracts that were NA.

```{r Loop through and write all the coding predictions to tables}
varnames <- c(
  "adapt_to_threat_simplified2",
  "blue_carbon",
  "climate_mitigation",
  "climate_threat_simplified",
  "data",
  "ecosystem_type_simplified2",
  "marine_system",
  "method_type_simplified",
  "oro_any_mitigation", "oro_any_nature", "oro_any_societal",
  "oro_branch"
)

# Directory to find the variable prediction csvs in 
predictionsDir <- here::here("data/raw-data/coding-predictions")
predictionsFiles <- dir(predictionsDir)


## Loop through and write all the csvs to the database
for(i in 1:length(varnames)){
  predFilename <- predictionsFiles[grep(varnames[i],predictionsFiles)]
  if(length(predFilename) != 1){
    print("error: length of file matches != 1")
    break
  }
  
  df <- readr::read_csv(file.path(predictionsDir, predFilename),show_col_types = FALSE)
  df <- df %>%
    rename(analysis_id = id)%>%
    filter(analysis_id %in% keepIDs)
  
  # File naming
  varnameSimple <- gsub("\\_simplified.*","",varnames[i]) # remove any suffix to simplfiy the variable name
  tblNameNew <- paste0("pred_", varnameSimple)
  
  # Write to database
  dbWriteTable(conn=new_db, 
             name=tblNameNew, value=df, append=FALSE, overwrite = FALSE)
}

src_dbi(new_db)
```



```{r disconnect databases}
DBI::dbDisconnect(new_db)
DBI::dbDisconnect(old_db)
```


