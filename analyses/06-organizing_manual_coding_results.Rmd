---
title: "06-organizing_manual_coding_results"
author: "Devi Veytia"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```

Note -- need to re-compile screening results to reflect additional exclusions from coding.


# Read in codebooks and format for distilBert

```{r Source formatting function}

source(here::here("R","formatCoding2distilBert.R"))

# note that this function does not combine rows -- each row of output = each row of input

```

## Combine all answers from set-0

```{r combine set-0 into most common answer}

## Read in files
set0Dir <- here::here("data","derived-data","coding","manual-coding","set-0_check")
set0Files <- dir(set0Dir)
set0Files <- set0Files[grepl(".csv", set0Files)] # only include csvs because macro files don't read in all the multiple options
set0Files <- set0Files[!grepl("metadata", set0Files)] # remove metadata file
set0Files <- set0Files[!grepl("combined", set0Files)] # remove file if already formatted


## Using metadata, format the dataframe to fill
metadataTemplate <- readr::read_csv(file.path(set0Dir, "set-0-screening-metadata.csv"))
metadataTemplate <- subset(metadataTemplate, select = c(sysrev_id, title, year))
metadataTemplate$sysrev_id <- as.character(metadataTemplate$sysrev_id)

## Format into binary responses for each variable x value combination
# bind into an array where each slice is a reviewer's response
for(f in 1:length(set0Files)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(set0Dir, set0Files[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = TRUE)
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$sysrev_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "sysrev_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("sysrev_id","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="sysrev_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("title","year","notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)
  # merge with template so formatting is the same
  df$data <- merge.data.frame(metadataTemplate, df$data, by="sysrev_id")
  
  # make sure no values greater 
 
  # save
  if(f==1){
    variables <- df$variables
    responseArray <- df$data
  }else{
    responseArray <- abind::abind(responseArray, df$data, along=3)
  }
  
}



# # save
# save(responseArray, file=file.path(set0Dir, "responseArray.RData"))
```

```{r calculate kappa statistic}
load(file.path(set0Dir, "responseArray.RData"))

# calculate
kappaStat <- apply(responseArray[,-c(1:5),], 2, irr::kappam.fleiss)

# load
load(file.path(set0Dir,"kappaStatistic.RData"))

# summarise
kappaStatSummary <- unlist(lapply(kappaStatSummary, function(x) x$value))
hist(kappaStatSummary)
```


```{r combine set 0 into the most common answer}
load(file.path(set0Dir, "responseArray.RData"))

# function to find most frequently appearing value
fun <- function(x){
  as.numeric(names(which.max(table(x))))
}


responseDf <- apply(responseArray[,-c(1:5),], 1:2, FUN=fun) # condense to most frequently appearing value
responseDf <- cbind(responseArray[,c(1:5),1], responseDf) # bind back in ID columns


# change name of coder
responseDf[,"coder_1"] <- rep("all_combined", nrow(responseDf))


# # save
# write.csv(responseDf, file.path(set0Dir,"codebook_combined-formatted_set-0.csv"),row.names = FALSE)

```


## Format single coder responses

```{r format into binary columns for predictions in distilBERT}

## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","sets")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    codebookSheet="Codebook",
    skipLines=3,returnVariableString = returnVariableString)
  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

View(responseDf)

```


## Combine all responses together

```{r read in combined answers from set-0}
set0Responses <- readr::read_csv(file.path(set0Dir,"codebook_combined-formatted_set-0.csv"), show_col_types = FALSE)
```


```{r Join with abstract by using sysrev_id and export}
# metadata from screening
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))

screen_results_merged_sub <- screen_results_merged %>% 
  select(sysrev_id, abstract) %>%
  mutate(sysrev_id = as.character(sysrev_id)) 
rm(screen_results_merged)

# merge in with coding results
responseDf <- merge.data.frame(responseDf, screen_results_merged_sub, by="sysrev_id", all.x = TRUE, all.y=FALSE)


## Write
write.table(
  responseDf,
  file = file.path(codingDir, "manual-coding-format-distilBERT.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```

# Evaluate if supplemental coding is needed 

Visualize frequencies to ensure balanced samples of most important variables: ORO_type, ORO_development_stage, Climate_threat, Impact_nature,  Restoration, Safe_fish, Safe_space


```{r}
# vector of variable names to tabulate
variables_to_tab <- c("ORO_type", "ORO_development_stage", "Climate_threat", "climate_mitigation", "adapt_to_threat", "Impact_nature",  "Restoration")


# index of which columns belong to which variables
variableInd <- unlist(lapply(strsplit(colnames(responseDf), "[.]"), function(x) x[[1]]))

# change to all variables
variables_to_tab <- unique(variableInd)
  
# empty list to fill with tabulated tables (one item for each variable)
tabulated_variables <- list()

for(v in 1:length(variables_to_tab)){
  colsInd <- grep(variables_to_tab[v], variableInd, ignore.case = TRUE)
  
  if(is.character(responseDf[,colsInd])){next} # can't tabulate character vectors
  
  if(!is.null(dim(responseDf[,colsInd]))){
    tab <- colSums(responseDf[,colsInd])
    names(tab) <- unlist(lapply(strsplit(colnames(responseDf)[colsInd], "[.]"), function(x) x[[2]]))
  }else{
    tab <- tabulate(responseDf[,colsInd])
  }
  tabulated_variables[[v]] <- tab
  names(tabulated_variables)[v] <- variables_to_tab[v]
}


sink(here::here("outputs","coding","tabulated_coding_round1.txt"))
print(tabulated_variables)
sink()

```


From this, yes I need to supplement the following categories:

1. oro_type (we need more articles on M.Increase efficiency and N.Human assisted evolution)
2. climate_threat (need at least more articles on temperature -- we could group the others into the other category)
3. adapt_to_threat = Natural 


# Find keywords to search for more needed article categories

```{r}
library(litsearchr)
library(dplyr)


# settings for keyword search
vars <- c("climate_threat.Temperature",
          "oro_type.M_Increase_efficiency",
          "oro_type.N_Human_assisted_evolution",
          "adapt_to_threat.Natural")

all_stopwords <- get_stopwords("English")


## loop through to get keywords

for(v in 1:length(vars)){
  ind <- which(colnames(responseDf) == vars[v])
  temp <- responseDf[which(responseDf[,ind] == 1),]
  
  View(temp[,c("title","abstract")])
  
  minFreq <- ceiling(0.1*nrow(temp)) # at least certain % of articles must include this word
  minN <- 2 # minimum number of words in a keyword
  
  # rake keywords from text
  rakedkeywords <- 
  litsearchr::extract_terms(
    text = paste(temp$title, temp$abstract),
    method = "fakerake",
    min_freq = minFreq,
    ngrams = TRUE,
    min_n = minN,
    language = "English",
    stopwords = all_stopwords
  )
  length(rakedkeywords)
  
  # Create a co-occurrence network
  naivedfm <-
    litsearchr::create_dfm(
      elements = paste(temp$title, temp$abstract),
      features = rakedkeywords
    )
  
  naivegraph <-
    litsearchr::create_network(
      search_dfm = naivedfm,
      min_studies = 2,
      min_occ = 2
    )
  
  
  # Calculate strength for each keyword
  strengths <- igraph::strength(naivegraph)
  
  require(dplyr)
  term_strengths <- data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
    mutate(rank=rank(strength, ties.method="min")) %>%
    arrange(strength)
  
  # finds the cutoff that retains 80% of the total strength
  cutoff <-
    litsearchr::find_cutoff(
      naivegraph,
      method = "cumulative",
      percent = .80,
      imp_method = "strength"
    )
  
  View(term_strengths)
  

}

```

Keywords are saved in coding/keyword-matches-supplemental-coding

# Visualise correlations?




