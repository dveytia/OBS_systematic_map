---
title: "06-organizing_manual_coding_results"
author: "Devi Veytia"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```



# Read in codebooks and format for distilBert

```{r format into binary columns for predictions in distilBERT}
## Source formatting function
source(here::here("R","formatCoding2distilBert.R"))

## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","set-1-to-18")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    codebookSheet="Codebook",
    skipLines=3,returnVariableString = returnVariableString)
  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

View(responseDf)

```



```{r Join with abstract by using sysrev_id and export}
# metadata from screening
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))

screen_results_merged_sub <- screen_results_merged %>% 
  select(sysrev_id, abstract) %>%
  mutate(sysrev_id = as.character(sysrev_id)) 
rm(screen_results_merged)

# merge in with coding results
responseDf <- merge.data.frame(responseDf, screen_results_merged_sub, by="sysrev_id", all.x = TRUE, all.y=FALSE)


## Write
write.table(
  responseDf,
  file = file.path(codingDir, "manual-coding-format-distilBERT.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```

# Evaluate if supplemental coding is needed 

Visualize frequencies to ensure balanced samples of most important variables: ORO_type, ORO_development_stage, Climate_threat, Impact_nature,  Restoration, Safe_fish, Safe_space


```{r}
# vector of variable names to tabulate
variables_to_tab <- c("ORO_type", "ORO_development_stage", "Climate_threat", "climate_mitigation", "adapt_to_threat", "Impact_nature",  "Restoration")


# index of which columns belong to which variables
variableInd <- unlist(lapply(strsplit(colnames(responseDf), "[.]"), function(x) x[[1]]))

# change to all variables
variables_to_tab <- unique(variableInd)
  
# empty list to fill with tabulated tables (one item for each variable)
tabulated_variables <- list()

for(v in 1:length(variables_to_tab)){
  colsInd <- grep(variables_to_tab[v], variableInd, ignore.case = TRUE)
  
  if(is.character(responseDf[,colsInd])){next} # can't tabulate character vectors
  
  if(!is.null(dim(responseDf[,colsInd]))){
    tab <- colSums(responseDf[,colsInd])
    names(tab) <- unlist(lapply(strsplit(colnames(responseDf)[colsInd], "[.]"), function(x) x[[2]]))
  }else{
    tab <- tabulate(responseDf[,colsInd])
  }
  tabulated_variables[[v]] <- tab
  names(tabulated_variables)[v] <- variables_to_tab[v]
}


sink(here::here("outputs","coding","tabulated_coding_round1.txt"))
print(tabulated_variables)
sink()

```


From this, yes I need to supplement the following categories:

1. oro_type (we need more articles on M.Increase efficiency and N.Human assisted evolution)
2. climate_threat (need at least more articles on temperature -- we could group the others into the other category)
3. adapt_to_threat = Natural 


# Find keywords to search for more needed article categories

```{r}
library(litsearchr)
library(dplyr)


# settings for keyword search
vars <- c("climate_threat.Temperature",
          "oro_type.M_Increase_efficiency",
          "oro_type.N_Human_assisted_evolution",
          "adapt_to_threat.Natural")

all_stopwords <- get_stopwords("English")


## loop through to get keywords

for(v in 1:length(vars)){
  ind <- which(colnames(responseDf) == vars[v])
  temp <- responseDf[which(responseDf[,ind] == 1),]
  
  View(temp[,c("title","abstract")])
  
  minFreq <- ceiling(0.1*nrow(temp)) # at least certain % of articles must include this word
  minN <- 2 # minimum number of words in a keyword
  
  # rake keywords from text
  rakedkeywords <- 
  litsearchr::extract_terms(
    text = paste(temp$title, temp$abstract),
    method = "fakerake",
    min_freq = minFreq,
    ngrams = TRUE,
    min_n = minN,
    language = "English",
    stopwords = all_stopwords
  )
  length(rakedkeywords)
  
  # Create a co-occurrence network
  naivedfm <-
    litsearchr::create_dfm(
      elements = paste(temp$title, temp$abstract),
      features = rakedkeywords
    )
  
  naivegraph <-
    litsearchr::create_network(
      search_dfm = naivedfm,
      min_studies = 2,
      min_occ = 2
    )
  
  
  # Calculate strength for each keyword
  strengths <- igraph::strength(naivegraph)
  
  require(dplyr)
  term_strengths <- data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
    mutate(rank=rank(strength, ties.method="min")) %>%
    arrange(strength)
  
  # finds the cutoff that retains 80% of the total strength
  cutoff <-
    litsearchr::find_cutoff(
      naivegraph,
      method = "cumulative",
      percent = .80,
      imp_method = "strength"
    )
  
  View(term_strengths)
  

}

```

Keywords are saved in coding/keyword-matches-supplemental-coding

# Visualise correlations?




