---
title: "07-supplemental_coding"
author: "Devi Veytia"
date: "2023-04-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
```


# Identify which articles may fit needed categories

Do this for each undersampled "category":
- identify documents that have key terms
- screen
- code



## Assemble articles to search

```{r source functions}
functionsToSource <- c("clean_string.R", "screen.R","tokenization.R","utils.R")
for(i in 1:length(functionsToSource)){
  source(here::here("R", functionsToSource[i]))
}

```


```{r get metadata for articles with predicted relevance greater than 0.5 and which haven't been screened}
require(dbplyr)

all_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db) # look at all the tables in the database

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(duplicate_id, analysis_id,title, author, year, journal, abstract, keywords, research_areas)

# predicted relevance for each article off of initial calculations
predRel <- tbl(all_db, "predRel")
relRefs <- predRel %>%
  select(analysis_id, relevance_mean) %>%
  filter(0.5 <= relevance_mean) 

# the lookup table of sysrev_ids so I can filter out those articles which have already been screened
sysrev_id <- tbl(all_db, "sysrevid_2_analysisid_lookup")

# merge together to get metadata of references with predicted relevance > 0.5
my_text <- left_join(relRefs, dedup_sub,by = "analysis_id") 
my_text <- my_text %>%
  left_join(sysrev_id %>% select(duplicate_id, sysrev_id), by="duplicate_id")

# filter to exclude articles that already have a sysrev_id (i.e. have already been screened)
my_text <- my_text %>%
  filter(is.na(sysrev_id))

my_text <- my_text %>% collect() # collect all entries

nrow(my_text)

## Disconnect databases
dbDisconnect(all_db)


```

```{r process the text to search}
# group title, abstract together
my_text <- my_text %>%
  mutate(text = paste(title, abstract))

# Lemmitization and clean string to remove extra spaces, numbers and punctuation
# takes about 1-2 min
my_text$text <- clean_string(my_text$text) 
my_text$text <- textstem::lemmatize_strings(my_text$text)
my_text$text <- uk2us::convert_uk2us(my_text$text)

# save
# saveRDS(my_text, 
#         here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))

```

## Search using keywords

I assembled a list of keywords grouped by different descriptive factors we would like to extract information about. We then scanned the article title, abstract and keywords and counted the occurrences of matches to these terms (boolean response of "yes" for at least one match/article).

```{r load in keywords to search and process}

## Load spreadsheet of keywords to extract and clean the text
nlp_search_terms <- read.csv(here::here("data","derived-data","coding",
                                        "keyword-matches-supplemental-coding","keyword-search-tokens.csv"))
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling

# separate out into single terms and expressions
single_words <- nlp_search_terms$Term[which(nlp_search_terms$Term.type == "single")]
expressions <- nlp_search_terms$Term[which(nlp_search_terms$Term.type == "expression")]
# name them by their corresponding group
names(single_words) <- nlp_search_terms$Group[which(nlp_search_terms$Term.type == "single")]
names(expressions) <- nlp_search_terms$Group[which(nlp_search_terms$Term.type == "expression")]

# save
saveRDS(nlp_search_terms,
        here::here("data","derived-data","coding","keyword-matches-supplemental-coding","keyword-search-tokens-format.rds"))

```


## Screen for keywords

```{r screen for keywords}
screens_swd <- screen(my_text$text, single_words)
screens_expr <- screen(my_text$text, expressions)

# order dataset according to spreadsheet of terms
screens_all <- cbind(screens_expr, screens_swd)
screens_all <- screens_all[,match(gsub(" ","_", nlp_search_terms$Term), colnames(screens_all))]

# make sure everything is just a y/n response, to presence/absence
screens_all <- ifelse(1 <= screens_all, 1, 0)

screens_all <- cbind(data.frame(analysis_id = my_text$analysis_id),screens_all)


# saveRDS(screens_all, 
#         here::here("data","derived-data","coding","keyword-matches-supplemental-coding","keywordMatches.rds"))

```

```{r tabulate results by group to see how many matches overall}
dim(screens_all)
colnames(screens_all)

groups <- unique(nlp_search_terms$Group)

# empty matrix with one row for every group and a column for number of matches
tabSums <- data.frame(
  Group = groups,
  n_matches = rep(NA, length(groups))
) 


## Loop through the groups and tabulate sums 
for(g in 1:length(groups)){
  
  # which columns (terms) in the screens_all matrix apply to a given group
  colInd <- which(colnames(screens_all) %in% 
                    nlp_search_terms$Term[which(nlp_search_terms$Group == groups[g])])
  
  # across all the terms, I only care if there is a match to any term
  if(length(colInd)>1){
    tempTab <- rowSums(screens_all[,colInd], na.rm = TRUE)
  }else{
    tempTab <- screens_all[,colInd]
  }
  tempTab[tempTab > 1] <- 1
  
  # save to data frame
  tabSums$n_matches[which(tabSums$Group == groups[g])] <- sum(tempTab, na.rm=T)
  
  if(g == length(groups)){
    rm(colInd, tempTab)
  }
}

print(tabSums)
```

No matches for assisted adaptation -- Add a few more or maybe extend to looking at whole corpus again


# Screen articles within each keyword group

```{r source functions and libraries to tabulate and screen articles}
source(here::here("R","tabMatches.R"))

require(dplyr)
```

```{r load in data for screening}
screens_all <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","keywordMatches.rds"))
nlp_search_terms <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding",
                                       "keyword-search-tokens-format.rds"))
my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))

```

```{r screen efficiency articles}

## Identify and sort articles by number of matches -- this way hopefully prioritizing most relevant

screenDf <- tabMatches( # tablulate number of matches retrieved for each article
  screenMatrix = screens_all,
  group = "Efficiency",
  keywordGroupLookupTable = nlp_search_terms)

screenDf <- subset(screenDf, n_matches > 0) # subset to only the matches

screenDf <- merge.data.frame( # join with metadata
  screenDf, my_text %>% select(-text), by = "analysis_id", all.x = TRUE, all.y = FALSE
)

screenDf <- screenDf %>% # sort dataframe in order of likely decreasing relevance
  arrange(desc(n_matches), desc(relevance_mean))

# are the number of matches show a correlation with predicted relevance? Not really
# with(screenDf[sample(1:nrow(screenDf), 500),], plot(relevance_mean, n_matches), type = ".")



## Screen articles
# To avoid error, I have to rename analysis_id column as label

revtools::screen_abstracts(
  x = screenDf %>% 
    select(title, author, year, journal, abstract, research_areas, duplicate_id) %>%
    rename(label = duplicate_id) 
)

```



## Visualizing keyword search results

identify numbers of papers relevant to each topic. The tokens to search for are stored in /data/derived-data/coding/keyword-matches/keyword-search-tokens.csv. 

```{r lookup table of groups for different keywords}
nlp_search_terms <- read.csv(here::here("data","derived-data","coding","keyword-matches","keyword-search-tokens.csv"))


## Process terms so that they match the column names in the sqlite database
colnames(nlp_search_terms) <- c("Group","Group name", "Term","Term type")
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling
nlp_search_terms$Term <- gsub(" ","_", nlp_search_terms$Term)


## Change names of some groupings

# change groupings of mitigation keywords
nlp_search_terms$`Group name`[
  which(nlp_search_terms$`Group name` %in% c("removal","CO2 removal from seawater",
                                   "OIF","blue carbon","bioenergy"))
] <- "carbon removal or storage"


## Names for each of the papers
topicNames <- c(
  "R&D vs implementation of mitigation OROs",
  "Biodiv & NCP outcomes",
  "Restoration practices",
  "Coastal community adaptation portfolio"
)

```

```{r connect to database with the keyword matches}
## Connect to database
keywordCon <- RSQLite::dbConnect(RSQLite::SQLite(), 
                                 dbname = here::here("data","derived-data","coding","keyword-matches","keyword-matches.sqlite"))
src_dbi(keywordCon)

keywordMatches <- tbl(keywordCon, "keywordMatches")

```

Now visualize these results 
```{r create an object to store all the plots}
ggp_list <- list()
```


For paper 1, I just want a barchart, and then the total number with any occurrence
```{r paper 1}
## Identify which paper
PaperNumber=1
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper 1
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  #head() %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
nrow(paper_df)  

# by keyword group (Group name) tabulate how many hits there were
paper_tab <- paper_df %>%
  group_by(`Group name`) %>%
  summarise(n=n_distinct(analysis_id))

# how many articles retreived any hit
paper_total1 <- length(unique(paper_df$analysis_id))


## Display 

# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000


# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- paper_tab %>%
  mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = ymax+yoffset, label = paste("total papers = ", paper_total1)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]


```


For paper 2, they want to know papers that have biodiv, ES, or both
```{r paper 2}
## Identify which paper
PaperNumber=2
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
  

# # how man articles have either or both biodiv or ncp
counts = with(paper_df, table(tapply(`Group name`, analysis_id, function(x) paste(as.character(sort(unique(x))), collapse=' & '))))
counts = as.data.frame(t(counts))
colnames(counts) <- c("Topic", "Group name","n")
counts$Topic <- paste("Paper", PaperNumber)


# # by keyword group (Group name) tabulate how many hits there were
# paper_tab <- paper_df %>%
#   group_by(`Group name`) %>%
#   summarise(n=n_distinct(analysis_id))


# how many articles retreived any hit
paper_total <- length(unique(paper_df$analysis_id))



# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000

# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- counts %>%
  #mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = n, label=paste(n),group = `Group name`), position = position_stack(vjust=0.5))+
    #geom_text(aes(x=paste("Paper", PaperNumber), y = ymax+yoffset, label = paste("total papers = ", paper_total)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    #ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]

```


For paper 3, do the same as for paper 2
```{r paper 3}
## Identify which paper
PaperNumber=3
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
  

# # how man articles have either or both biodiv or ncp
counts = with(paper_df, table(tapply(`Group name`, analysis_id, function(x) paste(as.character(sort(unique(x))), collapse=' & '))))
counts = as.data.frame(t(counts))
colnames(counts) <- c("Topic", "Group name","n")
counts$Topic <- paste("Paper", PaperNumber)


# # by keyword group (Group name) tabulate how many hits there were
# paper_tab <- paper_df %>%
#   group_by(`Group name`) %>%
#   summarise(n=n_distinct(analysis_id))


# how many articles retreived any hit
paper_total <- length(unique(paper_df$analysis_id))



# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000

# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- counts %>%
  #mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = n, label=paste(n),group = `Group name`), position = position_stack(vjust=0.5))+
    #geom_text(aes(x=paste("Paper", PaperNumber), y = ymax+yoffset, label = paste("total papers = ", paper_total)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    #ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]

```


```{r paper 4}

## Identify which paper
PaperNumber=4
Paper = paste0("Paper", PaperNumber)
tempTitle1 <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")
tempTitle2 <- paste0(letters[PaperNumber+1],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper 
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  #head() %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)

# how many articles retreived any hit
paper_total4 <- length(unique(paper_df$analysis_id))


nrow(paper_df)


## Format to visualize as a heatmap
library(reshape2)
paper_tab <- paper_df[!duplicated(paper_df[,c("analysis_id","Group name")]),]


paper_tab_Intervention <- paper_tab[grep("Intervention",paper_tab$`Group name`),c("analysis_id","Group name")]
colnames(paper_tab_Intervention)[2] <- c("Intervention")
paper_tab_Intervention$Intervention <- gsub("Intervention: ","",paper_tab_Intervention$Intervention)

paper_tab_Population <- paper_tab[grep("Population",paper_tab$`Group name`),c("analysis_id","Group name")]
colnames(paper_tab_Population)[2] <- c("Population")
paper_tab_Population$Population <- gsub("Population: ","",paper_tab_Population$Population)

paper_tab_Threat <- paper_tab[grep("threat",paper_tab$`Group name`),c("analysis_id","Term")]
colnames(paper_tab_Threat)[2] <- c("Threat")


paperMerge <- merge(paper_tab_Intervention, paper_tab_Population, by="analysis_id")
paperMerge <- merge(paperMerge, paper_tab_Threat, by="analysis_id")


## Display
library(stringr)

# Visualize Population by intervention
ggp_list[[PaperNumber]] <- paperMerge %>%
  group_by(Intervention, Population) %>%
  summarise(n=n()) %>%
  ggplot(aes(Population, Intervention, fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()+
  scale_y_discrete(labels = function(x) str_wrap(x, width=10))+
  geom_text(aes(label = n), col="red")+
  labs(title = paste(strwrap(tempTitle1, width = 40), collapse="\n"),
       caption = paste("total articles = ", paper_total4))+
  theme(
      legend.position = "na",
      legend.title = element_blank(),
      #axis.text.x = element_blank(),
      #axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )


# Visualise threat by intervention
ggp_list[[PaperNumber+1]] <- paperMerge %>%
  group_by(Intervention, Threat) %>%
  summarise(n=n()) %>%
  ggplot(aes(Threat, Intervention, fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()+
  scale_y_discrete(labels = function(x) str_wrap(x, width=10))+
  geom_text(aes(label = n), col="red")+
  labs(title = paste(strwrap(tempTitle2, width = 40), collapse="\n"),
       caption = paste("total articles = ", paper_total4))+
  theme(
      legend.position = "na",
      legend.title = element_blank(),
      axis.text.x = element_text(angle=45,hjust=1),
      #axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
ggp_list[[PaperNumber+1]]
```

```{r create all plots together}

pdf(file = here::here("data","derived-data","coding","keyword-matches","keyword-matches-plots.pdf"),
    width = 10, height=15)
egg::ggarrange(
  plots = ggp_list,
  nrow=3,
  newpage=FALSE
)
dev.off()

```


```{r close databset connection}
## Close connection
RSQLite::dbDisconnect(keywordCon)

```



```{r clean environment}
rm(list=ls())
```


## Export metadata for review by teams

```{r source functions}
functionsToSource <- c("clean_string.R", "screen.R","tokenization.R","utils.R")
for(i in 1:length(functionsToSource)){
  source(here::here("R", functionsToSource[i]))
}

```


```{r lookup table of groups for different keywords}
nlp_search_terms <- read.csv(here::here("data","derived-data","coding","keyword-matches","keyword-search-tokens.csv"))


## Process terms so that they match the column names in the sqlite database
colnames(nlp_search_terms) <- c("Group","Group name", "Term","Term type")
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling
nlp_search_terms$Term <- gsub(" ","_", nlp_search_terms$Term)


## Change names of some groupings

# change groupings of mitigation keywords
nlp_search_terms$`Group name`[
  which(nlp_search_terms$`Group name` %in% c("carbon","removal","CO2 removal from seawater",
                                   "OIF","blue carbon","bioenergy"))
] <- "carbon removal or storage"


## Names for each of the papers
topicNames <- c(
  "R&D vs implementation of mitigation OROs",
  "Biodiv & NCP outcomes",
  "Restoration practices",
  "Coastal community adaptation portfolio"
)

```

```{r connect to database with the keyword matches}
## Connect to database
keywordCon <- RSQLite::dbConnect(RSQLite::SQLite(), 
                                 dbname = here::here("data","derived-data","coding","keyword-matches","keyword-matches.sqlite"))
src_dbi(keywordCon)

keywordMatches <- tbl(keywordCon, "keywordMatches")
keywordMatches <- keywordMatches %>% collect

dbDisconnect(keywordCon)
```

```{r get associated metadata for the matches}

## Get tables from databases
dedup_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite"), 
                                 create=FALSE)
src_dbi(dedup_db)
dedups <- tbl(dedup_db, "uniquerefs")



metadata <- dedups %>% 
  select(analysis_id, author, title, source_title, year, abstract, doi) %>%
  collect()

dbDisconnect(dedup_db)
```

```{r join metadata to keyword matches}

keywordMatchesMerge <- merge(keywordMatches, metadata, by="analysis_id", all.x = TRUE, all.y=FALSE)

# reorder columns so keyword matches come last
cols <- colnames(keywordMatchesMerge)
cols <- cols[-which(cols %in% c("analysis_id", "author","title","source_title","year","abstract","doi"))]
keywordMatchesMerge <- keywordMatchesMerge[,c("analysis_id", "author","title","source_title","year","abstract","doi", cols)]
  
```



```{r For each paper filter to relevant articles and export as a csv}
## Inputs
out_dir <- here::here("data","derived-data","coding","keyword-matches","keyword-matches-and-metadata-csv")


papers <- unique(nlp_search_terms$Group)

# columns that contain the metadata variables -- the keep
metadata_cols <- c("analysis_id", "author","title","source_title","year","abstract","doi")




for(p in 1:length(papers)){
  
  # columns that contain the keywords relevant to that paper
  paper_cols <- nlp_search_terms$Term[which(nlp_search_terms$Group == papers[p])]
  
  # subset the dataframe to only the metadata columns and the keywords relevant to the paper
  tempDf <- keywordMatchesMerge[,c(metadata_cols, paper_cols)]
  
  
  # compress all keyword columns into one column containing all the matches in one string
  
  # replace binary indicator with keyword name
  for(j in 1:length(paper_cols)){
    tempDf[,paper_cols[j]] <- ifelse(tempDf[,paper_cols[j]] > 0, paper_cols[j], NA)
  }
  
  # make one column that joins all the keywords in that row
  matches <- apply(tempDf[,paper_cols], 1, function(x){
    x <- x[!is.na(x)]
    if(length(x)==0){
      x <- NA
    }else{
      x <- paste(x, collapse = ", ")
    }
    return(x)
  })
  
  tempDf <- cbind(tempDf[,metadata_cols], data.frame(keyword_matches = matches))
  tempDf <- tempDf %>%
    filter(!is.na(abstract), !is.na(keyword_matches))
  
  
  ## If it is paper 2, make another column if there is a doi match to Galparoso et al
  if(papers[p] == "Paper2"){
    # read in
    galRef <- revtools::read_bibliography(here::here("data","raw-data","citation-chasing","Galparsoro_etal_2002_references.ris"))
    galRef <- subset(galRef, !is.na(doi))
    galmatch <- tempDf$doi %in% galRef$doi 
    ngalmatch <- sum(galmatch)
    
    tempDf <- cbind(tempDf, galmatch)
    
    colnames(tempDf)[ncol(tempDf)] <- paste0("Galparsoro_match_total_",ngalmatch)
  }

  
  ## Write to csv
  write.csv(tempDf, file = file.path(out_dir, paste(papers[p],"keyword-matches.csv", sep="-")))
}

```




