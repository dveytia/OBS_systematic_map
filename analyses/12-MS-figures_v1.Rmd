---
title: "12-MS-figures_v1"
author: "Devi Veytia"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Set up


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(ggplot2)

```

```{r database filepath}
all_db_fp <- here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite")

```

```{r colourpalette}
branchPal <- c("Mitigation" = "#35a7d9","Nature" = "forestgreen", "Societal"="#7670a8")

```




# 1. Summarise the corpus


```{r PRISMA diagram}

## SQLITE CALCULATIONS

# connect to db
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE) 

# Get a table with all the references and calculate:
# number retreived from each source
# number of unseen articles
allrefs <- tbl(all_db,"allrefs_join") 
databaseNum <- allrefs%>%group_by(source_database)%>% summarise(n=n()) %>% as.data.frame() 
naAbstracts <- allrefs%>%filter(is.na(abstract))%>% summarise(n=n()) %>% as.data.frame()

# Get a table of all the de-duplicated references 
dedups <- tbl(all_db, "uniquerefs") 
nUnique <- dedups %>%
  filter(!is.na(abstract)) %>%
  summarise(n=n())%>%
  as.data.frame()

# Get a table of all those predicted to be relevant
predRel <- tbl(all_db, "predRel2")
nInclude <- predRel %>% 
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)) %>%
  as.data.frame()

# Disconnect the database
dbDisconnect(all_db)



## PLOT THE PRISMA DIAGRAM


PRISMAFlow <- c(
  paste("START_PHASE:", 
        format(databaseNum$n[databaseNum$source_database == "Web Of Science"], format = "d", big.mark=","),
        "articles from WOS"),
  paste("START_PHASE:", format(databaseNum$n[databaseNum$source_database == "Scopus"], 
                               format = "d", big.mark=","),
        "articles from Scopus"),
  paste(format(sum(databaseNum$n), format = "d", big.mark=","),"articles in total"),
  paste("EXCLUDE_PHASE:", format(naAbstracts, format = "d", big.mark=","),"abstracts removed as NA"),
  paste(format(sum(databaseNum$n)-naAbstracts, format = "d", big.mark=","),"eligible articles"),
  paste("EXCLUDE_PHASE:",
        format(sum(databaseNum$n)-naAbstracts-nUnique, format = "d", big.mark=","),"duplicates removed"),
  paste(format(nUnique, format = "d", big.mark=","),"unique articles"),
  paste("EXCLUDE_PHASE:", format(nUnique-nInclude[3], format = "d", big.mark=","),"articles excluded"),
  paste(format(nInclude[3], format = "d", big.mark=","),"articles predicted relevant (upper)")
)

w <- 12
h <- w*0.7

pdf(here::here("figures/supplementary/prisma-diagram.pdf"), width = w, height = h)
PRISMAFlowChart <- metagear::plot_PRISMA(PRISMAFlow, 
                                         design = c(E = "lightcoral", flatArrow = TRUE),
                                         excludeDistance = 0.8, colWidth = 50)
dev.off()

```


# Broad trends and WOS metadata

```{r Annual trend in number of documents}

## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs")
predRel <- tbl(all_db, "predRel2") 
predBranch <- tbl(all_db, "predBranch")

# Calculate the number of relevant predictions for each year
df <- predRel %>%
  left_join(dedups, by="analysis_id") %>%
  left_join(predBranch, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(n_lower = sum(0.5 <= relevance_lower),
            n_total = sum(0.5 <= relevance_mean),
            n_upper = sum(0.5 <= relevance_upper),
            n_mitigation = sum(0.5 <= Mitigation_mean & 0.5 <= relevance_mean),
            n_nature = sum(0.5 <= Nature_mean & 0.5 <= relevance_mean),
            n_societal = sum(0.5 <= Societal_mean & 0.5 <= relevance_mean)) %>%
  collect()

# Disconnect the database
dbDisconnect(all_db)


## Data formatting
df$year <- as.Date(paste0("01-01-", df$year), format = "%d-%m-%Y")
df <- df %>% filter(year < as.Date("01-01-2023", format = "%d-%m-%Y"))

df_melt <- reshape2::melt(df, measure.vars = c("n_total","n_mitigation","n_nature","n_societal"),
                     value.name = "mean_prediction", variable.name = "variable") %>%
  mutate(variable = factor(variable, levels = c("n_total","n_mitigation","n_nature","n_societal"),
                           labels = c("All articles","Mitigation","Natural resilience","Societal\nadaptation")))
  
# add log scale
df_log <- df %>%
  mutate(n_lower = log(n_lower), n_upper = log(n_upper), scale = "log")
df_log <- rbind(
  df %>% mutate(scale = "normal"),
  df_log
)
df_log$scale <- factor(df_log$scale,
                       levels = c("normal","log"),
                       labels = c("un-transformed","log-transformed"))

df_melt_log <- df_melt %>%
  mutate(mean_prediction = log(mean_prediction), scale = "log")
df_melt_log <- rbind(
  df_melt %>% mutate(scale = "normal"),
  df_melt_log
)
df_melt_log$scale <- factor(df_melt_log$scale, 
                            levels = c("normal","log"),
                            labels = c("un-transformed","log-transformed"))

# Model fit
mod <- lm(log(n_total)~year, data = df, subset = n_total > 0)
summary(mod)
pred_df <- data.frame(year = seq(min(df$year), max(df$year), by=1))
pred_y <- predict(mod, pred_df, interval = "confidence", type="response")
pred_df <- rbind(
  cbind(pred_df, as.data.frame(pred_y), data.frame(scale = rep("log-transformed", nrow(pred_df)))),
  cbind(pred_df, as.data.frame(exp(pred_y)), data.frame(scale = rep("un-transformed", nrow(pred_df))))
)
pred_df$scale <- factor(pred_df$scale, levels = c("un-transformed","log-transformed"))



formula_text <- data.frame(
  label = paste0("y == ", 
                 format(coef(mod)[1], scientific=TRUE, digits = 2), "*(",
                 format(coef(mod)[2], scientific=TRUE, digits = 2),"^x)"),
  year=as.Date("1932-01-01"),
  y=max(df$n_total),
  scale = factor("un-transformed", levels = c("un-transformed","log-transformed"))
)

## Plot
annualTrend_ggp <- ggplot()+
  facet_grid(scale ~., scales = "free_y")+
  geom_ribbon(data=df_log, aes(x=year, ymin = n_lower, ymax = n_upper), fill = "lightgrey")+
  geom_line(data = df_melt_log, aes(x=year, y=mean_prediction, col = variable), linewidth = 1)+
  geom_line(data = pred_df, aes(x=year, y=fit), col="red", linetype = "dashed", linewidth = 1)+
  geom_text(data=formula_text, aes(x=year, y=y, 
                                   label = label),parse=TRUE, color = "red")+
  geom_text(data=formula_text, aes(x=year, y=y-1000, 
                                   label = paste("R^2 == ",
                                                 format(summary(mod)$adj.r.squared, digits = 2))),
            color = "red", parse=TRUE)+
  scale_color_manual(values = c("black",as.vector(branchPal)), 
                     name = "Mean predicted\nrelevance")+
  labs(x="Year", y="Number of articles predicted relevant", 
       caption = "all terms are significant p << 0.05")+
  scale_x_date()+
  theme_classic()+
  theme(legend.position = "bottom")

annualTrend_ggp

ggsave(here::here("figures/main/annual-trends.pdf"),plot=annualTrend_ggp,
       width = 7, height=4.5, units="in")

```


```{r Annual trend by research area}
## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 
predBranch <- tbl(all_db, "predBranch")

# "research_areas", web_of_science_categories",
# Don't do "organization or" "funding" or "affiliation"  yet, wait to compare to geoparsing results

# Calculate the number of relevant predictions for each year
n_relevant <- predRel %>%
  left_join(dedups, by="analysis_id") %>%
  left_join(predBranch, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(n_lower = sum(0.5 <= relevance_lower),
            n_total = sum(0.5 <= relevance_mean),
            n_upper = sum(0.5 <= relevance_upper),
            n_mitigation = sum(0.5 <= Mitigation_mean & 0.5 <= relevance_mean),
            n_nature = sum(0.5 <= Nature_mean & 0.5 <= relevance_mean),
            n_societal = sum(0.5 <= Societal_mean & 0.5 <= relevance_mean)) %>%
  collect()

research_area_df <- predRel %>%
  filter(0.5 <= relevance_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(research_areas)) %>%
  select(analysis_id, duplicate_id, research_areas, web_of_science_categories, year)%>%
  filter(!is.na(year)) %>%
  mutate(research_areas = eval(parse(text=gsub("\\", "", deparse(research_areas), fixed=TRUE))),
         first_research_area = gsub("\\;.*","", research_areas)) %>%
  collect()

# Disconnect the database
dbDisconnect(all_db)




## Data Formatting & Calculations

## For number of relevant articles
# year as date
n_relevant$year <- as.Date(paste0("01-01-", n_relevant$year), format = "%d-%m-%Y")
n_relevant <- n_relevant %>% filter(year < as.Date("01-01-2023", format = "%d-%m-%Y"))

# Melt by ORO branch
n_relevant_melt <- reshape2::melt(n_relevant, measure.vars = c("n_total","n_mitigation","n_nature","n_societal"),
                     value.name = "mean_prediction", variable.name = "variable") %>%
  mutate(variable = factor(variable, levels = c("n_total","n_mitigation","n_nature","n_societal"),
                           labels = c("All articles","Mitigation","Natural resilience","Societal\nadaptation")))

# Model fit
mod <- lm(log(n_total)~year, data = n_relevant, subset = n_total > 0)
pred_df <- data.frame(year = seq(min(n_relevant$year), max(n_relevant$year), by=1))
pred_y <- predict(mod, pred_df, interval = "confidence", type="response")
pred_df <- cbind(pred_df, as.data.frame(exp(pred_y)))
  
formula_text <- data.frame(
  label = paste0("y == ", 
                 format(coef(mod)[1], scientific=TRUE, digits = 2), "*(",
                 format(coef(mod)[2], scientific=TRUE, digits = 2),"^x)"),
  year=as.Date("1960-01-01"),
  y=max(n_relevant$n_total)
)



## Research areas

# tabulate the number of different research areas in a year and
# filter the tabulated results to just the high-sampled research areas

# tabulate
years <- unique(research_area_df$year)
for(y in 1:length(years)){
  tempList <- strsplit(research_area_df$research_areas[research_area_df$year == years[y]],split = "; ")
  tempTab <- as.data.frame(table(unlist(lapply(tempList, unique))))
  tempTab$year <- years[y]
  if(y==1)
    research_area_tab <- tempTab
  else
    research_area_tab <- rbind(research_area_tab, tempTab)
}
# rename columns
research_area_tab$year <- as.Date(paste0(research_area_tab$year,"-01-01"))
research_area_tab <- research_area_tab%>%
  rename(research_area = Var1, n_articles = Freq)
# Find which research areas are in the higher percentile
low_research_areas <- research_area_tab %>%
  group_by(research_area) %>%
  summarise(n_articles=sum(n_articles)) %>%
  ungroup()%>%
  arrange(desc(n_articles)) %>%
  slice(1:10) %>%
  arrange(research_area)
raLevels <- as.character(low_research_areas$research_area)

# subset
research_area_tab <- research_area_tab %>%
  filter(year < as.Date("2023-01-01")) %>%
  mutate(research_area = ifelse(research_area %in% raLevels,
                                as.character(research_area), "Other")) %>% 
  mutate(research_area = factor(research_area, levels = c(raLevels, "Other")))

# Calculate as a proportion of total articles
yearlyTotal <- research_area_tab %>%
  group_by(year) %>%
  summarise(yearlyTotal = sum(n_articles))

research_area_tab <- research_area_tab %>%
  merge(yearlyTotal, by="year")%>%
  mutate(proportion = n_articles/yearlyTotal)



## PLOT

## Annual trend in number of articles
annualTrend_ggp2 <- ggplot()+
  geom_ribbon(data=n_relevant, aes(x=year, ymin = n_lower, ymax = n_upper), fill = "lightgrey")+
  geom_line(data = n_relevant_melt, aes(x=year, y=mean_prediction, col = variable), linewidth = 1)+
  geom_line(data = pred_df, aes(x=year, y=fit), col="red", linetype = "dashed", linewidth = 1)+
  geom_text(data=formula_text, aes(x=year, y=y, 
                                   label = label),parse=TRUE, color = "red")+
  geom_text(data=formula_text, aes(x=year, y=y-1000, 
                                   label = paste("R^2 == ",
                                                 format(summary(mod)$adj.r.squared, digits = 2))),
            color = "red", parse=TRUE)+
  scale_color_manual(values = c("black",as.vector(branchPal)), 
                     name = "Mean predicted\nrelevance")+
  labs(x="Year", y="Articles predicted relevant (n)")+
  scale_x_date()+
  theme_classic()+
  theme(legend.position = "right")

annualTrend_ggp2

## Annual trend in the articles by research area
annualResearchArea_ggp <- ggplot()+
  geom_col(data = research_area_tab, aes(x=year, y=n_articles, fill=research_area),
           position = "stack")+
  scale_x_date()+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Articles predicted relevant (n)",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  

annualResearchArea_ggp


# Show as a proportion

annualResearchAreaProportion_ggp <- research_area_tab %>%
  ggplot()+
  geom_col(aes(x=year, y=proportion, fill=research_area),
           position = "stack")+
  scale_x_date(limits = c(as.Date("1990-01-01"),as.Date("2022-01-01")))+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Proportional contribution",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  




pdf(here::here("figures/main/annual-trends-research-area.pdf"),
    width = 7, height=6)
egg::ggarrange(annualTrend_ggp2, annualResearchAreaProportion_ggp,
          nrow=2, ncol=1, newpage = FALSE, labels = c("a.","b."))
dev.off()
```

```{r trend in research area by oro branch}

## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predBranch <- tbl(all_db, "predBranch")

# Get the number of articles that are relevant, for any ORO branch
# and their metadata
research_area_branch_df <- predBranch %>%
  filter(0.5 <= Mitigation_upper |
           0.5 <= Nature_upper |
           0.5 <= Societal_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(research_areas)) %>%
  select(analysis_id, duplicate_id, research_areas, year, 
         Mitigation_upper, Nature_upper, Societal_upper)%>%
  filter(!is.na(year)) %>%
  collect()


# Disconnect the database
dbDisconnect(all_db)


## LOAD OECD REASEARCH AREA CLASSIFICATIONS
oecdCat <- readxl::read_xlsx(here::here("data/raw-data/research-area-schemas/OECD-Category-Mapping.xlsx"))
oecdCat <- oecdCat[,c(1,3)]
colnames(oecdCat) <- c("OECD_research_area","research_area_upper")
oecdCat$OECD_level = readr::parse_number(oecdCat$OECD_research_area)
oecdCat$OECD_high_level = floor(oecdCat$OECD_level)
oecdCatHighLevel <- oecdCat[,c("OECD_research_area","OECD_level")]
oecdCatHighLevel <- oecdCatHighLevel[!duplicated(oecdCatHighLevel),]
oecdCatHighLevel <- oecdCatHighLevel[which(oecdCatHighLevel$OECD_level %in% c(1:6)),]
colnames(oecdCatHighLevel)[1] <- "OECD_research_area_high"
oecdCat <- oecdCat %>% left_join(oecdCatHighLevel, by="OECD_level")
oecdCat <- oecdCat[!duplicated(oecdCat[,c("research_area_upper","OECD_research_area_high")]),]
oecdCat <- oecdCat %>% arrange("OECD_level")

research_area_tab %>%
  head() %>%
  mutate(research_area_upper = toupper(research_area))%>%
  left_join(oecdCat, by="research_area_upper")



## CALCULATIONS

# tabulate the number of different research areas in a year and branch,
# filter the tabulated results to just the high-sampled research areas

# Format the columns
research_area_branch_df <- research_area_branch_df %>% 
  mutate(research_areas = eval(parse(text=gsub("\\", "", deparse(research_areas), fixed=TRUE)))) 

research_area_branch_df_melt <- reshape2::melt(
  research_area_branch_df, measure.vars = c("Mitigation_upper", "Nature_upper", "Societal_upper"),
  value.name = "relevance_upper", variable.name = "ORO_branch"
)
# make sure only keep the rows that are predicted relevant for that particular branch
research_area_branch_df_melt <- research_area_branch_df_melt %>%
  filter(0.5 <= relevance_upper)

research_area_branch_df_melt$ORO_branch <- factor(
  research_area_branch_df_melt$ORO_branch,
  levels = c("Mitigation_upper", "Nature_upper", "Societal_upper"),
  labels = c("Mitigation","Natural resilience","Societal adaptation"))

# Loop through all the unique year and branch combinations to tabulate the research areas
years <- unique(research_area_df$year) 
branches <- levels(research_area_branch_df_melt$ORO_branch) 
for(y in 1:length(years)){
  for(b in 1:length(branches)){
    # Tabulate appearances of each unique research area for that year and branch
    tempList <- strsplit(
      research_area_branch_df_melt$research_areas[
        research_area_branch_df_melt$year == years[y] &
          research_area_branch_df_melt$ORO_branch == branches[b]],
      split = "; ")
    if(length(tempList)==0){
      next
    }
    
    tempTab <- as.data.frame(table(unlist(lapply(tempList, unique))))
    # match to get to OCED level
    matches <- sapply(toupper(tempTab$Var1), function(x) grep(x, oecdCat$research_area_upper))
    if(!("list" %in% class(matches))){
      matches <- list(as.vector(matches))
    }
    tempTab$oecd_research_area <- oecdCat$OECD_research_area_high[
      sapply(matches, function(x) ifelse(length(x)==0, NA, min(x)))]
    
    # If no match, separate by "&" symbol and look for individual matches
    # Start by separating dataframes to only investigate NAs
    noMatch <- subset(tempTab, is.na(oecd_research_area))
    tempTab <- subset(tempTab, !is.na(oecd_research_area))
    # Get new research areas by splitting at &
    newRAs <- unlist(strsplit(as.character(noMatch$Var1),split='&',fixed=TRUE))
    newRAs <- trimws(newRAs, which = c("both"))
    # Match elongated dataframe with old frequencies
    matches <- sapply(newRAs, function(x) grep(x, noMatch$Var1))
    newFreq <- unlist(sapply(matches, function(x) sum(noMatch$Freq[x], na.rm=T)))
    noMatch <- data.frame(
     Var1 = newRAs,
     Freq = newFreq
    )
    rownames(noMatch) <- NULL
    noMatch <- subset(noMatch, Var1 != "Science") # Too broad
    # Look for OECD match
    matches <- sapply(toupper(noMatch$Var1), function(x) grep(x, oecdCat$research_area_upper))
    if(length(matches)==0){next} # if no matches, continue
    if(sum(sapply(matches, function(x) length(x))) == 0){next} # if no matches, continue
    noMatch$oecd_research_area <- oecdCat$OECD_research_area_high[
      sapply(matches, function(x) ifelse(length(x)==0, NA, min(x)))]
    noMatch <- na.omit(noMatch)
    
    # Bind dataframes back together and Join
    tempTab <- rbind(tempTab, noMatch)
    tempTab$year <- years[y]
    tempTab$ORO_branch <- branches[b]
    if(y==1 & b==1)
      research_area_branch_tab <- tempTab
    else
      research_area_branch_tab <- rbind(research_area_branch_tab, tempTab)
  }
}

# rename columns
research_area_branch_tab$year <- as.Date(paste0(research_area_branch_tab$year,"-01-01"))
research_area_branch_tab$ORO_branch <- factor(research_area_branch_tab$ORO_branch,
                                              levels = branches)
research_area_branch_tab <- research_area_branch_tab%>%
  rename(research_area = Var1, n_articles = Freq)

research_area_branch_tab$oecd_research_area <- factor(
  research_area_branch_tab$oecd_research_area, 
  levels = oecdCatHighLevel$OECD_research_area_high[order(oecdCatHighLevel$OECD_level)]
)

# # Find which are the top n research areas for each branch overall
# low_research_branch_areas <- research_area_branch_tab %>%
#   group_by(research_area, ORO_branch) %>%
#   summarise(n_articles=sum(n_articles)) %>%
#   ungroup() %>%
#   arrange(desc(n_articles)) %>%
#   group_by(ORO_branch) %>%
#   slice(1:7)
# length(unique(low_research_branch_areas$research_area)) # number of total research areas
# raLevels <- unique(low_research_branch_areas$research_area)[order(unique(low_research_branch_areas$research_area))]
# raLevels <- as.character(raLevels)
# 
# # subset -- all other research areas put into the "other" category
# research_area_branch_tab <- research_area_branch_tab %>%
#   filter(year < as.Date("2023-01-01")) %>%
#   mutate(research_area = ifelse(research_area %in% raLevels,
#                                 as.character(research_area), "Other")) %>% 
#   mutate(research_area = factor(research_area, levels = c(raLevels, "Other"))) 



# Calculate the number of articles as a proportion of the total
yearlyBranchTotal <- research_area_branch_tab %>%
  group_by(year, ORO_branch) %>%
  summarise(totalArticles = sum(n_articles))

research_area_branch_tab <- research_area_branch_tab %>%
  merge(yearlyBranchTotal, by=c("year","ORO_branch")) %>%
  mutate(proportion = n_articles/totalArticles)


## PLOT
annualResearchAreaProportionBranch_ggp <- research_area_branch_tab %>%
  ggplot()+
  facet_grid(ORO_branch~.)+
  geom_col(aes(x=year, y=proportion, fill=research_area),
           position = "stack")+
  scale_x_date(limits = c(as.Date("1990-01-01"), as.Date("2022-01-01")))+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Proportional contribution",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")

annualResearchAreaProportionBranch_oecd_ggp <- research_area_branch_tab %>%
  ggplot()+
  facet_grid(ORO_branch~.)+
  geom_col(aes(x=year, y=proportion, fill=oecd_research_area),
           position = "stack")+
  scale_x_date(limits = c(as.Date("1990-01-01"), as.Date("2022-01-01")))+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Proportional contribution",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  
annualResearchAreaProportionBranch_ggp
annualResearchAreaProportionBranch_oecd_ggp


ggsave(here::here("figures/supplementary/annual-trends-branch-research-area.pdf"), 
       plot = annualResearchAreaProportionBranch_ggp, width = 7, height=6, units="in")

ggsave(here::here("figures/supplementary/annual-trends-branch-oecd-research-area.pdf"), 
       plot = annualResearchAreaProportionBranch_oecd_ggp, width = 7, height=6, units="in")

```


```{r Map of First Author Affiliation}
## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 

# Get the number of articles that are relevant, and their metadata
affiliation_df <- predRel %>%
  filter(0.5 <= relevance_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(affiliation)) %>%
  select(analysis_id, duplicate_id, affiliation)%>%
  collect()

# Disconnect the database
dbDisconnect(all_db)



## DATA FORMATTING
world_map <- map_data("world")

# look for matches between each full affiliatoin and a region in the world map
affiliation_df$region <- rep(NA, nrow(affiliation_df))
regions <- unique(world_map$region)
for(r in 1:length(regions)){
  matches <- grepl(regions[r], affiliation_df$affiliation,ignore.case = TRUE)
  affiliation_df$region[matches] <- regions[r]
}
sum(is.na(affiliation_df$region)) # no matches - 10678
sum(!is.na(affiliation_df$region)) # compared to # of matches - 46911

# Calculate the number of affiliations for each region
library(ggthemes)
affiliation_df_tab <- affiliation_df %>%
  group_by(region) %>%
  summarise(n_articles = n()) %>%
  right_join(world_map %>% filter(! long > 180), by = "region")


ggplot(data=affiliation_df_tab, aes(long, lat, group=group, fill = n_articles))+
  geom_polygon()+
  scale_fill_viridis_c()+
  coord_map("moll") 

lout = 100
bb <- data.frame(
  long = c(seq(-180,180,length.out = lout),
           rep(180, lout),
           seq(180,-180, length.out = lout),
           rep(-180, lout)),
  lat = c(rep(-90, lout),
          seq(-90,90,length.out = lout),
          rep(90, lout),
          seq(90,-90, length.out = lout))
)
bb$group <- paste("boundingBox")


affiliationMap_ggp <- ggplot()+
  geom_polygon(data = affiliation_df_tab, aes(long, lat, group=group, fill = n_articles))+
  geom_polygon(data = bb, aes(long, lat, group=group), col="black", fill = "transparent")+
  scale_fill_viridis_c(name = "Number of articles")+
  coord_map("moll") +
  theme_map()+
  theme(legend.position = "bottom", legend.key.width = unit(0.5, units = "in"))


# Another option for improvement could be this:
# https://seethedatablog.wordpress.com/2016/12/23/r-simple-world-map-robinson-ggplot/

```


```{r Save Annual trends research area and affiliation map multipanel}
## SQLITE CALCULATIONS -------------------
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 
predBranch <- tbl(all_db, "predBranch")

# Get the number of relevant predictions for each year
n_relevant <- predRel %>%
  left_join(dedups, by="analysis_id") %>%
  left_join(predBranch, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(n_lower = sum(0.5 <= relevance_lower),
            n_total = sum(0.5 <= relevance_mean),
            n_upper = sum(0.5 <= relevance_upper),
            n_mitigation = sum(0.5 <= Mitigation_mean & 0.5 <= relevance_mean),
            n_nature = sum(0.5 <= Nature_mean & 0.5 <= relevance_mean),
            n_societal = sum(0.5 <= Societal_mean & 0.5 <= relevance_mean)) %>%
  collect()


# Get the research area for relevant articles
research_area_df <- predRel %>%
  filter(0.5 <= relevance_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(research_areas)) %>%
  select(analysis_id, duplicate_id, research_areas, web_of_science_categories, year)%>%
  filter(!is.na(year)) %>%
  mutate(research_areas = eval(parse(text=gsub("\\", "", deparse(research_areas), fixed=TRUE))),
         first_research_area = gsub("\\;.*","", research_areas)) %>%
  collect()


# Get the affiliation metadata for relevant articles
affiliation_df <- predRel %>%
  filter(0.5 <= relevance_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(affiliation)) %>%
  select(analysis_id, duplicate_id, affiliation)%>%
  collect()

# Disconnect the database
dbDisconnect(all_db)



## PLOT 1: Annual trends in # of relevant articles ------------------

## Data formatting & Calculations


# year as date
n_relevant$year <- as.Date(paste0("01-01-", n_relevant$year), format = "%d-%m-%Y")
n_relevant <- n_relevant %>% filter(year < as.Date("01-01-2023", format = "%d-%m-%Y"))
# Melt by ORO branch
n_relevant_melt <- reshape2::melt(n_relevant, measure.vars = c("n_total","n_mitigation","n_nature","n_societal"),
                     value.name = "mean_prediction", variable.name = "variable") %>%
  mutate(variable = factor(variable, levels = c("n_total","n_mitigation","n_nature","n_societal"),
                           labels = c("All articles","Mitigation","Natural resilience","Societal\nadaptation")))
# Model fit
mod <- lm(log(n_total)~year, data = n_relevant, subset = n_total > 0)
pred_df <- data.frame(year = seq(min(n_relevant$year), max(n_relevant$year), by=1))
pred_y <- predict(mod, pred_df, interval = "confidence", type="response")
pred_df <- cbind(pred_df, as.data.frame(exp(pred_y)))
# Dataframe of where to write the model text on the plot
formula_text <- data.frame(
  label = paste0("y == ", 
                 format(coef(mod)[1], scientific=TRUE, digits = 2), "*(",
                 format(coef(mod)[2], scientific=TRUE, digits = 2),"^x)"),
  year=as.Date("1960-01-01"),
  y=max(n_relevant$n_total)
)


## Plotting
## Annual trend in number of articles
annualTrend_ggp2 <- ggplot()+
  geom_ribbon(data=n_relevant, aes(x=year, ymin = n_lower, ymax = n_upper), fill = "lightgrey")+
  geom_line(data = n_relevant_melt, aes(x=year, y=mean_prediction, col = variable), linewidth = 1)+
  geom_line(data = pred_df, aes(x=year, y=fit), col="red", linetype = "dashed", linewidth = 1)+
  geom_text(data=formula_text, aes(x=year, y=y, 
                                   label = label),parse=TRUE, color = "red")+
  geom_text(data=formula_text, aes(x=year, y=y-1000, 
                                   label = paste("R^2 == ",
                                                 format(summary(mod)$adj.r.squared, digits = 2))),
            color = "red", parse=TRUE)+
  scale_color_manual(values = c("black",as.vector(branchPal)), 
                     name = "Mean predicted\nrelevance")+
  labs(x="Year", y="Articles predicted relevant (n)")+
  scale_x_date()+
  theme_classic()+
  theme(legend.position = "right")




## PLOT 2: Contribution of different research areas -------------------------------

## Data Formatting and Calculations

# tabulate the number of different research areas in a year and
# filter the tabulated results to just the high-sampled research areas
# tabulate

# Try mapping WOS research areas (252 categories) to OECD category scheme which are only ~45
# http://help.prod-incites.com/inCites2Live/filterValuesGroup/researchAreaSchema/oecdCategoryScheme.html
oecdCat <- readxl::read_xlsx(here::here("data/raw-data/research-area-schemas/OECD-Category-Mapping.xlsx"))
oecdCat <- oecdCat[,c(1,3)]
colnames(oecdCat) <- c("OECD_research_area","research_area_upper")
oecdCat$OECD_level = readr::parse_number(oecdCat$OECD_research_area)
oecdCat$OECD_high_level = floor(oecdCat$OECD_level)
oecdCatHighLevel <- oecdCat[,c("OECD_research_area","OECD_level")]
oecdCatHighLevel <- oecdCatHighLevel[!duplicated(oecdCatHighLevel),]
oecdCatHighLevel <- oecdCatHighLevel[which(oecdCatHighLevel$OECD_level %in% c(1:6)),]
colnames(oecdCatHighLevel)[1] <- "OECD_research_area_high"
oecdCat <- oecdCat %>% left_join(oecdCatHighLevel, by="OECD_level")
oecdCat <- oecdCat[!duplicated(oecdCat[,c("research_area_upper","OECD_research_area_high")]),]
oecdCat <- oecdCat %>% arrange("OECD_level")

research_area_tab %>%
  head() %>%
  mutate(research_area_upper = toupper(research_area))%>%
  left_join(oecdCat, by="research_area_upper")

years <- unique(research_area_df$year)
for(y in 1:length(years)){
  # Tabulate appearances of each unique research area for that year
  tempList <- strsplit(research_area_df$research_areas[research_area_df$year == years[y]],split = "; ")
  tempTab <- as.data.frame(table(unlist(lapply(tempList, unique))))
  # match to get to OCED level
  matches <- sapply(toupper(tempTab$Var1), function(x) grep(x, oecdCat$research_area_upper))
  tempTab$oecd_research_area <- oecdCat$OECD_research_area_high[
    sapply(matches, function(x) ifelse(length(x)==0, NA, min(x)))]
  
  # If no match, separate by "&" symbol and look for individual matches
  # Start by separating dataframes to only investigate NAs
  noMatch <- subset(tempTab, is.na(oecd_research_area))
  tempTab <- subset(tempTab, !is.na(oecd_research_area))
  # Get new research areas by splitting at &
  newRAs <- unlist(strsplit(as.character(noMatch$Var1),split='&',fixed=TRUE))
  newRAs <- trimws(newRAs, which = c("both"))
  # Match elongated dataframe with old frequencies
  matches <- sapply(newRAs, function(x) grep(x, noMatch$Var1))
  newFreq <- unlist(sapply(matches, function(x) sum(noMatch$Freq[x], na.rm=T)))
  noMatch <- data.frame(
   Var1 = newRAs,
   Freq = newFreq
  )
  rownames(noMatch) <- NULL
  noMatch <- subset(noMatch, Var1 != "Science") # Too broad
  # Look for OECD match
  matches <- sapply(toupper(noMatch$Var1), function(x) grep(x, oecdCat$research_area_upper))
  if(sum(sapply(matches, function(x) length(x))) == 0){next} # if no matches, continue
  noMatch$oecd_research_area <- oecdCat$OECD_research_area_high[
    sapply(matches, function(x) ifelse(length(x)==0, NA, min(x)))]
  noMatch <- na.omit(noMatch)
  
  # Bind dataframes back together and Join
  tempTab <- rbind(tempTab, noMatch)
  tempTab$year <- years[y]
  if(y==1)
    research_area_tab <- tempTab
  else
    research_area_tab <- rbind(research_area_tab, tempTab)
}
# rename columns
research_area_tab$year <- as.Date(paste0(research_area_tab$year,"-01-01"))
research_area_tab <- research_area_tab%>% rename(research_area = Var1, n_articles = Freq)
research_area_tab$oecd_research_area <- factor(
  research_area_tab$oecd_research_area, 
  levels = oecdCatHighLevel$OECD_research_area_high[order(oecdCatHighLevel$OECD_level)]
)

# # Find which research areas are in the higher percentile
# low_research_areas <- research_area_tab %>%
#   group_by(research_area) %>%
#   summarise(n_articles=sum(n_articles)) %>%
#   ungroup()%>%
#   arrange(desc(n_articles)) %>%
#   slice(1:10) %>%
#   arrange(research_area)
# raLevels <- as.character(low_research_areas$research_area)
# # Format the research area factor to just the highest levels and group others into "Other"
# research_area_tab <- research_area_tab %>%
#   filter(year < as.Date("2023-01-01")) %>%
#   mutate(research_area = ifelse(research_area %in% raLevels,
#                                 as.character(research_area), "Other")) %>% 
#   mutate(research_area = factor(research_area, levels = c(raLevels, "Other")))

# Calculate as a proportion of total articles
yearlyTotal <- research_area_tab %>%
  group_by(year) %>%
  summarise(yearlyTotal = sum(n_articles))

research_area_tab <- research_area_tab %>%
  merge(yearlyTotal, by="year")%>%
  mutate(proportion = n_articles/yearlyTotal)


## Plotting
annualResearchAreaProportion_ggp <- research_area_tab %>%
  ggplot()+
  geom_col(aes(x=year, y=proportion, fill=research_area),
           position = "stack")+
  scale_x_date(limits = c(as.Date("1990-01-01"),as.Date("2022-01-01")))+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Proportional contribution",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  

annualResearchAreaProportion_OECD_ggp <- research_area_tab %>%
  ggplot()+
  geom_col(aes(x=year, y=proportion, fill=oecd_research_area),
           position = "stack")+
  scale_x_date(limits = c(as.Date("1990-01-01"),as.Date("2022-01-01")))+
  #scale_fill_brewer(type = "qual",palette = "Paired",labels = scales::label_wrap(10))+
  labs(y="Proportional contribution",x="Year",
       fill= "OECD\nResearch\narea")+
  theme_classic()+
  theme(legend.position = "bottom")

## PLOT 3: Map of affiliation -------------------------------------------

## DATA FORMATTING
world_map <- map_data("world")

# look for matches between each full affiliatoin and a region in the world map
affiliation_df$region <- rep(NA, nrow(affiliation_df))
regions <- unique(world_map$region)
for(r in 1:length(regions)){
  matches <- grepl(regions[r], affiliation_df$affiliation,ignore.case = TRUE)
  affiliation_df$region[matches] <- regions[r]
}
sum(is.na(affiliation_df$region)) # no matches - 10678
sum(!is.na(affiliation_df$region)) # compared to # of matches - 46911

# Calculate the number of affiliations for each region
library(ggthemes)
affiliation_df_tab <- affiliation_df %>%
  group_by(region) %>%
  summarise(n_articles = n()) %>%
  right_join(world_map %>% filter(! long > 180), by = "region")


ggplot(data=affiliation_df_tab, aes(long, lat, group=group, fill = n_articles))+
  geom_polygon()+
  scale_fill_viridis_c()+
  coord_map("moll") 

lout = 100
bb <- data.frame(
  long = c(seq(-180,180,length.out = lout),
           rep(180, lout),
           seq(180,-180, length.out = lout),
           rep(-180, lout)),
  lat = c(rep(-90, lout),
          seq(-90,90,length.out = lout),
          rep(90, lout),
          seq(90,-90, length.out = lout))
)
bb$group <- paste("boundingBox")


affiliationMap_ggp <- ggplot()+
  geom_polygon(data = affiliation_df_tab, aes(long, lat, group=group, fill = n_articles))+
  geom_polygon(data = bb, aes(long, lat, group=group), col="black", fill = "transparent")+
  scale_fill_viridis_c(name = "Number of articles")+
  coord_map("moll") +
  theme_map()+
  theme(legend.position = "bottom", legend.key.width = unit(0.5, units = "in"))


# Another option for improvement could be this:
# https://seethedatablog.wordpress.com/2016/12/23/r-simple-world-map-robinson-ggplot/
mat <- rbind(c(1,3),c(2,2))

pdf(here::here("figures/main/annual-trends-research-area-affiliation.pdf"),
    width = 10, height=6)
gridExtra::grid.arrange(annualTrend_ggp2+ggtitle("a."), annualResearchAreaProportion_ggp+ggtitle("c.")+guides(fill = guide_legend(ncol = 2, byrow = TRUE)), affiliationMap_ggp+ggtitle("b."),
          layout_matrix = mat)
while(!is.null(dev.list())) dev.off()


## NOTE that the counts of the branches are conditional on predicted relevance mean prediction, otherwise the mitigation mean line surpasses the mean line of all the articles and it looks confusing

```



```{r Save Annual trends research area and oro_type barplot multipanel}
## LOAD DATA
## Read in the predictions for oro_type
# Note these predictions were made using the upper confidence limit for ORO_branch
oroAnyFiles <- dir(here::here("data/raw-data/coding-predictions"))
oroAnyFiles <- oroAnyFiles[grep("oro_any", oroAnyFiles)]
predOROAnyList <- list()
for(i in 1:length(oroAnyFiles)){
  temp <- readr::read_csv(
    here::here("data/raw-data/coding-predictions",oroAnyFiles[i]),
    show_col_types = FALSE
  )
  colNames <- colnames(temp)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(temp) <- colNames
  temp$oro_branch <- gsub("oro_any_","",oroAnyFiles[i])
  temp$oro_branch <- gsub("_predictions.csv","",temp$oro_branch)
  predOROAnyList[[i]] <- temp
}

## Make a dataframe of the predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  boundaries <- c("upper","mean","lower")
  for(b in 1:length(boundaries)){
    temp_b <- cbind(temp[,1], temp$oro_branch, temp[grep(boundaries[b], colnames(temp))])
    colnames(temp_b)[2]<- "oro_branch"
    colnames(temp_b) <- gsub(paste0("_",boundaries[b]),"", colnames(temp_b))
    temp_b <- reshape2::melt(temp_b, variable.name = "oro_any", value.name = boundaries[b], 
                             id.vars = c("analysis_id","oro_branch"))
    if(b==1){
      temp_melt <- temp_b
    }else{
      temp_melt <- merge(temp_melt, temp_b, by=c("analysis_id","oro_branch","oro_any"))
    }
  }
  if(i==1)
    predOROAny <- temp_melt
  else
    predOROAny <- rbind(predOROAny, temp_melt)
}
rm(predOROAnyList)


## BARPLOT OF NUMBER OF ARTICLES PREDICTED RELEVANT FOR EACH TYPE
temp <- predOROAny %>%
  mutate(oro_branch = factor(oro_branch,
                             levels = c("mitigation","nature","societal"),
                             labels = c("Mitigation","Natural resilience","Societal adaptation"))) %>%
  group_by(oro_branch, oro_any)%>%
  summarise(n_mean = sum(0.5 <= mean),
            n_lower = sum(0.5 <= lower),
            n_upper = sum(0.5 <= upper))%>%
  arrange(oro_branch,n_mean)%>%
  ungroup()%>%
  mutate(valueOrder = as.factor(row_number()))
  
numberOROTypeBar_ggp <- ggplot(temp, aes(x=valueOrder, y=n_mean, fill = oro_branch))+
  geom_col()+
  geom_errorbar(aes(ymin = n_lower, ymax = n_upper), width=.2)+
  geom_text(aes(label = n_mean), nudge_y = 1000, size=3, col="red")+ #nudge_x=0.2,
  scale_fill_manual(name = "ORO Branch", values = as.vector(branchPal))+
  scale_x_discrete(limits = levels(temp$valueOrder), 
                    labels = gsub("_"," ", temp$oro_any[order(temp$valueOrder)]))+
  #ylim(c(0, max(temp$n_upper)+2000))+
  labs(y="Articles predicted relevant (n)")+
  theme(
    panel.background = element_rect(fill="white",colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_text(angle=45, hjust=1),
    axis.title.x = element_blank()
  )


mat <- rbind(c(1,3),c(2,2))

pdf(here::here("figures/main/annual-trends-research-area-nType.pdf"),
    width = 10, height=8)
gridExtra::grid.arrange(
  annualTrend_ggp2+ggtitle("a.")+
    theme(plot.margin = unit(c(0.1,0,2,0.1), "cm"),legend.position = "bottom")+
    guides(color = guide_legend(nrow = 2, byrow = TRUE)),
  annualResearchAreaProportion_ggp+ggtitle("c.")+guides(fill = guide_legend(ncol = 2, byrow = TRUE)),
  numberOROTypeBar_ggp+ggtitle("b.")+theme(legend.position = "bottom"),
  
  layout_matrix = mat, heights = c(5,3), widths = c(4.5,5.5))
while(!is.null(dev.list())) dev.off()

```

```{r Save annual trends oecd research area oro_type barplot and affiliation map multipanel}

oro_legend <- ggpubr::get_legend(
  annualTrend_ggp2+
    scale_color_manual(values = c("black",as.vector(branchPal)),
                       labels = c("All branches","Mitigation","Natural resilience",
                                  "Societal adaptation"),
                       name = "ORO branch")+
    theme(legend.position = "bottom",
          legend.key.width = unit(0.5,"in"),
          legend.text = element_text(size = 12),
          legend.title = element_text(size = 14))+
    guides(color = guide_legend(nrow = 1, byrow = TRUE, override.aes = list(linewidth = 10)))
  )
oro_legend <- ggpubr::as_ggplot(oro_legend)


while(!is.null(dev.list())) dev.off()

mat <- rbind(c(1,2),
             c(5,5),
             c(3,4))

pdf(here::here("figures/main/annual-trends-oecd-research-area-oroType-affiliation-map.pdf"), width = 10, height=10)
gridExtra::grid.arrange(
  
  annualTrend_ggp2+
    ggtitle("a.")+
    scale_y_continuous(label = scales::unit_format(scale = 0.001,unit = "K"))+
    theme(plot.margin = unit(c(0.1,0,2.5,0.1), "cm"),legend.position = "none"), #"bottom" 
    #guides(color = guide_legend(nrow = 2, byrow = TRUE)),
  
  numberOROTypeBar_ggp+
    ggtitle("b.")+
    scale_y_continuous(label = scales::unit_format(scale = 0.001,unit = "K"))+
    theme(legend.position = "none",
          plot.margin = unit(c(0,0.1,0,0.1), "cm")), # "bottom
  
  annualResearchAreaProportion_OECD_ggp+
    scale_fill_brewer(type = "qual",palette = "Paired",labels = scales::label_wrap(15))+
    ggtitle("c.")+
    theme(plot.margin = unit(c(0.8,0,0,0.1), "cm"))+
    guides(fill = guide_legend("",nrow = 2, byrow = TRUE)),
  
  affiliationMap_ggp+
    ggtitle("d.")+
    theme(plot.margin = unit(c(0,0,1.5,0), "cm"))+
    guides(fill = guide_colourbar("Number\nof articles")),
  
  oro_legend,
  
  layout_matrix = mat, heights = c(4.5,0.5,4), widths = c(5,5))

while(!is.null(dev.list())) dev.off()

```


# 2. Verifying model fit

```{r performance of DistilBERT vs SVM, echo=FALSE, fig.cap="Performance of DistilBERT vs SVM", out.width = '100%'}
knitr::include_graphics(here::here("figures/supplementary/relevance-predictions-SVM-vs-DistilBERT.png"))
```

```{r violin plots of predicted relevance for seen articles}

## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 
screens <- tbl(all_db, "all-screen-results_screenExcl-codeIncl") %>%
   select(duplicate_id, sample_screen, include_screen)

df <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  inner_join(screens, by = "duplicate_id") %>%
  collect()

summary(df)

# disconnect database
dbDisconnect(all_db)



## PLOT
relevancePredictionsSeenViolin_ggp <- df %>%
  select(sample_screen, include_screen, relevance_lower, relevance_mean, relevance_upper) %>%
  #filter(!(sample_screen == "test list" & include_screen == 0)) %>%
  mutate(sample_screen = factor(ifelse(sample_screen == "test list", "Test list","Other"),
                           levels = c("Test list","Other")),
         include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  mutate(estimate = factor(estimate, 
                           labels = c(paste0(c("Lower","Mean","Upper"), "\nestimate"))))%>%
  
  ggplot(aes(x=include_screen, y=value, fill = sample_screen)) +
  facet_wrap(vars(estimate))+
  geom_violin(trim=TRUE, aes(colour = sample_screen), linewidth = 0.25)+
  #geom_text(data=tabText, aes(label = `predicted number`), nudge_x = 0.3, size=3, col="red")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  labs(fill = "Sample method", colour = "Sample method")+
  labs(x="Reviewer decision", y = "Predicted relevance")+
  theme_bw()+
  theme(
    legend.position = "bottom"
  )


ggsave(here::here("figures/supplementary/relevancePredictions_seen_violin.pdf"), 
       plot = relevancePredictionsSeenViolin_ggp,
       width = 7, height = 4, units="in")
```



Resolving differences between ORO branches violin plot


Resolving differences between ORO type violin plot


# Heatmaps of interactions

Heatmap of ORO type by marine system

Heatmap of ORO type by ecosystem_type

Heatmap of outcomes (mitigation, adapt to threat human or natural) by ORO type

Heatmap of outcomes by ecosystem_type

Heatmap of Ecosystem type x blue carbon category?

Data_type?


# Geoparsing results

Sankey Diagram of First Author Affiliation compared to where research is conducted 

Maps of # articles by ORO type? ie. one panel with one map/ORO type 



