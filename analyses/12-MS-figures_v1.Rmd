---
title: "12-MS-figures_v1"
author: "Devi Veytia"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Set up


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(ggplot2)

```

```{r database filepath}
all_db_fp <- here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite")

```

```{r colourpalette}
branchPal <- c("Mitigation" = "#35a7d9","Nature" = "forestgreen", "Societal"="#7670a8")

```




# 1. Summarise the corpus


```{r PRISMA diagram}

## SQLITE CALCULATIONS

# connect to db
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE) 

# Get a table with all the references and calculate:
# number retreived from each source
# number of unseen articles
allrefs <- tbl(all_db,"allrefs_join") 
databaseNum <- allrefs%>%group_by(source_database)%>% summarise(n=n()) %>% as.data.frame() 
naAbstracts <- allrefs%>%filter(is.na(abstract))%>% summarise(n=n()) %>% as.data.frame()

# Get a table of all the de-duplicated references 
dedups <- tbl(all_db, "uniquerefs") 
nUnique <- dedups %>%
  filter(!is.na(abstract)) %>%
  summarise(n=n())%>%
  as.data.frame()

# Get a table of all those predicted to be relevant
predRel <- tbl(all_db, "predRel2")
nInclude <- predRel %>% 
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)) %>%
  as.data.frame()

# Disconnect the database
dbDisconnect(all_db)



## PLOT THE PRISMA DIAGRAM


PRISMAFlow <- c(
  paste("START_PHASE:", 
        format(databaseNum$n[databaseNum$source_database == "Web Of Science"], format = "d", big.mark=","),
        "articles from WOS"),
  paste("START_PHASE:", format(databaseNum$n[databaseNum$source_database == "Scopus"], 
                               format = "d", big.mark=","),
        "articles from Scopus"),
  paste(format(sum(databaseNum$n), format = "d", big.mark=","),"articles in total"),
  paste("EXCLUDE_PHASE:", format(naAbstracts, format = "d", big.mark=","),"abstracts removed as NA"),
  paste(format(sum(databaseNum$n)-naAbstracts, format = "d", big.mark=","),"eligible articles"),
  paste("EXCLUDE_PHASE:",
        format(sum(databaseNum$n)-naAbstracts-nUnique, format = "d", big.mark=","),"duplicates removed"),
  paste(format(nUnique, format = "d", big.mark=","),"unique articles"),
  paste("EXCLUDE_PHASE:", format(nUnique-nInclude[3], format = "d", big.mark=","),"articles excluded"),
  paste(format(nInclude[3], format = "d", big.mark=","),"articles predicted relevant (upper)")
)

w <- 12
h <- w*0.7

pdf(here::here("figures/supplementary/prisma-diagram.pdf"), width = w, height = h)
PRISMAFlowChart <- metagear::plot_PRISMA(PRISMAFlow, 
                                         design = c(E = "lightcoral", flatArrow = TRUE),
                                         excludeDistance = 0.8, colWidth = 50)
dev.off()

```

```{r performance of DistilBERT vs SVM, echo=FALSE, fig.cap="Performance of DistilBERT vs SVM", out.width = '100%'}
knitr::include_graphics(here::here("figures/supplementary/relevance-predictions-SVM-vs-DistilBERT.png"))
```

```{r violin plots of predicted relevance for seen articles}

## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 
screens <- tbl(all_db, "all-screen-results_screenExcl-codeIncl") %>%
   select(duplicate_id, sample_screen, include_screen)

df <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  inner_join(screens, by = "duplicate_id") %>%
  collect()

summary(df)

# disconnect database
dbDisconnect(all_db)



## PLOT
relevancePredictionsSeenViolin_ggp <- df %>%
  select(sample_screen, include_screen, relevance_lower, relevance_mean, relevance_upper) %>%
  #filter(!(sample_screen == "test list" & include_screen == 0)) %>%
  mutate(sample_screen = factor(ifelse(sample_screen == "test list", "Test list","Other"),
                           levels = c("Test list","Other")),
         include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  mutate(estimate = factor(estimate, 
                           labels = c(paste0(c("Lower","Mean","Upper"), "\nestimate"))))%>%
  
  ggplot(aes(x=include_screen, y=value, fill = sample_screen)) +
  facet_wrap(vars(estimate))+
  geom_violin(trim=TRUE, aes(colour = sample_screen), linewidth = 0.25)+
  #geom_text(data=tabText, aes(label = `predicted number`), nudge_x = 0.3, size=3, col="red")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  labs(fill = "Sample method", colour = "Sample method")+
  labs(x="Reviewer decision", y = "Predicted relevance")+
  theme_bw()+
  theme(
    legend.position = "bottom"
  )


ggsave(here::here("figures/supplementary/relevancePredictions_seen_violin.pdf"), 
       plot = relevancePredictionsSeenViolin_ggp,
       width = 7, height = 4, units="in")
```


```{r Annual trend in number of documents}

## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs")
predRel <- tbl(all_db, "predRel2") 
predBranch <- tbl(all_db, "predBranch")

# Calculate the number of relevant predictions for each year
df <- predRel %>%
  left_join(dedups, by="analysis_id") %>%
  left_join(predBranch, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(n_lower = sum(0.5 <= relevance_lower),
            n_total = sum(0.5 <= relevance_mean),
            n_upper = sum(0.5 <= relevance_upper),
            n_mitigation = sum(0.5 <= Mitigation_mean & 0.5 <= relevance_mean),
            n_nature = sum(0.5 <= Nature_mean & 0.5 <= relevance_mean),
            n_societal = sum(0.5 <= Societal_mean & 0.5 <= relevance_mean)) %>%
  collect()

# Disconnect the database
dbDisconnect(all_db)


## Data formatting
df$year <- as.Date(paste0("01-01-", df$year), format = "%d-%m-%Y")
df <- df %>% filter(year < as.Date("01-01-2023", format = "%d-%m-%Y"))

df_melt <- reshape2::melt(df, measure.vars = c("n_total","n_mitigation","n_nature","n_societal"),
                     value.name = "mean_prediction", variable.name = "variable") %>%
  mutate(variable = factor(variable, levels = c("n_total","n_mitigation","n_nature","n_societal"),
                           labels = c("All articles","Mitigation","Natural resilience","Societal\nadaptation")))
  
# add log scale
df_log <- df %>%
  mutate(n_lower = log(n_lower), n_upper = log(n_upper), scale = "log")
df_log <- rbind(
  df %>% mutate(scale = "normal"),
  df_log
)
df_log$scale <- factor(df_log$scale,
                       levels = c("normal","log"),
                       labels = c("un-transformed","log-transformed"))

df_melt_log <- df_melt %>%
  mutate(mean_prediction = log(mean_prediction), scale = "log")
df_melt_log <- rbind(
  df_melt %>% mutate(scale = "normal"),
  df_melt_log
)
df_melt_log$scale <- factor(df_melt_log$scale, 
                            levels = c("normal","log"),
                            labels = c("un-transformed","log-transformed"))

# Model fit
mod <- lm(log(n_total)~year, data = df, subset = n_total > 0)
summary(mod)
pred_df <- data.frame(year = seq(min(df$year), max(df$year), by=1))
pred_y <- predict(mod, pred_df, interval = "confidence", type="response")
pred_df <- rbind(
  cbind(pred_df, as.data.frame(pred_y), data.frame(scale = rep("log-transformed", nrow(pred_df)))),
  cbind(pred_df, as.data.frame(exp(pred_y)), data.frame(scale = rep("un-transformed", nrow(pred_df))))
)
pred_df$scale <- factor(pred_df$scale, levels = c("un-transformed","log-transformed"))



formula_text <- data.frame(
  label = paste0("y == ", 
                 format(coef(mod)[1], scientific=TRUE, digits = 2), "*(",
                 format(coef(mod)[2], scientific=TRUE, digits = 2),"^x)"),
  year=as.Date("1932-01-01"),
  y=max(df$n_total),
  scale = factor("un-transformed", levels = c("un-transformed","log-transformed"))
)

## Plot
annualTrend_ggp <- ggplot()+
  facet_grid(scale ~., scales = "free_y")+
  geom_ribbon(data=df_log, aes(x=year, ymin = n_lower, ymax = n_upper), fill = "lightgrey")+
  geom_line(data = df_melt_log, aes(x=year, y=mean_prediction, col = variable), linewidth = 1)+
  geom_line(data = pred_df, aes(x=year, y=fit), col="red", linetype = "dashed", linewidth = 1)+
  geom_text(data=formula_text, aes(x=year, y=y, 
                                   label = label),parse=TRUE, color = "red")+
  geom_text(data=formula_text, aes(x=year, y=y-1000, 
                                   label = paste("R^2 == ",
                                                 format(summary(mod)$adj.r.squared, digits = 2))),
            color = "red", parse=TRUE)+
  scale_color_manual(values = c("black",as.vector(branchPal)), 
                     name = "Mean predicted\nrelevance")+
  labs(x="Year", y="Number of articles predicted relevant", 
       caption = "all terms are significant p << 0.05")+
  scale_x_date()+
  theme_classic()+
  theme(legend.position = "bottom")

annualTrend_ggp

ggsave(here::here("figures/main/annual-trends.pdf"),plot=annualTrend_ggp,
       width = 7, height=4.5, units="in")

```


```{r Annual trend by research area}
## SQLITE CALCULATIONS
# connect to db and load tables
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),all_db_fp, create=FALSE)
dedups <- tbl(all_db,"uniquerefs") %>% filter(!is.na(abstract))
predRel <- tbl(all_db, "predRel2") 
predBranch <- tbl(all_db, "predBranch")

# "research_areas", web_of_science_categories",
# Don't do "organization or" "funding" or "affiliation"  yet, wait to compare to geoparsing results

# Calculate the number of relevant predictions for each year
n_relevant <- predRel %>%
  left_join(dedups, by="analysis_id") %>%
  left_join(predBranch, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(n_lower = sum(0.5 <= relevance_lower),
            n_total = sum(0.5 <= relevance_mean),
            n_upper = sum(0.5 <= relevance_upper),
            n_mitigation = sum(0.5 <= Mitigation_mean & 0.5 <= relevance_mean),
            n_nature = sum(0.5 <= Nature_mean & 0.5 <= relevance_mean),
            n_societal = sum(0.5 <= Societal_mean & 0.5 <= relevance_mean)) %>%
  collect()

research_area_df <- predRel %>%
  filter(0.5 <= relevance_upper) %>%
  left_join(dedups, by="analysis_id") %>%
  filter(!is.na(research_areas)) %>%
  select(analysis_id, duplicate_id, research_areas, web_of_science_categories, year)%>%
  filter(!is.na(year)) %>%
  mutate(research_areas = eval(parse(text=gsub("\\", "", deparse(research_areas), fixed=TRUE))),
         first_research_area = gsub("\\;.*","", research_areas)) %>%
  collect()

# Disconnect the database
dbDisconnect(all_db)




## Data Formatting & Calculations

## For number of relevant articles
# year as date
n_relevant$year <- as.Date(paste0("01-01-", n_relevant$year), format = "%d-%m-%Y")
n_relevant <- n_relevant %>% filter(year < as.Date("01-01-2023", format = "%d-%m-%Y"))

# Melt by ORO branch
n_relevant_melt <- reshape2::melt(n_relevant, measure.vars = c("n_total","n_mitigation","n_nature","n_societal"),
                     value.name = "mean_prediction", variable.name = "variable") %>%
  mutate(variable = factor(variable, levels = c("n_total","n_mitigation","n_nature","n_societal"),
                           labels = c("All articles","Mitigation","Natural resilience","Societal\nadaptation")))

# Model fit
mod <- lm(log(n_total)~year, data = n_relevant, subset = n_total > 0)
pred_df <- data.frame(year = seq(min(n_relevant$year), max(n_relevant$year), by=1))
pred_y <- predict(mod, pred_df, interval = "confidence", type="response")
pred_df <- cbind(pred_df, as.data.frame(exp(pred_y)))
  
formula_text <- data.frame(
  label = paste0("y == ", 
                 format(coef(mod)[1], scientific=TRUE, digits = 2), "*(",
                 format(coef(mod)[2], scientific=TRUE, digits = 2),"^x)"),
  year=as.Date("1932-01-01"),
  y=max(n_relevant$n_total)
)



## Research areas

# tabulate the number of different research areas in a year and
# filter the tabulated results to just the high-sampled research areas

# tabulate
years <- unique(research_area_df$year)
for(y in 1:length(years)){
  tempList <- strsplit(research_area_df$research_areas[research_area_df$year == years[y]],split = "; ")
  tempTab <- as.data.frame(table(unlist(lapply(tempList, unique))))
  tempTab$year <- years[y]
  if(y==1)
    research_area_tab <- tempTab
  else
    research_area_tab <- rbind(research_area_tab, tempTab)
}
# rename columns
research_area_tab$year <- as.Date(paste0(research_area_tab$year,"-01-01"))
research_area_tab <- research_area_tab%>%
  rename(research_area = Var1, n_articles = Freq)
# Find which research areas are in the higher percentile
low_research_areas <- research_area_tab %>%
  group_by(research_area) %>%
  summarise(n_articles=sum(n_articles)) %>%
  arrange(desc(n_articles))
low_research_areas <- low_research_areas[1:10,] # take the top 10
low_research_areas <- low_research_areas %>%
  arrange(research_area)
# subset
research_area_tab <- research_area_tab %>%
  filter(research_area %in% low_research_areas$research_area &
           year < as.Date("2023-01-01"))
# Factor order of research area alphabetically
research_area_tab$research_area <- factor(research_area_tab$research_area,
                                          levels = low_research_areas$research_area)


## PLOT

## Annual trend in number of articles
annualTrend_ggp2 <- ggplot()+
  geom_ribbon(data=n_relevant, aes(x=year, ymin = n_lower, ymax = n_upper), fill = "lightgrey")+
  geom_line(data = n_relevant_melt, aes(x=year, y=mean_prediction, col = variable), linewidth = 1)+
  geom_line(data = pred_df, aes(x=year, y=fit), col="red", linetype = "dashed", linewidth = 1)+
  geom_text(data=formula_text, aes(x=year, y=y, 
                                   label = label),parse=TRUE, color = "red")+
  geom_text(data=formula_text, aes(x=year, y=y-1000, 
                                   label = paste("R^2 == ",
                                                 format(summary(mod)$adj.r.squared, digits = 2))),
            color = "red", parse=TRUE)+
  scale_color_manual(values = c("black",as.vector(branchPal)), 
                     name = "Mean predicted\nrelevance")+
  labs(x="Year", y="Articles predicted relevant (n)")+
  scale_x_date()+
  theme_classic()+
  theme(legend.position = "right")

annualTrend_ggp2

## Annual trend in the articles by research area
annualResearchArea_ggp <- ggplot()+
  geom_col(data = research_area_tab, aes(x=year, y=n_articles, fill=research_area),
           position = "stack")+
  scale_x_date()+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Articles predicted relevant (n)",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  

annualResearchArea_ggp


# Calculate as a proportion
yearlyTotal <- research_area_tab %>%
  group_by(year) %>%
  summarise(yearlyTotal = sum(n_articles))

annualResearchAreaProportion_ggp <- research_area_tab %>%
  merge(yearlyTotal, by="year")%>%
  mutate(proportion = n_articles/yearlyTotal)%>%
  
  ggplot()+
  geom_col(aes(x=year, y=proportion, fill=research_area),
           position = "stack")+
  scale_x_date()+
  scale_fill_brewer(type = "qual",palette = "Paired")+
  labs(y="Proportional contribution",x="Year",
       fill= "Research\narea")+
  theme_classic()+
  theme(legend.position = "right")
  




pdf(here::here("figures/main/annual-trends-research-area.pdf"),
    width = 7, height=6)
egg::ggarrange(annualTrend_ggp2, annualResearchAreaProportion_ggp,
          nrow=2, ncol=1, newpage = FALSE, labels = c("a.","b."))
dev.off()
```


Maybe it would be good to subset the research area trend by branch as well?? Can do temporal or as pie charts?





# 2. Intervention breakdown





