---
title: "Database-description-coauthors"
author: "Devi Veytia"
date: "2023-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning =FALSE)
```


```{r set up}

## Load libraries
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(reticulate)
library(ggplot2)


```


# Background

Here I present the preliminary predictions from the machine learning model for (1) relevance, and (2) metadata coding.

To achieve this, first our search string was used to retrieve a literature from two citation indexed databases: Web of Science and Scopus. These articles were then de-duplicated based on title and year matching. 

From this de-duplicated corpus, a subset of articles were screened for inclusion or exclusion:
```{r}
screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screenSummary <- table(screens2$include_screen)
names(screenSummary) <- c("exclude","include")
print(screenSummary)

rm(screens2, screenSummary)
```

To generate predicted relevance for all "unseen" articles, a machine learning binary classification model was trained and tested on the "seen" corpus summarized above (see Section 1).

The articles that were labelled as inclusions from the screened subset above were then coded for metadata variables. For each variable, the labels decided by a human were used to train multi-label or binary classifiers (using the former if the variable allowed multiple choice values) analagous to the method used above. After the model training and selection, the best performing model was used to calculate predicted relevance for the "unseen" articles with a mean predicted relevance greater or equal to 0.5 (see Section 2).

# Screening

```{r read in files for relevance predictions}

## READ IN 
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

predRel <- tbl(all_db, "predRel2") %>% collect() 
dedups <- tbl(all_db, "uniquerefs") %>% 
  select(analysis_id, duplicate_id, title)

screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screens2 <- screens2 %>%
  select(duplicate_id, sample_screen, include_screen)


## FORMAT
predRelMeta <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  collect()
predRelMeta <- predRelMeta %>% full_join(screens2, by = "duplicate_id")

# disconnect
dbDisconnect(all_db)

```

In total, the model predicts the following numbers of relevant articles for the mean +/- 1 standard deviation:
```{r summaries of numbers of articles across all error margins}

predRel %>%
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)
  )

```


However, a lot of information about model performance can be gained by comparing the distribution of predicted relevance (a continuous value from 0 to 1 with articles greater or equal to 0.5 being considered an inclusion) to screening decisions made by human reviewers (binary outcome = inclusion vs exclusion). Ideally, high predicted relevance will be seen for articles which were included, and vice versa. 

Of the test list articles (our list of benchmark/exemplary articles), was was the predicted relevance? Overall, both mean and upper estimate (+1 standard deviation) demonstrate 100% recall for the test list articles (i.e. no false negatives).
```{r fig.cap="predicted relevance for test list articles", out.width="80%", fig.asp=0.67, fig.align='center'}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  ggplot(aes(x = estimate, y = value, fill = estimate))+
  geom_violin(trim=TRUE)+
  geom_hline(yintercept = 0.5)+
  scale_fill_discrete(guide="none")+
  labs(x="estimate", y = "Predicted relevance")+
  theme_bw()
  
```

Note that in the "lower" is there one False negative:
```{r}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  group_by(include_screen, estimate) %>%
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) 

```



This can be extended to all the articles screened. The red text labels show the counts of articles which fall into each possible outcome with a binary classification (true positive, false positive, true negative, false negative). Overall the model performs fairly well with high accuracy by predicting true positives and true negatives. While taking the upper limit minimizes the number of false negatives, it also performs more poorly on precision (more false positives). 


```{r tabulate the number of included and excluded screens fall on either side of the 0.5 threshold}
tabText <- predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  group_by(include_screen, estimate) %>% 
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) %>% 
  ungroup()%>%
  reshape2::melt(
    measure.vars = c("pred_include","pred_exclude"),
    variable.name = "prediction_include", value.name = "predicted number"
  ) %>%
  mutate(value = ifelse(prediction_include == "pred_include", 0.75,0.25))%>%
  mutate(prediction_include = factor(
    prediction_include, levels = c("pred_include","pred_exclude"), 
    labels = c("include","exclude")
  ))

```

```{r fig.cap= "The distribution of predicted relevance for all articles screened by model estimate", out.width="90%", fig.asp=0.7, fig.align='center'}

predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  
  ggplot(aes(x= include_screen, y=value, fill = include_screen)) +
  facet_wrap(vars(estimate))+
  geom_violin(trim=TRUE)+
  geom_text(data=tabText, aes(label = `predicted number`), nudge_x = 0.3, size=3, col="red")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  scale_fill_discrete(guide="none")+
  labs(x="Reviewer decision", y = "Predicted relevance")+
  theme_bw()

```


In the Callaghan et al 2021 paper, the upper estimate of relevance was used as this is the most conservative. For now, I suggest staying with this precedent and we can always truncate at the mean estimate if we want to.



# Coding

## Overview: Tabulation of articles across all variables


FILL HERE

## ORO type

Since our ORO typology is hierarchical (see below), an incremental approach was taken to predict the type of ORO mentioned in the abstract. First the "branch" of the ORO was predicted (i.e. Mitigation, Natural resilience and Societal adaptation). Given the branch predicted (e.g. Societal), then the more granular type of ORO was predicted (Socio-institutional or Built infrastructure). 

```{r}
knitr::include_graphics(here::here("figures/external/ORO-typology.png")) 
```



### ORO branch

```{r read in the oro_branch labels}
## READ IN

# ORO branch
predBranch <- readr::read_csv(
  here::here("data/raw-data/coding-predictions/oro_branch_predictions.csv")
)

colnames(predBranch) <- c("analysis_id",
                          paste("Mitigation", c("mean","std","lower","upper"), sep="_"),
                          paste("Nature", c("mean","std","lower","upper"), sep="_"),
                          paste("Societal", c("mean","std","lower","upper"), sep="_")
                          )

# Relevance predictions
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
predRel <- tbl(all_db, "predRel2") %>% collect()


# Coding decisions
codebookCombinedSimplifiedMore <- tbl(all_db, "allCodingSimplifiedVariablesMore") %>% collect()


dbDisconnect(all_db)

```

```{r}
# if an inclusion but none of the branches relevant, then unclear
predBranch$Unclear <- ifelse(predBranch$Mitigation_mean < 0.5 &
                       predBranch$Nature_mean < 0.5 &
                       predBranch$Societal_mean < 0.5, 1,0)

# merge with relevance predictions for inclusions, branch predictions and actual coding decisions
df <- predRel %>%
  filter(0.5 <= relevance_mean)%>%
  left_join(predBranch, by = "analysis_id") %>%
  left_join(codebookCombinedSimplifiedMore %>%
               select(analysis_id, title, oro_branch.Mitigation, 
                      oro_branch.Nature, oro_branch.Societal))

```

Firstly, of all the articles predicted relevant at the screeing stage, how many articles are predicted for each ORO branch? to be relevant for each ORO branch? Note the unclear option is chosen for any article included at the screening stage, but without a relevance prediction for any particular branch.

```{r tabulate oro_branch}
predBranch[,c(grep("upper", colnames(predBranch)), grep("Unclear", colnames(predBranch)))]%>%
  reshape2::melt(variable.name = "ORO_branch") %>% 
  group_by(ORO_branch)%>%
  summarise(n_articles = sum(0.5 <= value))
```


```{r}

# melt based on seen decisions for each unit of publication
branchDf <- df %>%
  select(analysis_id, relevance_mean, Mitigation_mean, Nature_mean, Societal_mean,
         oro_branch.Mitigation, oro_branch.Nature,oro_branch.Societal)%>%
  reshape2::melt(
    measure.vars = c("oro_branch.Mitigation","oro_branch.Nature","oro_branch.Societal"),
    #measure.vars = c("Mitigation_mean","Nature_mean","Societal_mean"),
    value.name = "seen_branch", variable.name = "oro_branch") %>%
  mutate(oro_branch = factor(oro_branch, 
                             c("oro_branch.Mitigation","oro_branch.Nature",
                               "oro_branch.Societal"),
                             c("Mitigation","Nature","Societal")))%>%
  mutate(seen_branch = ifelse(seen_branch==1, as.character(oro_branch), NA)) %>%
  select(-c(oro_branch)) %>%
  filter(!is.na(seen_branch))

# add a column for the predictions for the branch that was picked when seen
pred <- rep(NA, nrow(branchDf))
for(i in 1:length(pred)){
  if(branchDf$seen_branch[i] == "Mitigation"){
    pred[i] <- branchDf[i, grep("Mitigation", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Nature"){
    pred[i] <- branchDf[i, grep("Natur", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Societal"){
    pred[i] <- branchDf[i, grep("Societal", colnames(branchDf), ignore.case = T)]
  }else{
    pred[i] <- NA
  }
}
branchDf$seen_branch_pred <- pred; rm(pred)
```

```{r fig.cap = "what is the predicted relevance for the branch that was chosen by the reviewer?", out.width="90%", fig.asp=0.7, fig.align='center', eval=FALSE}

ggplot(branchDf, aes(x=seen_branch, y=seen_branch_pred, fill = seen_branch))+
  labs(x = "Reviewer branch classification",
       y= "Predicted relevance for classified branch")+
  geom_violin(trim=TRUE)+
  scale_fill_discrete(guide="none")+
    theme_bw()
```



```{r fig.cap="facetted by the branch seen, which branches were predicted relevant?", out.width="90%", fig.asp=0.7, fig.align='center'}

branchDf %>%
  select(analysis_id, Mitigation_mean, Societal_mean, Nature_mean, seen_branch) %>%
  reshape2::melt(measure.vars = c("Mitigation_mean", "Societal_mean", "Nature_mean"),
                 variable.name = "predictedBranch", value.name = "predictedRelevance")%>% 
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedBranch, y= predictedRelevance, fill=predictedBranch))+
  geom_violin()+
  facet_wrap(vars(seen_branch))+
  labs(y="predicted relevance", x = "predicted ORO branch", fill = "predicted ORO branch",
       caption="Facetted by the branch seen, what branches were predicted to be relevant?")+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```



## ORO type

```{r clear r working environment}
rm(list = ls())
```

```{r read in oro_any predictions}

# Note these predictions were made using the upper confidence limit for ORO_branch
oroAnyFiles <- dir(here::here("data/raw-data/coding-predictions"))
oroAnyFiles <- oroAnyFiles[grep("oro_any", oroAnyFiles)]

predOROAnyList <- list()

for(i in 1:length(oroAnyFiles)){
  temp <- readr::read_csv(
    here::here("data/raw-data/coding-predictions",oroAnyFiles[i]),
    show_col_types = FALSE
  )
  colNames <- colnames(temp)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(temp) <- colNames
  
  temp$oro_branch <- gsub("oro_any_","",oroAnyFiles[i])
  temp$oro_branch <- gsub("_predictions.csv","",temp$oro_branch)
  
  
  predOROAnyList[[i]] <- temp
  
}


## Make a dataframe of just the upper predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  temp <- temp[grep("upper", colnames(temp))]
  colnames(temp) <- gsub("_upper","", colnames(temp))
  temp <- reshape2::melt(temp, variable.name = "oro_any")
  temp <- cbind(predOROAnyList[[i]][,c("analysis_id","oro_branch")], temp)
  
  if(i==1)
    predOROAny <- temp
  else
    predOROAny <- rbind(predOROAny, temp)
}

rm(predOROAnyList)

## Read in other metadata
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

dedups <- tbl(all_db, "uniquerefs") %>% 
  select(analysis_id, duplicate_id, title) %>%
  collect()

codebookCombinedSimplifiedMore <- tbl(all_db, "allCodingSimplifiedVariablesMore") %>% collect()


## FORMAT
predOROAnyMeta <- dedups %>%
  inner_join(predOROAny, by = "analysis_id") 
rm(dedups)

# disconnect
dbDisconnect(all_db)

```

How many articles are predicted relevant for each category?

```{r fig.cap="number of articles relevant to each ORO type", out.width="90%", fig.asp=0.8, fig.align='center'}
temp <- predOROAny %>%
  group_by(oro_branch, oro_any)%>%
  summarise(n_articles = sum(0.5 <= value))%>%
  arrange(oro_branch,n_articles)%>%
  ungroup()%>%
  mutate(valueOrder = as.factor(row_number()))
  
ggplot(temp, aes(x=valueOrder, y=n_articles, fill = oro_branch))+
  geom_col()+
  geom_text(aes(label = n_articles), nudge_y = 2000, size=3)+
  scale_fill_discrete(name = "ORO Branch")+
  scale_x_discrete(limits = levels(temp$valueOrder), 
                    labels = gsub("_"," ", temp$oro_any[order(temp$valueOrder)]))+
  labs(y="Number predicted\nrelevant articles (upper)")+
  theme(
    panel.background = element_rect(fill="white",colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_text(angle=90, hjust=1),
    axis.title.x = element_blank()
  )
```


From the figure above, you can see an additional category added under the Natural resilience ORO branch: Conservation. This was trialed as a combination between Protection and Restoration, as often in the literature these options are used together as a broader conservation portfolio and may be difficult to distinguish. Here I compare the performance of these three labels (protection, restoration and conservation) against the decisions from the seen articles.

```{r}

# melt based on seen decisions for each unit of publication
predOROAnyDf <- predOROAnyMeta %>%
  filter(oro_any %in% c("Protection","Restoration","Conservation"))%>%
  reshape2::dcast(analysis_id~oro_any, value.var = "value") %>%
  left_join(codebookCombinedSimplifiedMore %>%
               select(analysis_id, oro_any.N_Protection,oro_any.N_Restoration))%>% 
  reshape2::melt(
    measure.vars = c("oro_any.N_Protection","oro_any.N_Restoration"),
    value.name = "seen_oro", variable.name = "oro_any") %>%
  mutate(oro_any = factor(oro_any, 
                             c("oro_any.N_Protection","oro_any.N_Restoration"),
                             c("Protection","Restoration")))%>%
  mutate(seen_oro = ifelse(seen_oro==1, as.character(oro_any), NA)) %>%
  select(-c(oro_any)) 
  
```



```{r fig.cap="facetted by the branch seen, which branches were predicted relevant?"}

predOROAnyDf %>%
  filter(!is.na(seen_oro))%>%
  reshape2::melt(measure.vars = c("Protection","Restoration","Conservation"),
                 variable.name = "predictedORO", value.name = "predictedRelevance")%>% 
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedORO, y= predictedRelevance, fill=predictedORO))+
  geom_violin()+
  facet_wrap(vars(seen_oro))+
  labs(y="Predicted relevance", x = "Predicted ORO type", fill = "predicted\nORO type")+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```


This indicates that for protection OROs, the model predicts a higher protection relevance than restoration, however the reverse is not true for restoration. Conservation is consistently high for both.



Extending this analysis to all the types of OROs (not including the Conservation label), this yeilds:

```{r}
# melt based on seen decisions for each unit of publication
# 1 row = 1 article x seen ORO combinations
oroAnyCols <- colnames(codebookCombinedSimplifiedMore)[grep("oro_any", colnames(codebookCombinedSimplifiedMore))]
oroAnyColsLabels <- gsub("oro_any..._","",gsub("oro_any.._","",oroAnyCols))
oroAnyColsLabels <- gsub("_"," ",oroAnyColsLabels)

predOROAnyDf <- predOROAnyMeta %>%
  reshape2::dcast(analysis_id~oro_any, value.var = "value") %>%
  left_join(codebookCombinedSimplifiedMore[c("analysis_id", oroAnyCols)])%>%
  reshape2::melt(
    measure.vars = oroAnyCols,
    value.name = "seen_oro", variable.name = "oro_any") %>%
  mutate(oro_any = factor(oro_any,levels = oroAnyCols,labels = oroAnyColsLabels))%>%
  mutate(seen_oro = ifelse(seen_oro==1, as.character(oro_any), NA)) %>%
  select(-c(oro_any)) 
  
```


```{r fig.cap="facetted by the oro type seen, which were predicted relevant?", out.width="100%", fig.asp=1.1}
# define colour scale
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

facetCol <- data.frame(
  seen_oro = factor(oroAnyColsLabels, oroAnyColsLabels),
  oroColor = gg_color_hue(length(oroAnyColsLabels))
)

# Plot
predOROAnyDf %>%
  select(-c(Conservation))%>%
  filter(!is.na(seen_oro))%>%
  reshape2::melt(id.vars = c("analysis_id", "seen_oro"),
                 variable.name = "predictedORO", value.name = "predictedRelevance")%>% 
  filter(!is.na(predictedORO))%>%
  mutate(predictedORO = factor(gsub("_"," ", predictedORO)),
         seen_oro = factor(gsub("_"," ",seen_oro)))  %>%

  ggplot()+
  geom_violin(aes(x=predictedORO, y= predictedRelevance, col=predictedORO),
              trim=TRUE, width=1.5, scale = "count")+
  facet_wrap(vars(seen_oro), labeller=label_wrap_gen(35))+
  geom_rect(data=facetCol, aes(xmin=-Inf, xmax=Inf, ymin=1.1, ymax=1.5,
                               fill=seen_oro))+
  coord_cartesian(clip = "off", ylim=c(0,1))+
  labs(y="Predicted relevance", x = "Predicted ORO type",
       guide = "Predicted\nORO type")+
  scale_fill_discrete(guide="none")+
  scale_colour_manual(guide="none", values = facetCol$oroColor, breaks = facetCol$seen_oro)+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "lightgrey"),
    #axis.text.x = element_blank(),
    strip.background = element_rect(fill=NA),
    axis.text.x = element_text(angle=90, hjust=1)
  )


```
