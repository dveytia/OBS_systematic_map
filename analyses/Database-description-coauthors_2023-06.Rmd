---
title: "Systematic Map database description"
author: "Devi Veytia"
date: "2023-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning =FALSE)
```


```{r set up}

## Load libraries
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(reticulate)
library(ggplot2)


```


# Background

Here I present the preliminary predictions from the machine learning model for (1) relevance, and (2) metadata coding.

To achieve this, first we used our search string to retrieve a literature from two citation indexed databases: Web of Science and Scopus. These articles were then de-duplicated based on title and year matching. 

From this de-duplicated corpus, a subset of articles were screened for inclusion or exclusion, with the following results:
```{r}
screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screenSummary <- table(screens2$include_screen)
names(screenSummary) <- c("exclude","include")

knitr::kable(screenSummary,col.names = c("Screening decision","Number of articles"))

rm(screens2, screenSummary)
```

To generate predicted relevance for all "unseen" articles, a machine learning binary classification model was trained and tested on the "seen" corpus summarized above (see "Screening" section). The model generates a prediction of relevance -- a value between 0 and 1, with 1 being relevant and 0 being irrelevant. Estimates for the mean, lower (mean - 1 std) and upper (mean + 1 std) are also generated. Following the approach of Callaghan et al 2021, we use the upper estimate for the relevance predictions which is the most conservative. A value of greater or equal to 0.5 would be considered an inclusion.

The articles that were labelled as inclusions from the screened subset above were then coded for metadata variables. For each variable, the labels decided by a human were used to train multi-label or binary classifiers (using the former if the variable allowed multiple choice values) analagous to the method used above(see "Coding" section).

```{r fig.cap="PRISMA diagram of the corpus",fig.width=12, fig.asp=0.6}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

allrefs <- tbl(all_db,"allrefs_join") 
databaseNum <- allrefs%>%group_by(source_database)%>% summarise(n=n())
databaseNum <- as.data.frame(databaseNum)
naAbstracts <- allrefs%>%filter(is.na(abstract))%>% summarise(n=n())
naAbstracts <- as.data.frame(naAbstracts)

dedups <- tbl(all_db, "uniquerefs") 
nUnique <- dedups %>%
  filter(!is.na(abstract)) %>%
  summarise(n=n())%>%
  as.data.frame()

predRel <- tbl(all_db, "predRel2")
nInclude <- predRel %>% 
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)) %>%
  as.data.frame()
  
dbDisconnect(all_db)

require(metagear)

PRISMAFlow <- c(
  paste("START_PHASE:", databaseNum$n[databaseNum$source_database == "Web Of Science"],
        "articles from WOS"),
  paste("START_PHASE:", databaseNum$n[databaseNum$source_database == "Scopus"],
        "articles from Scopus"),
  paste(sum(databaseNum$n),"articles in total"),
  paste("EXCLUDE_PHASE:", naAbstracts,"abstracts removed as NA"),
  paste(sum(databaseNum$n)-naAbstracts,"eligible articles"),
  paste("EXCLUDE_PHASE:",sum(databaseNum$n)-naAbstracts-nUnique,"duplicates removed"),
  paste(nUnique,"unique articles"),
  paste(nInclude[3],"articles predicted relevant"),
  paste("EXCLUDE_PHASE:", nUnique-nInclude[3],"articles excluded")
)


PRISMAFlowChart <- metagear::plot_PRISMA(PRISMAFlow, 
                                         design = c(E = "lightcoral", flatArrow = TRUE),
                                         excludeDistance = 0.8, colWidth = 50)

```


# Screening

```{r read in files for relevance predictions}

## READ IN 
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

predRel <- tbl(all_db, "predRel2") %>% collect() 
dedups <- tbl(all_db, "uniquerefs") %>% 
  filter(!is.na(abstract))%>%
  select(analysis_id, duplicate_id, title) %>%
  collect

screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screens2 <- screens2 %>%
  select(duplicate_id, sample_screen, include_screen)


## FORMAT
predRelMeta <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  collect()
predRelMeta <- predRelMeta %>% full_join(screens2, by = "duplicate_id")

# disconnect
dbDisconnect(all_db)

```

In total, the model predicts the following numbers of relevant articles for the mean +/- 1 standard deviation:
```{r summaries of numbers of articles across all error margins}

relevanceTab <- predRel %>%
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)
  )

knitr::kable(relevanceTab)

```


To dig further, a lot of information about model performance can be gained by comparing the distribution of predicted relevance to screening decisions made by human reviewers (binary outcome = inclusion vs exclusion). Ideally, high predicted relevance will be seen for articles which were included, and vice versa. 

Of the test list articles (our list of benchmark/exemplary articles), was was the predicted relevance? Overall, both mean and upper estimate (+1 standard deviation) demonstrate 100% recall for the test list articles (i.e. no false negatives).
```{r fig.cap="predicted relevance for test list articles", out.width="80%", fig.asp=0.5, fig.align='center'}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  mutate(estimate = gsub("_"," ", estimate))%>%
  ggplot(aes(x = estimate, y = value, fill = estimate))+
  geom_violin(trim=TRUE)+
  geom_hline(yintercept = 0.5)+
  scale_fill_discrete(guide="none")+
  labs(x="estimate", y = "Predicted relevance")+
  theme_bw()
  
```


```{r Note that for the "lower" estimate is there only one False negative, eval = FALSE}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  group_by(include_screen, estimate) %>%
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) 

```



This can be extended to all the articles screened, not just the test list articles. The red text labels show the counts of articles which fall into each possible outcome with a binary classification (true positive, false positive, true negative, false negative). Overall the model performs fairly well with high accuracy by predicting true positives and true negatives. While taking the upper limit minimizes the number of false negatives, it also performs more poorly on precision (more false positives). 


```{r tabulate the number of included and excluded screens fall on either side of the 0.5 threshold}
tabText <- predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  group_by(include_screen, estimate) %>% 
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) %>% 
  ungroup()%>%
  reshape2::melt(
    measure.vars = c("pred_include","pred_exclude"),
    variable.name = "prediction_include", value.name = "predicted number"
  ) %>%
  mutate(value = ifelse(prediction_include == "pred_include", 0.75,0.25))%>%
  mutate(prediction_include = factor(
    prediction_include, levels = c("pred_include","pred_exclude"), 
    labels = c("include","exclude")
  )) %>%
  mutate(estimate = gsub("_"," ", estimate))

```

```{r fig.cap= "The distribution of predicted relevance for all articles screened by model estimate", out.width="90%", fig.asp=0.5, fig.align='center'}

predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "estimate", value.name = "value"
  ) %>%
  mutate(estimate = gsub("_"," ",estimate))%>%
  
  ggplot(aes(x= include_screen, y=value, fill = include_screen)) +
  facet_wrap(vars(estimate))+
  geom_violin(trim=TRUE)+
  geom_text(data=tabText, aes(label = `predicted number`), nudge_x = 0.3, size=3, col="red")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  scale_fill_discrete(guide="none")+
  labs(x="Reviewer decision", y = "Predicted relevance")+
  theme_bw()

```




# Coding

This section will summarise and visualize the model predictions for each of the variables coded so far. At the end of the section will be a summary table providing the article counts (i.e. number of articles predicted relevant) for each variable.

## ORO Branch

Since our ORO typology is hierarchical (see below), an incremental approach was taken to predict the type of ORO mentioned in the abstract. First the "branch" of the ORO was predicted (i.e. Mitigation, Natural resilience and Societal adaptation). Given the branch predicted (e.g. Societal), then the more granular type of ORO was predicted (Socio-institutional or Built infrastructure). 

```{r}
knitr::include_graphics(here::here("figures/external/ORO-typology.png")) 
```




```{r read in the oro_branch labels}
## READ IN

# ORO branch
predBranch <- readr::read_csv(
  here::here("data/raw-data/coding-predictions/oro_branch_predictions.csv")
)

colnames(predBranch) <- c("analysis_id",
                          paste("Mitigation", c("mean","std","lower","upper"), sep="_"),
                          paste("Nature", c("mean","std","lower","upper"), sep="_"),
                          paste("Societal", c("mean","std","lower","upper"), sep="_")
                          )

# Relevance predictions
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
predRel <- tbl(all_db, "predRel2") %>% collect()


# Coding decisions
codebookCombinedSimplifiedMore <- tbl(all_db, "allCodingSimplifiedVariablesMore") %>% collect()


dbDisconnect(all_db)

# if an inclusion but none of the branches relevant, then unclear
predBranch$Unclear <- ifelse(predBranch$Mitigation_mean < 0.5 &
                       predBranch$Nature_mean < 0.5 &
                       predBranch$Societal_mean < 0.5, 1,0)

```


Firstly, of all the articles predicted relevant at the screeing stage, how many articles are predicted to be relevant for each ORO branch? Note the unclear option is chosen for any article included at the screening stage, but without a relevance prediction for any particular branch.

```{r tabulate oro_branch}
branchTab <- predBranch[,c(grep("upper", colnames(predBranch)), grep("Unclear", colnames(predBranch)))]%>%
  reshape2::melt(variable.name = "ORO_branch") %>% 
  group_by(ORO_branch)%>%
  summarise(n_articles = sum(0.5 <= value))

knitr::kable(branchTab)
```


We can next look at which branches were predicted to be relevant, compared to what branch was selected:

```{r merge with relevance predictions for inclusions branch predictions and actual coding decisions}

df <- predRel %>%
  filter(0.5 <= relevance_mean)%>%
  left_join(predBranch, by = "analysis_id") %>%
  left_join(codebookCombinedSimplifiedMore %>%
               select(analysis_id, title, oro_branch.Mitigation, 
                      oro_branch.Nature, oro_branch.Societal))

```

```{r}

# melt based on seen decisions for each unit of publication
branchDf <- df %>%
  select(analysis_id, relevance_mean, Mitigation_mean, Nature_mean, Societal_mean,
         oro_branch.Mitigation, oro_branch.Nature,oro_branch.Societal)%>%
  reshape2::melt(
    measure.vars = c("oro_branch.Mitigation","oro_branch.Nature","oro_branch.Societal"),
    #measure.vars = c("Mitigation_mean","Nature_mean","Societal_mean"),
    value.name = "seen_branch", variable.name = "oro_branch") %>%
  mutate(oro_branch = factor(oro_branch, 
                             c("oro_branch.Mitigation","oro_branch.Nature",
                               "oro_branch.Societal"),
                             c("Mitigation","Nature","Societal")))%>%
  mutate(seen_branch = ifelse(seen_branch==1, as.character(oro_branch), NA)) %>%
  select(-c(oro_branch)) %>%
  filter(!is.na(seen_branch))

# add a column for the predictions for the branch that was picked when seen
pred <- rep(NA, nrow(branchDf))
for(i in 1:length(pred)){
  if(branchDf$seen_branch[i] == "Mitigation"){
    pred[i] <- branchDf[i, grep("Mitigation", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Nature"){
    pred[i] <- branchDf[i, grep("Natur", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Societal"){
    pred[i] <- branchDf[i, grep("Societal", colnames(branchDf), ignore.case = T)]
  }else{
    pred[i] <- NA
  }
}
branchDf$seen_branch_pred <- pred; rm(pred)
```

```{r fig.cap = "what is the predicted relevance for the branch that was chosen by the reviewer?", out.width="90%", fig.asp=0.5, fig.align='center', eval=FALSE}

ggplot(branchDf, aes(x=seen_branch, y=seen_branch_pred, fill = seen_branch))+
  labs(x = "Reviewer branch classification",
       y= "Predicted relevance for classified branch")+
  geom_violin(trim=TRUE)+
  scale_fill_discrete(guide="none")+
    theme_bw()
```



```{r fig.cap="facetted by the branch seen, which branches were predicted relevant?", out.width="90%", fig.asp=0.5, fig.align='center'}

branchDf %>%
  select(analysis_id, Mitigation_mean, Societal_mean, Nature_mean, seen_branch) %>%
  reshape2::melt(measure.vars = c("Mitigation_mean", "Societal_mean", "Nature_mean"),
                 variable.name = "predictedBranch", value.name = "predictedRelevance")%>% 
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedBranch, y= predictedRelevance, fill=predictedBranch))+
  #geom_hline(yintercept = 0.5)+
  geom_violin()+
  facet_wrap(vars(seen_branch))+
  labs(y="predicted relevance", x = "predicted ORO branch", fill = "predicted ORO branch",
       caption="Facetted by the branch seen, what branches were predicted to be relevant?")+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```

Overall the ORO branch predictions seem to perform well for Mitigation and Societal branches, but the Natural resilience branch both mitigation and societal are showing high relevance predictions (although not at much as for "Nature" itself). Often though, many Natural resilience options tend to lie at the intersection between mitigation and societal adaptation goals, so it is not unsurprising that some of this language crosses over and predicts relevance for mitigation and societal as well.

## ORO type


```{r read in oro_any predictions}
# Note these predictions were made using the upper confidence limit for ORO_branch
oroAnyFiles <- dir(here::here("data/raw-data/coding-predictions"))
oroAnyFiles <- oroAnyFiles[grep("oro_any", oroAnyFiles)]

predOROAnyList <- list()

for(i in 1:length(oroAnyFiles)){
  temp <- readr::read_csv(
    here::here("data/raw-data/coding-predictions",oroAnyFiles[i]),
    show_col_types = FALSE
  )
  colNames <- colnames(temp)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(temp) <- colNames
  
  temp$oro_branch <- gsub("oro_any_","",oroAnyFiles[i])
  temp$oro_branch <- gsub("_predictions.csv","",temp$oro_branch)
  
  
  predOROAnyList[[i]] <- temp
  
}


## Make a dataframe of just the upper predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  temp <- temp[grep("upper", colnames(temp))]
  colnames(temp) <- gsub("_upper","", colnames(temp))
  temp <- reshape2::melt(temp, variable.name = "oro_any")
  temp <- cbind(predOROAnyList[[i]][,c("analysis_id","oro_branch")], temp)
  
  if(i==1)
    predOROAny <- temp
  else
    predOROAny <- rbind(predOROAny, temp)
}

rm(predOROAnyList)
```

```{r join in oro_any metadata}
predOROAnyMeta <- dedups %>%
  inner_join(predOROAny, by = "analysis_id") 
rm(dedups)

```


Next, looking at the more granular typology of ORO type, we can first ask "How many articles (over the whole relevant corpus) are predicted relevant for each ORO type?"

```{r fig.cap="number of articles relevant to each ORO type", out.width="90%", fig.asp=0.65, fig.align='center'}
temp <- predOROAny %>%
  group_by(oro_branch, oro_any)%>%
  summarise(n_articles = sum(0.5 <= value))%>%
  arrange(oro_branch,n_articles)%>%
  ungroup()%>%
  mutate(valueOrder = as.factor(row_number()))
  
ggplot(temp, aes(x=valueOrder, y=n_articles, fill = oro_branch))+
  geom_col()+
  geom_text(aes(label = n_articles), nudge_y = 2000, size=3)+
  scale_fill_discrete(name = "ORO Branch")+
  scale_x_discrete(limits = levels(temp$valueOrder), 
                    labels = gsub("_"," ", temp$oro_any[order(temp$valueOrder)]))+
  labs(y="Number predicted\nrelevant articles (upper)")+
  theme(
    panel.background = element_rect(fill="white",colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_text(angle=90, hjust=1),
    axis.title.x = element_blank()
  )
```


We may also want to ask, how many articles contain predicted relevance for more than one ORO type, and which types display the most linkages? Note in the figure that the pink rectangle identifies inter-branch interactions between mitigation and natural resilience, the purple between mitigation and societal, and the green between societal adaptation and natural resilience. 

These inter-branch interactions occur most often between Conservation options (Protection and Restoration) and CO2 removal and storage, as well as with societal adaptation options. This may reflect the multiple objectives for both mitigation and societal adaptation that often motivate nature-based solutions and adaptation options.

```{r fig.cap="Interlinkages between ORO types", out.width="80%", fig.asp=0.8}
# make a co-occurrence matrix
oroAnyLabels <- gsub("_"," ", levels(predOROAny$oro_any))
oroAnyLabels <- replace(oroAnyLabels, oroAnyLabels=="Socioinstitutional", "Socio-institutional")
temp <- predOROAny %>%
  filter(0.5 <= value)
temp$oro_any <- factor(as.character(temp$oro_any),
                       levels(predOROAny$oro_any),
                       labels = oroAnyLabels)
temp <- crossprod(table(temp[,c("analysis_id","oro_any")]))
diag(temp) <- NA
temp[!lower.tri(temp)] <- NA

# melt into long format for plotting
temp <- reshape2::melt(temp, variable.name="oro_any",value.name = "n_articles")
colnames(temp) <- c("oro_any1","oro_any2","n_articles")

pal <- c("white",RColorBrewer::brewer.pal(9,"Blues")[-c(1:3)])
#pal[2:3]<- colorspace::darken(pal[2:3])

# plot
temp %>%
  ggplot(aes(oro_any1, oro_any2, fill=n_articles))+
  geom_tile()+
  scale_fill_gradientn(colours = pal)+
  geom_text(aes(label=n_articles), size=2.5)+
  #scale_fill_gradient2(low="#eff3ff", high="#2171b5",na.value = "grey")+
  geom_rect(xmin=3.5,xmax=7.5-0.02,ymin=0,ymax=3.5-0.02, col="pink", 
            fill="transparent", linewidth=0.75)+
  geom_rect(xmin=7.5+0.02, xmax=9.5,ymin=0,ymax=3.5-0.02, col="purple", 
            fill="transparent", linewidth=0.75)+
  geom_rect(xmin=7.5, xmax=9.5, ymin=3.5+0.02,ymax=7.5, col="green", 
            fill="transparent", linewidth=0.75)+
  labs(
    y= "",x="",fill="Number\nof articles"
  )+
  theme(
    panel.background = element_rect(fill="white", colour="black"),
    axis.text.x = element_text(angle=90, hjust=1)
  )
    

```

From the figures above, we can see an additional category added under the Natural resilience ORO branch: Conservation. This was trialed as a union between Protection and/or Restoration, as often in the literature these options are used together as a broader conservation portfolio and may be difficult to distinguish. Here I compare the performance of these three labels (protection, restoration and conservation) against the decisions from the seen articles.

```{r}

# melt based on seen decisions for each unit of publication
predOROAnyDf <- predOROAnyMeta %>%
  filter(oro_any %in% c("Protection","Restoration","Conservation"))%>%
  reshape2::dcast(analysis_id~oro_any, value.var = "value") %>%
  left_join(codebookCombinedSimplifiedMore %>%
               select(analysis_id, oro_any.N_Protection,oro_any.N_Restoration))%>% 
  reshape2::melt(
    measure.vars = c("oro_any.N_Protection","oro_any.N_Restoration"),
    value.name = "seen_oro", variable.name = "oro_any") %>%
  mutate(oro_any = factor(oro_any, 
                             c("oro_any.N_Protection","oro_any.N_Restoration"),
                             c("Protection","Restoration")))%>%
  mutate(seen_oro = ifelse(seen_oro==1, as.character(oro_any), NA)) %>%
  select(-c(oro_any)) 
  
```



```{r fig.cap="facetted by the branch seen, which branches were predicted relevant?",out.width="90%", fig.asp=0.5, fig.align='center'}

predOROAnyDf %>%
  filter(!is.na(seen_oro))%>%
  reshape2::melt(measure.vars = c("Protection","Restoration","Conservation"),
                 variable.name = "predictedORO", value.name = "predictedRelevance")%>% 
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedORO, y= predictedRelevance, fill=predictedORO))+
  #geom_hline(yintercept = 0.5)+
  geom_violin()+
  facet_wrap(vars(seen_oro))+
  labs(y="Predicted relevance", x = "Predicted ORO type", fill = "predicted\nORO type")+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```



This indicates that for protection OROs, the model predicts a higher protection relevance than restoration, however the reverse is not true for restoration. Conservation is consistently high for both.


Extending this analysis to all the types of OROs (not including the Conservation label), this yeilds the following figure. Note that each panel refers to the ORO type that was chosen by the reviewer.

```{r}
# melt based on seen decisions for each unit of publication
# 1 row = 1 article x seen ORO combinations
oroAnyCols <- colnames(codebookCombinedSimplifiedMore)[grep("oro_any", colnames(codebookCombinedSimplifiedMore))]
oroAnyColsLabels <- gsub("oro_any..._","",gsub("oro_any.._","",oroAnyCols))
oroAnyColsLabels <- gsub("_"," ",oroAnyColsLabels)

predOROAnyDf <- predOROAnyMeta %>%
  reshape2::dcast(analysis_id~oro_any, value.var = "value") %>%
  left_join(codebookCombinedSimplifiedMore[c("analysis_id", oroAnyCols)])%>%
  reshape2::melt(
    measure.vars = oroAnyCols,
    value.name = "seen_oro", variable.name = "oro_any") %>%
  mutate(oro_any = factor(oro_any,levels = oroAnyCols,labels = oroAnyColsLabels))%>%
  mutate(seen_oro = ifelse(seen_oro==1, as.character(oro_any), NA)) %>%
  select(-c(oro_any)) 
  
```


```{r fig.cap="facetted by the oro type seen, which were predicted relevant?", out.width="100%", fig.asp=1.1}
# define colour scale
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

facetCol <- data.frame(
  seen_oro = factor(oroAnyColsLabels, oroAnyColsLabels),
  oroColor = gg_color_hue(length(oroAnyColsLabels))
)

# Plot
predOROAnyDf %>%
  select(-c(Conservation))%>%
  filter(!is.na(seen_oro))%>%
  reshape2::melt(id.vars = c("analysis_id", "seen_oro"),
                 variable.name = "predictedORO", value.name = "predictedRelevance")%>% 
  filter(!is.na(predictedORO))%>%
  mutate(predictedORO = factor(gsub("_"," ", predictedORO), oroAnyColsLabels),
         seen_oro = factor(gsub("_"," ",seen_oro), oroAnyColsLabels))  %>%

  ggplot()+
  geom_hline(yintercept = 0.5)+
  geom_violin(aes(x=predictedORO, y= predictedRelevance, col=predictedORO),
              trim=TRUE, width=1.5, scale = "count", linewidth = 0.75)+
  facet_wrap(vars(seen_oro), labeller=label_wrap_gen(35))+
  geom_rect(data=facetCol, aes(xmin=-Inf, xmax=Inf, ymin=1.05, ymax=1.35,
                               fill=seen_oro))+
  coord_cartesian(clip = "off", ylim=c(0,1))+
  labs(y="Predicted relevance", x = "Predicted ORO type",
       guide = "Predicted\nORO type")+
  scale_fill_discrete(guide="none")+
  scale_colour_manual(guide="none", values = facetCol$oroColor, breaks = facetCol$seen_oro)+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "lightgrey"),
    #axis.text.x = element_blank(),
    strip.background = element_rect(fill=NA),
    axis.text.x = element_text(angle=90, hjust=1)
  )


```

## Product-specific variables

### Product 2: biodiversity_metric and impact_nature

```{r read in biodiversity_metric and impact_nature}

## READ IN
# to speed this up only look at upper predictions

# biodiversity_metric
predBiodiv <- readr::read_csv(
  here::here("data/raw-data/coding-predictions/biodiversity_metric_predictions.csv"),
  show_col_types = FALSE
)
predBiodiv <- predBiodiv[,c(1,grep("upper", colnames(predBiodiv)))]
colnames(predBiodiv) <- c("analysis_id",paste("biodiversity_metric",c("upper"), sep="_"))

# impact_nature
predImpactNature <- readr::read_csv(
  here::here("data/raw-data/coding-predictions/impact_nature_predictions.csv"),
  show_col_types = FALSE
)
predImpactNature <- predImpactNature[,c(1,grep("upper", colnames(predImpactNature)))]
colnames(predImpactNature) <- c("analysis_id",paste("impact_nature",c("upper"), sep="_"))


## Join datasets together
predPaper2 <- merge(predBiodiv, predImpactNature,by="analysis_id")

```

While neither of these variables received good F1 scores, we can still compare the predicted relevance with the codded answers.
```{r out.width="80%", fig.asp = 0.6}
df <- codebookCombinedSimplifiedMore %>%
  select(analysis_id, biodiversity_metric, impact_nature)%>%
  reshape2::melt(id.vars = "analysis_id", variable.name = "variable",
                 value.name = "seen_value")%>%
  left_join(predPaper2 %>%
              rename(biodiversity_metric = biodiversity_metric_upper,
                     impact_nature = impact_nature_upper) %>%
              reshape2::melt(id.vars="analysis_id",variable.name = "variable",
                             value.name = "predicted_relevance"))%>%
  filter(!is.na(predicted_relevance)) %>% 
  mutate(seen_value = factor(seen_value, levels = c(0,1), 
                             labels = c("No","Yes")))
  
ggplot(df, aes(x=variable, y=predicted_relevance, fill=seen_value, col=seen_value))+
  geom_violin(trim=TRUE, position="dodge")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  labs(y = "Predicted relevance", fill="Reviewer\ndecision",col="Reviewer\ndecision")+
  theme_bw()

```

However, looking at the distributions, we could attain better precision by increasing the threshold for predicted inclusion.

Which "threshold" would yield the best F1 ratio (harmonic mean of precision and recall)?

```{r Which "threshold" would yield the best F1 ratio, out.width="80%", fig.asp=0.6}
# different thresholds to try
thresholds <- seq(0.5,1, length.out=100)

# calculate the precision and recall and F1 for that threshold
for(i in 1:length(thresholds)){
  temp <- df %>%
    group_by(variable)%>%
    summarise(
      TP = sum(seen_value == "Yes" & thresholds[i] <= predicted_relevance),
      FP = sum(seen_value == "No" & thresholds[i] <= predicted_relevance),
      FN = sum(seen_value == "Yes" & predicted_relevance < thresholds[i])
    )%>%
    group_by(variable)%>%
    summarise(
      precision=TP/(TP+FP),
      recall=TP/(TP+FN)
    )%>%
    mutate(F1=(2*recall*precision)/(recall+precision),
           threshold = rep(thresholds[i],2))

 if(i==1){
   thresholdsDf <- temp
 }else{
   thresholdsDf <- rbind(thresholdsDf, temp)
 }
}
thresholdsDf <- na.omit(thresholdsDf)



## Plot the change in F1 over the different thresholds and find the max
maxVals <- thresholdsDf %>%
  group_by(variable)%>%
  summarise(
    threshold = threshold[which.max(F1)],
    F1 = F1[which.max(F1)]
  )

ggplot(thresholdsDf, aes(threshold, F1))+
  facet_wrap(vars(variable))+
  geom_point()+
  geom_text(data=maxVals, aes(x=threshold-0.05, y=F1/2, label=signif(threshold,2)),
             col="red")+
  geom_vline(data=maxVals,aes(xintercept=threshold),col="red")+
  labs(x="Threshold")+
  theme_bw()

```


### Product 3: restoration

### Product 4: safe_fish and safe_space


## Summary: Tabulation of articles across all variables


The following table provides the number of articles predicted to be relevant for each category. Categories are defined by the columns "variable" and "label". Note that for each variable, multiple labels can be selected for one article. The number of articles is derived using the upper estimate of predicted relevance.

These tabulations are also saved in the file titled "codingTabulations.xlsx"

```{r make a dataframe tabulating all the counts for all variables and values}
# tabulate ORO_branch
oroBranchTab <- predBranch[,c(grep("upper", colnames(predBranch)), grep("Unclear", colnames(predBranch)))]%>%
  reshape2::melt(variable.name = "label") %>% 
  group_by(label)%>%
  summarise(n_articles_upper = sum(0.5 <= value))%>%
  mutate(variable = paste("oro_branch"),
         label = gsub("_upper","", label)) %>%
  select(variable, label, n_articles_upper)

# tabulate ORO_any
oroAnyTab <- predOROAny %>%
  group_by(oro_branch,oro_any)%>%
  summarise(n_articles_upper = sum(0.5 <= value))%>%
  arrange(oro_branch,n_articles_upper)%>%
  ungroup()%>%
  select(-c(oro_branch))%>%
  mutate(variable = paste("oro_type"))%>%
  rename(label = oro_any) %>%
  select(variable, label, n_articles_upper)


## Join all tabulations together & write
tabulations <- rbind(oroBranchTab,
                     oroAnyTab)

writexl::write_xlsx(tabulations,
                    here::here("data/derived-data/files_for_coauthors/codingTabulations.xlsx"))


```

```{r display table of tabulations}
knitr::kable(tabulations)
```


# README: Information about the dataset

All files are formatted as tab-delimited .txt files, so that commas and punctuation appearing in the free text columns (e.g. abstract) do not mess up the allocations of cells.

Each file contains a different componenet of information from different stages of the analysis (described below), and each can be joined by the following common ID column:

analysis_id = A unique identifier for each publiction retrieved by the search strategy
Each dataset 



**Dataset 1.1: screeningDecisions.txt**

This file contains the screening decisions for each article that was coded manually to train the data.

The columns include:
reviewer = the name of the first reviewer
reviewer_2 = the name of the second reviewer if the article underwent a double-blind check.
test_list = will equal "test list" if the article was a part of the test list or "NA" if not
include_screen = a binary value of 0 or 1, 1 indicating the article was included


```{r}
screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screens2$sample_screen <- ifelse(screens2$sample_screen == "test list", 
                                 screens2$sample_screen, NA)
colnames(screens2)[which(colnames(screens2) == "sample_screen")] <- "test_list"

write.table(
  screens2[,c("analysis_id","reviewer","reviewer_2","test_list","include_screen")],
  file = here::here("data","derived-data","files_for_coauthors","screeningDecisions.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```

**Dataset 1.2: codingDecisions.txt**

This file contains the coding decisions for each article that was coded manually to train the data. Descriptions for the different variables and values can be found in the "variableDescriptions.xlsx" file.

The columns include:
coder_1 = the name of the first coder
coder_2 = the name of the second coder if the article was coded twice for a consistency check.

The rest of the columns follow the following nomenclature: [name of variable].[name of value] 
Note that multiple values can be selected for each variable. A value of 1 indicates a selection, while 0 indicates absence.

```{r compile and save coded variables}
write.table(
  codebookCombinedSimplifiedMore %>% select(-c("sysrev_id")),
  file = here::here("data","derived-data","files_for_coauthors","codingDecisions.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```


**Dataset 2: deduplicatedCorpus.txt**

This file contains the de-duplicated corpus of literature that is used for deriving predictions from the model once it is trained.

The additional columns include:

duplicate_id = A unique identifier for each unit of publication (i.e. after de-duplication).
n_duplicates = the number of articles which were assigned to the same duplicate_id
title = the title of the article
abstract = the abstract of the article
keywords = the keywords of the article
year = the year of publication
doi = the DOI of the article if available
source_title = the title of the source of the publication (e.g. journal name)


```{r}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
dedups <- tbl(all_db, "uniquerefs") %>%
  select(analysis_id, duplicate_id, n_duplicates, title, abstract, keywords, year, doi, source_title) %>%
  filter(!is.na(abstract))%>%
  collect()

dbDisconnect(all_db)

write.table(
  dedups,
  file = here::here("data","derived-data","files_for_coauthors","deduplicatedCorpus.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```


**Dataset 3: upperPredictions.txt**

This file contains the predictions (using the upper estimate) of the machine learning model for the screening and coded variables. The column names for the variables and values adopt the same naming convention used in the codingDecisions.txt file. However, instead of a binary value of 0 or 1 for exclusion vs inclusion, the column will contain a continuous value from 0 to 1 indicating the degree of relevance. Columns with predicted relevance greater or equal to 0.5 are classified as a relevant response.

the suffix "_upper" on the column names indicates that it is the upper estimate of predicted relevance

```{r}
# Note these predictions were made using the upper confidence limit for ORO_branch
oroAnyFiles <- dir(here::here("data/raw-data/coding-predictions"))
oroAnyFiles <- oroAnyFiles[grep("oro_any", oroAnyFiles)]
for(i in 1:length(oroAnyFiles)){
  temp <- readr::read_csv(
    here::here("data/raw-data/coding-predictions",oroAnyFiles[i]),
    show_col_types = FALSE
  )
  colnames(temp)[1] <- "analysis_id"
  temp <- temp[,c(1, grep("upper", colnames(temp)))]
  colnames(temp) <- gsub(" - upper_pred","_upper",colnames(temp))
  colnames(temp) <- gsub(" - upper_prediction","_upper",colnames(temp))
  if(i==1)
    predOROWide <- temp
  else
    predOROWide <- merge(predOROWide, temp, all=T, by="analysis_id")
}


upperPred <- predRel %>% 
  select(analysis_id, relevance_upper) %>%
  left_join(predBranch[c(1, grep("upper", colnames(predBranch)))], by="analysis_id")%>%
  left_join(predOROWide, by="analysis_id")

colnames(upperPred)[3:5] <- paste0("oro_branch.",colnames(upperPred)[3:5])


write.table(
  upperPred,
  file = here::here("data","derived-data","files_for_coauthors","upperPredictions.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```





