---
title: "Database-description-coauthors"
author: "Devi Veytia"
date: "2023-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning =FALSE)
```


```{r set up}

## Load libraries
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(reticulate)
library(ggplot2)


```

# Background

Here I present the preliminary predictions from the machine learning model for (1) relevance, and (2) metadata coding.

To achieve this, first our search string was used to retrieve a literature from two citation indexed databases: Web of Science and Scopus. These articles were then de-duplicated based on title and year matching. 

From this de-duplicated corpus, a subset of articles were screened for inclusion or exclusion:
```{r}
screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screenSummary <- table(screens2$include_screen)
names(screenSummary) <- c("exclude","include")
print(screenSummary)

rm(screens2, screenSummary)
```

To generate predicted relevance for all "unseen" articles, a machine learning binary classification model was trained and tested on the "seen" corpus summarized above (see Section 1).

The articles that were labelled as inclusions from the screened subset above were then coded for metadata variables. For each variable, the labels decided by a human were used to train multi-label or binary classifiers (using the former if the variable allowed multiple choice values) analagous to the method used above. After the model training and selection, the best performing model was used to calculate predicted relevance for the "unseen" articles with a predicted relevance greater or equal to 0.5 at the mean confidence level (see Section 2).

# Screening

```{r read in files for relevance predictions}

## READ IN 
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

predRel <- tbl(all_db, "predRel2") 
dedups <- tbl(all_db, "uniquerefs") %>% 
  select(analysis_id, duplicate_id, title, abstract, keywords, doi)

screens2 <- read.delim2(here::here("data","derived-data","screening","screened-records","all-screen-results_screenExcl-codeIncl.txt"), quote = "")
screens2 <- screens2 %>%
  select(duplicate_id, sample_screen, include_screen)


## FORMAT
predRelMeta <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  collect()
predRelMeta <- predRelMeta %>% full_join(screens2, by = "duplicate_id")

# disconnect
dbDisconnect(all_db)

```

In total, the model predicts the following numbers of relevant articles for each error margin (lower, mean, upper):
```{r summaries of numbers of articles across all error margins}

predRelMeta %>%
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)
  )

```


However, a lot of information about model performance can be gained by comparing the distribution of predicted relevance (a continuous value from 0 to 1 with articles greater or equal to 0.5 being considered an inclusion) to screening decisions made by human reviewers (binary outcome = inclusion vs exclusion). Ideally, high predicted relevance will be seen for articles which were included, and vice versa. 

Of the test list articles (our list of benchmark/exemplary articles), was was the predicted relevance? Overall, both mean and upper confidence intervals demonstrate 100% recall for the test list articles (i.e. no false negatives).
```{r fig.cap="predicted relevance for test list articles", out.width="80%", fig.asp=0.67, fig.align='center'}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "confidence", value.name = "value"
  ) %>%
  ggplot(aes(x = confidence, y = value, fill = confidence))+
  geom_violin(trim=TRUE)+
  geom_hline(yintercept = 0.5)+
  scale_fill_discrete(guide="none")+
  labs(x="Confidence", y = "Predicted relevance")+
  theme_bw()
  
```

Note that in the "lower" is there one False negative:
```{r}
predRelMeta %>%
  filter(sample_screen == "test list" & include_screen == 1) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "confidence", value.name = "value"
  ) %>%
  group_by(include_screen, confidence) %>%
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) 

```



This can be extended to all the articles screened. The red text labels show the counts of articles which fall into each possible outcome with a binary classification (true positive, false positive, true negative, false negative). Overall the model performs fairly well with high accuracy by predicting true positives and true negatives. While taking the upper limit minimizes the number of false negatives, it also performs more poorly on precision (more false positives). 


```{r tabulate the number of included and excluded screens fall on either side of the 0.5 threshold}
tabText <- predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "confidence", value.name = "value"
  ) %>%
  group_by(include_screen, confidence) %>% 
  summarise(pred_include = sum(0.5 <= value), pred_exclude = sum(value < 0.5)) %>% 
  ungroup()%>%
  reshape2::melt(
    measure.vars = c("pred_include","pred_exclude"),
    variable.name = "prediction_include", value.name = "predicted number"
  ) %>%
  mutate(value = ifelse(prediction_include == "pred_include", 0.75,0.25))%>%
  mutate(prediction_include = factor(
    prediction_include, levels = c("pred_include","pred_exclude"), 
    labels = c("include","exclude")
  ))

```

```{r fig.cap= "The distribution of predicted relevance for all articles screened by model confidence"}

predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  reshape2::melt(
    measure.vars = paste("relevance", c("lower","mean","upper"), sep="_"),
    variable.name = "confidence", value.name = "value"
  ) %>%
  
  ggplot(aes(x= include_screen, y=value, fill = include_screen)) +
  facet_wrap(vars(confidence))+
  geom_violin(trim=TRUE)+
  geom_text(data=tabText, aes(label = `predicted number`), nudge_x = 0.3, size=3, col="red")+
  geom_hline(yintercept = 0.5)+
  scale_y_continuous(limits = c(0,1))+
  scale_fill_discrete(guide="none")+
  labs(x="Reviewer decision", y = "Predicted relevance")+
  theme_bw()

```


Since our corpus is so large, it may be more sensible to explore the most relevant articles and use the mean of the relevance predictions which performs moderately in both minimizing false negatives and false positives.


# Coding

## ORO type

Since our ORO typology is hierarchical (see below), an incremental approach was taken to predict the type of ORO mentioned in the abstract. First the "branch" of the ORO was predicted (i.e. Mitigation, Natural resilience and Societal adaptation). Given the branch predicted (e.g. Societal), then the more granular type of ORO was predicted (Socio-institutional, Built infrastructure). 

```{r}
knitr::include_graphics(here::here("figures/external/ORO-typology.png")) 
```

### ORO branch

```{r read in the oro_branch labels}
## READ IN

# ORO branch
predBranch <- readr::read_csv(
  here::here("data/raw-data/coding-predictions/oro_branch_predictions.csv")
)

colnames(predBranch) <- c("analysis_id",
                          paste("Mitigation", c("mean","std","lower","upper"), sep="_"),
                          paste("Nature", c("mean","std","lower","upper"), sep="_"),
                          paste("Societal", c("mean","std","lower","upper"), sep="_")
                          )

if(anyDuplicated(predBranch$analysis_id)){ # check for duplicates
  print("duplicates present")
}else{print("no duplicates")}

# Relevance predictions
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
predRel <- tbl(all_db, "predRel2") %>% collect()


# Coding decisions
codebookCombinedSimplifiedMore <- tbl(all_db, "allCodingSimplifiedVariablesMore") %>% collect()


dbDisconnect(all_db)

```

```{r format dataframes together to only inclusions and their predicted branches}
# merge with relevance predictions for inclusions, branch predictions and actual coding decisions
df <- predRel %>%
  filter(0.5 <= relevance_mean)%>%
  left_join(predBranch, by = "analysis_id") %>%
  left_join(codebookCombinedSimplifiedMore %>%
               select(analysis_id, title, oro_branch.Mitigation, 
                      oro_branch.Nature, oro_branch.Societal))

# if an inclusion but none of the branches relevant, then unclear
df$Unclear <- ifelse(df$Mitigation_mean < 0.5 |
                       df$Nature_mean < 0.5 |
                       df$Societal_mean < 0.5, 1,0)

```


```{r melt based on seen decisions for each article}

# melt based on seen decisions for each unit of publication
branchDf <- df %>%
  select(analysis_id, relevance_mean, Mitigation_mean, Nature_mean, Societal_mean,
         oro_branch.Mitigation, oro_branch.Nature,oro_branch.Societal)%>%
  reshape2::melt(
    measure.vars = c("oro_branch.Mitigation","oro_branch.Nature","oro_branch.Societal"),
    #measure.vars = c("Mitigation_mean","Nature_mean","Societal_mean"),
    value.name = "seen_branch", variable.name = "oro_branch") %>%
  mutate(oro_branch = factor(oro_branch, 
                             c("oro_branch.Mitigation","oro_branch.Nature",
                               "oro_branch.Societal"),
                             c("Mitigation","Nature","Societal")))%>%
  mutate(seen_branch = ifelse(seen_branch==1, as.character(oro_branch), NA)) %>%
  select(-c(oro_branch)) %>%
  filter(!is.na(seen_branch))

# add a column for the predictions for the branch that was picked when seen
pred <- rep(NA, nrow(branchDf))
for(i in 1:length(pred)){
  if(branchDf$seen_branch[i] == "Mitigation"){
    pred[i] <- branchDf[i, grep("Mitigation", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Nature"){
    pred[i] <- branchDf[i, grep("Natur", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Societal"){
    pred[i] <- branchDf[i, grep("Societal", colnames(branchDf), ignore.case = T)]
  }else{
    pred[i] <- NA
  }
}
branchDf$seen_branch_pred <- pred; rm(pred)
```

```{r fig.cap = "what is the predicted relevance for the branch that was chosen by the reviewer?"}

ggplot(branchDf, aes(x=seen_branch, y=seen_branch_pred, fill = seen_branch))+
  labs(x = "Reviewer branch classification",
       y= "Predicted relevance for classified branch")+
  geom_violin(trim=TRUE)+
  scale_fill_discrete(guide="none")+
    theme_bw()
```


```{r fig.cap="facetted by the branch seen, which branches were predicted relevant?"}

branchDf %>%
  select(analysis_id, Mitigation_mean, Societal_mean, Nature_mean, seen_branch) %>%
  reshape2::melt(measure.vars = c("Mitigation_mean", "Societal_mean", "Nature_mean"),
                 variable.name = "predictedBranch", value.name = "predictedRelevance")%>% 
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedBranch, y= predictedRelevance, fill=predictedBranch))+
  geom_violin()+
  facet_wrap(vars(seen_branch))+
  labs(y="predicted relevance", x = "predicted ORO branch", fill = "predicted ORO branch",
       caption="Facetted by the branch seen, what branches were predicted to be relevant?")+
  theme(
    panel.background = element_rect(fill="white", colour = "black"),
    panel.grid.major = element_line(colour = "grey"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```


```{r}
# tabulate the actual and predicted Branch classifications (just compare selections to the maximum predicted relevance branch, even though there may be multiple predictions/ analysis)
lev <- c("Mitigation","Nature","Societal")
# get all the branches predicted for each article
predictedBranch <- df %>%
  select(analysis_id, Mitigation_mean, Nature_mean, Societal_mean) %>%
  mutate(Mitigation = ifelse(0.5 <= Mitigation_mean, Mitigation_mean, NA),
         Nature = ifelse(0.5 <= Nature_mean, Nature_mean, NA),
         Societal = ifelse(0.5 <= Societal_mean, Societal_mean, NA)) %>% 
  reshape2::melt(measure.vars = lev, variable.name = "predictedBranch",
                 value.name = "predictedBranchRelevance") %>%
  filter(!is.na(predictedBranchRelevance)) %>% 
  select(analysis_id, predictedBranch, predictedBranchRelevance)


```












