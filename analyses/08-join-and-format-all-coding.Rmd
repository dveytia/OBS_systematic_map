---
title: "08-join-and-format-all-coding"
author: "Devi Veytia"
date: "2023-04-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
```



# Read in all coding results and format


```{r Source formatting function}

source(here::here("R","formatCoding2distilBert.R"))

# note that this function does not combine rows -- each row of output = each row of input

```


## Set-0 finalised


```{r combine set-0 into most common answer, eval=FALSE}

## Read in files
set0Dir <- here::here("data","derived-data","coding","manual-coding","set-0")
set0Files <- dir(set0Dir)
set0Files <- set0Files[grepl(".csv", set0Files)] # only include csvs because macro files don't read in all the multiple options
set0Files <- set0Files[!grepl("metadata", set0Files)] # remove metadata file
set0Files <- set0Files[!grepl("combined", set0Files)] # remove file if already formatted


## Using metadata, format the dataframe to fill
metadataTemplate <- readr::read_csv(file.path(set0Dir, "set-0-screening-metadata.csv"), show_col_types = FALSE)
metadataTemplate <- subset(metadataTemplate, select = c(sysrev_id, title, year))
metadataTemplate$sysrev_id <- as.character(metadataTemplate$sysrev_id)

## Format into binary responses for each variable x value combination
# bind into an array where each slice is a reviewer's response
for(f in 1:length(set0Files)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(set0Dir, set0Files[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = TRUE)
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$sysrev_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "sysrev_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("sysrev_id","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="sysrev_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("title","year","notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)
  # merge with template so formatting is the same
  df$data <- merge.data.frame(metadataTemplate, df$data, by="sysrev_id")
  
 
  # save
  if(f==1){
    variables <- df$variables
    responseArray <- df$data
  }else{
    responseArray <- abind::abind(responseArray, df$data, along=3)
  }
  
}



# # save
save(responseArray, file=file.path(set0Dir, "responseArray.RData"))
```

```{r calculate kappa statistic}
load(file.path(set0Dir, "responseArray.RData"))

# calculate
kappaStat <- apply(responseArray[,-c(1:5),], 2, irr::kappam.fleiss)

# summarise
kappaStatSummary <- unlist(lapply(kappaStat, function(x) x$value))
hist(kappaStatSummary)
```


```{r combine set 0 into the most common answer, eval=FALSE}
load(file.path(set0Dir, "responseArray.RData"))

# function to find most frequently appearing value
fun <- function(x){
  as.numeric(names(which.max(table(x))))
}


responseDf <- apply(responseArray[,-c(1:5),], 1:2, FUN=fun) # condense to most frequently appearing value
responseDf <- as.data.frame(responseDf)
responseDf <- cbind(responseArray[,c(1:5),1], responseDf) # bind back in ID columns


# change name of coder
responseDf[,"coder_1"] <- rep("all_combined", nrow(responseDf))


# for all inclusions where common decision is to include, but could not agree on ORO, set to unclear
incl <- which(responseDf[,"include_code"] == 1 & 
               sum(responseDf[,which(colnames(responseDf) == "oro_type.M_Renewables"):which(colnames(responseDf) == "oro_type.Unclear")] == 0))
noORO <- which(rowSums(responseDf[,which(colnames(responseDf) == "oro_type.M_Renewables"):which(colnames(responseDf) == "oro_type.Unclear")], na.rm=T) == 0)

responseDf[intersect(incl, noORO), "oro_type.Unclear"] <- 1


# # save

# screening decisions
write.csv(responseDf[,1:6], 
          here::here("data","derived-data","screening","screened-records","coding-screening_set-0.csv"),row.names = FALSE)

# coding decisions
write.csv(responseDf %>% filter(include_code == 1) %>% select(-c(include_code)),
  file.path(set0Dir,"codebook_combined-formatted_set-0.csv"),row.names = FALSE)

```



## Initial sets finalised



```{r format initial sets into binary columns for predictions in distilBERT}
## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","sets")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = TRUE)
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$sysrev_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "sysrev_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("sysrev_id","title","year","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="sysrev_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)

  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

#View(responseDf)

```


Make sure that these column names are included: "impact_economy.Tourism__Recreation" "impact_economy.Informal"           
[7] "impact_economy.Livelihood"          "impact_economy.Other"

And not "impact_economy.Tourism" "impact_economy.other" --> then there is an error in the impact_economy column, row 1 of the .csv file
```{r check for consistency in column names}
colnames(responseDf)
```


```{r save screening decisions seperately, eval=FALSE}
write.csv(responseDf[,1:6], 
          here::here("data","derived-data","screening","screened-records","coding-screening_initialSets.csv"),
          row.names = FALSE)

# only keep inclusions
responseDf <- responseDf %>%
  filter(include_code == 1) %>%
  select(-c(include_code))

# responseDf <- subset(responseDf, include_code == 1)
# responseDf <- responseDf[,which(colnames(responseDf) != "include_code")]
```



```{r read in combined answers from set-0 and merge in with responses from other sets}
set0Responses <- readr::read_csv(
  here::here("data","derived-data","coding","manual-coding","set-0","codebook_combined-formatted_set-0.csv"),
  show_col_types = FALSE
  )

responseDf <- rbind(set0Responses, responseDf)

```

```{r use the lookup table to join with analysis_id duplicate_id abstract and keywords}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db) # look at all the tables in the database

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(analysis_id, duplicate_id, title, year, abstract, keywords)

# lookup table of the sysrev_id to duplicate_id
sysrev_id <- tbl(all_db, "sysrevid_2_analysisid_lookup")
sysrev_id <- sysrev_id %>% select(duplicate_id, sysrev_id)

# join to get the relevant ids
relevantData <- sysrev_id %>%
  left_join(dedup_sub, by = "duplicate_id") %>%
  collect()
relevantData$sysrev_id <- as.character(relevantData$sysrev_id)

# disconnect
RSQLite::dbDisconnect(all_db)

dim(responseDf)

# merge metadata in by sysrev_id and order columns
responseDf <- responseDf %>%
  select(-c(year)) %>%
  left_join(relevantData, by = "sysrev_id")

# compare titles to makes sure the articles are the same
print(cbind(responseDf$title.x[1:5], responseDf$title.y[1:5]))

# remove duplicate title
responseDf <- responseDf %>%
  rename(title = title.x) %>%
  select(-c(title.y))

# order columns
idCols <- c("sysrev_id","analysis_id", "duplicate_id","title","abstract","keywords","year","coder_1","coder_2")
responseDf <- responseDf[,c(idCols, colnames(responseDf)[which(!(colnames(responseDf) %in% idCols))])]

# check there are now NAs for analysis_id or duplicate_id
sum(is.na(responseDf$analysis_id))
sum(is.na(responseDf$duplicate_id))
length(unique(responseDf$duplicate_id)) == length(responseDf$duplicate_id) # check all unique ids are unique
```

```{r add responses as a table to the sqlite database}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

RSQLite::dbWriteTable(all_db, "initial_coding", responseDf, overwrite=TRUE) #

RSQLite::dbDisconnect(all_db)

```


## Supplementary sets

```{r format supplementary sets}

## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","supplementary-sets")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = FALSE)
  
  # change column name because actually these weren't screened in sysrev so they don't have a sysrev_id
  colnames(df$data)[1] <- "duplicate_id"
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$duplicate_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "duplicate_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("duplicate_id","title","year","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="duplicate_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)


  
  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

#View(responseDf)

```

```{r check that ids are correct}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db) # look at all the tables in the database

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(analysis_id, duplicate_id, title, year, abstract, keywords) %>%
  collect()

RSQLite::dbDisconnect(all_db)

# look for title match when the dupid is matched
matches2 <- rep(NA, nrow(responseDf))

for(i in 1:length(matches2)){
  
  # what are all the possible matches?
  possibleMatches <- grep(responseDf$duplicate_id[i], dedup_sub$duplicate_id)
  
  # if only one, then simple, that's the match
  if(length(possibleMatches) == 1){
    matches2[i] <- possibleMatches
  }
  
  # if more than one possible match, look at the titles and match
  if(length(possibleMatches) > 1){
    titlematch <- stringdist::amatch(tolower(responseDf$title[i]), tolower(dedup_sub$title[possibleMatches]), 
                                     method = "osa", maxDist = 5, nomatch = NA)
    titlematch <- titlematch[!is.na(titlematch)]
    possibleMatches <- possibleMatches[titlematch[1]]
  }
  
  # assign the match id
  matches2[i] <- possibleMatches
}

# check for no NAs
sum(is.na(matches2))

# join with matches
responseDf2 <- cbind(
  my_text[matches2,c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  responseDf[,-c(1:3)]
)

responseDf2 <- responseDf2[!duplicated(responseDf2),]



# # text used for supplemental search
# my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))
# 
# # search for duplicate_id match
# matches2 <- rep(NA, nrow(responseDf))
# for(i in 1:length(matches2)){
#   
#   dupMatch <- which(my_text$duplicate_id == responseDf$duplicate_id[i])
#   
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"0"))
#   }
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"00"))
#   }
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"000"))
#   }
#   
#   matches2[i] <- dupMatch
# }
# sum(is.na(matches2))


```



```{r add supplementary responses as a table to the sqlite database}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

RSQLite::dbWriteTable(all_db, "supplementary_coding", responseDf2, overwrite=TRUE)



RSQLite::dbDisconnect(all_db)

```


# Merge all together and simplify columns


```{r merge with abstract and id metadata}

require(dbplyr)

all_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

#src_dbi(all_db) # look at all the tables in the database

# coding results from initial round
initialCoding <- tbl(all_db, "initial_coding")
initialCoding <- initialCoding %>% collect()

# coding results from supplemental round
supplementaryCoding <- tbl(all_db, "supplementary_coding")
supplementaryCoding <- supplementaryCoding %>% collect()
supplementaryCoding$sysrev_id <- rep(NA, nrow(supplementaryCoding)) # add a black sysrev_id

# check what columns they have in common and if any important ones are missing
cols <- intersect(colnames(initialCoding), colnames(supplementaryCoding))
colnames(initialCoding)[which(!(colnames(initialCoding) %in% colnames(supplementaryCoding)))]
colnames(supplementaryCoding)[which(!(colnames(supplementaryCoding) %in% colnames(initialCoding)))]

  
## Disconnect databases
dbDisconnect(all_db)


## append the data frames for initial and supplementary coding
codebookCombined <- rbind(initialCoding[,cols], supplementaryCoding[,cols])

# check IDs to make sure they are unique
length(unique(codebookCombined$duplicate_id))
length(codebookCombined$duplicate_id)
```


```{r tabulate all coding results}

# index of which columns belong to which variables
variableInd <- unlist(lapply(strsplit(colnames(codebookCombined), "[.]"), function(x) x[[1]]))

# identify variables to tabulate (i.e. not identification columns)
variables_to_tab <- unique(variableInd)
idCols <- c("sysrev_id","analysis_id","duplicate_id","title","abstract","keywords","year","coder_1","coder_2")
variables_to_tab <- variables_to_tab[which(!(variables_to_tab %in% idCols))]
  
# empty list to fill with tabulated tables (one item for each variable)
tabulated_variables <- list()

for(v in 1:length(variables_to_tab)){
  colsInd <- grep(variables_to_tab[v], variableInd, ignore.case = TRUE)
  
  if(is.character(codebookCombined[,colsInd])){next} # can't tabulate character vectors
  
  if(!is.null(dim(codebookCombined[,colsInd]))){
    tab <- colSums(codebookCombined[,colsInd], na.rm=TRUE)
    # if there are multiple values for each variable, get the variable name
    if(sum(grepl("[.]", colnames(codebookCombined)[colsInd])) > 0){
      names(tab) <- unlist(lapply(strsplit(colnames(codebookCombined)[colsInd], "[.]"), function(x) x[[2]]))
    }
  }else{
    tab <- tabulate(codebookCombined[,colsInd])
  }
  tabulated_variables[[v]] <- tab
  names(tabulated_variables)[v] <- variables_to_tab[v]
}


sink(here::here("outputs","coding","tabulated_coding_round1And2.txt"))
print(tabulated_variables)
sink()

```


```{r simplify coding variables and save as all-coding-format-distilBERT.txt}
idCols <- c("sysrev_id","analysis_id","duplicate_id","title","abstract","keywords","year","coder_1","coder_2")

source(here::here("R","simplifyCodebook.R"))

codebookCombinedSimplified <- simplifyCodebook(codebookCombined, idCols)

# check column names
colnames(codebookCombinedSimplified)

codebookCombinedSimplified <- codebookCombinedSimplified[!duplicated(codebookCombinedSimplified),]



## Save 
# write for Vicky & save
write.table(
  codebookCombinedSimplified,
  file = here::here("data","derived-data","coding", "all-coding-format-distilBERT.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)



# save to database as well
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
src_dbi(all_db)
RSQLite::dbWriteTable(all_db, "allCodingSimplifiedVariables", codebookCombinedSimplified, overwrite=TRUE) # , overwrite=TRUE

RSQLite::dbDisconnect(all_db)

```



# Compare these with all the articles that were screened to come up with updated screening decisions data


```{r compile final screening decisions from both initial and supplementary coding rounds}

## Coding from initial round 
codingScreen0 <- read.csv(
  here::here("data","derived-data","screening","screened-records","coding-screening_set-0.csv"))
codingScreen <- read.csv(
  here::here("data","derived-data","screening","screened-records","coding-screening_initialSets.csv"))

# join together
codingScreen <- rbind(codingScreen, codingScreen0)
rm(codingScreen0)
codingScreen <- codingScreen %>%
  rename(reviewer = coder_1, reviewer_2 = coder_2, include_screen = include_code) %>%
  select(sysrev_id, reviewer, reviewer_2, include_screen)


## Screening results from initial round
# now in screen results merged, remove articles that were in the coding screen, 
# so that it is just composed of entries that were excluded from the screening round 
# and never made it to coding
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData")) # screen_results_merged
initialScreen <- subset(screen_results_merged, !(sysrev_id %in% c(codingScreen$sysrev_id)))
initialScreen <- screen_results_merged %>%
  filter(!(sysrev_id %in% codingScreen$sysrev_id)) %>%
  rename(reviewer = screener) %>%
  mutate(reviewer_2 = as.character(Double_Blind)) %>%
  select(sysrev_id, reviewer, reviewer_2, include_screen, sample_screen) %>%
  mutate(sample_screen = as.character(sample_screen))

# need to get sample_screen from initial screening results
codingScreen <- codingScreen %>%
  left_join(screen_results_merged %>% select(sysrev_id, sample_screen), by = "sysrev_id") %>%
  mutate(sample_screen = as.character(sample_screen))



## join inital coding results with initial screening and get metadata
cols <- intersect(colnames(codingScreen), colnames(initialScreen))
codingScreen <- rbind(initialScreen[,cols], codingScreen[,cols])

# use the lookup table to join sysrev_id to analysis_id and duplicate_id
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"),
                                 create=FALSE)

sysrevIDLookup <- tbl(all_db, "sysrevid_2_analysisid_lookup")

dedups <- tbl(all_db,"uniquerefs")

codingScreen <- codingScreen %>%
  left_join(sysrevIDLookup %>% select(sysrev_id, analysis_id, duplicate_id), by="sysrev_id", copy=TRUE) %>%
  left_join(dedups %>% select(duplicate_id, title, abstract, keywords), by = "duplicate_id", copy=TRUE)

# format columns
codingScreen$include_screen <- codingScreen$include_screen == 1





## From the supplementary round

# get all screening results
supplementalScreenDir <- here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screening")
supplementalScreenFiles <- dir(supplementalScreenDir)

for(f in 1:length(supplementalScreenFiles)){
  temp <- read.csv(file.path(supplementalScreenDir, supplementalScreenFiles[f]))
  temp <- subset(temp, !is.na("screened_abstracts"))
  if(f==1){
    supplementalScreen <- temp
  }else{
    supplementalScreen <- rbind(supplementalScreen, temp)
  }
}
# clean
supplementalScreen <- subset(supplementalScreen, !is.na(screened_abstracts)) 
supplementalScreen <- supplementalScreen %>% 
  filter(!is.na(screened_abstracts)) # remove skipped articles
  

# add columns to be consistent with above
supplementalScreen$reviewer <- rep("Devi", nrow(supplementalScreen))
supplementalScreen$reviewer_2 <- rep(NA, nrow(supplementalScreen))
supplementalScreen$include_screen <- supplementalScreen$screened_abstracts == "selected"
supplementalScreen$sample_screen <- "supplemental coding"


## join with metadata id information
my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))

matches <- rep(NA, nrow(supplementalScreen))
for(i in 1:length(matches)){
  
  dupMatch <- which(my_text$duplicate_id == supplementalScreen$label[i])
  
  if(length(dupMatch)==0){
    dupMatch <- which(my_text$duplicate_id == paste0(supplementalScreen$label[i],"0"))
  }
  if(length(dupMatch)==0){
    dupMatch <- which(my_text$duplicate_id == paste0(supplementalScreen$label[i],"00"))
  }
  if(length(dupMatch)==0){
    dupMatch <- which(my_text$duplicate_id == paste0(supplementalScreen$label[i],"000"))
  }
  if(length(dupMatch)==0){
    dupMatch <- which(my_text$duplicate_id == paste0(supplementalScreen$label[i],"0000"))
  }
  
  matches[i] <- dupMatch
}
sum(is.na(matches))


# join with matches 
supplementalScreen2 <- cbind(
  my_text[matches, c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen[,c("reviewer","reviewer_2","include_screen","sample_screen")]
)
# remove a couple articles that were accidentially marked as include
supplementalScreen2 <- supplementalScreen2 %>%
  filter(!(duplicate_id %in% c("NA.217","2017.14740","2014.14080"))) 

#supplementalScreen2[which(supplementalScreen2$label == "1985.216"),]
supplementalScreen2$sysrev_id <- rep(NA, nrow(supplementalScreen2))
sum(is.na(supplementalScreen2$sample_screen))


## Join both data frames together
cols <- intersect(colnames(supplementalScreen2), colnames(codingScreen))
cols <- cols[order(cols)]
cols <- cols[c(2,3,9,6,7,8,4,10,1,5)] # re-order so its more sensible
allScreens <- rbind(
  supplementalScreen2[,cols], codingScreen[,cols]
)

# format
allScreens$reviewer <- as.factor(allScreens$reviewer)
allScreens$reviewer_2 <- as.factor(allScreens$reviewer_2)
allScreens$sample_screen <- as.factor(allScreens$sample_screen)
summary(allScreens)



## Save to database
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
#src_dbi(all_db)
RSQLite::dbWriteTable(all_db, "allScreen_afterCoding", allScreens, overwrite=TRUE)

RSQLite::dbDisconnect(all_db)



```


```{r summarise screening stats}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
allScreens <- tbl(all_db, "allScreen_afterCoding")
allScreens <- allScreens %>% collect()

RSQLite::dbDisconnect(all_db)

# how many inclusions vs exclusions
table(allCodingJoin$include_screen)
# FALSE  TRUE 
#  2056   961 


# Coding effort
screen_effort_summary <- allScreens %>%
  filter(sample_screen != "test list") %>%
  summarise(
    DV = sum(grepl("devi", reviewer, ignore.case = TRUE)),
    JGP = sum(grepl("jean", reviewer, ignore.case = TRUE)),
    AC = sum(grepl("adrien", reviewer, ignore.case = TRUE)),
    LB = sum(grepl("bopp", reviewer, ignore.case = TRUE)),
    YS = sum(grepl("yunne", reviewer, ignore.case = TRUE)),
    FV = sum(grepl("Fred", reviewer, ignore.case = TRUE))
  )
screen_effort_summary
#     DV JGP  AC  LB  YS FV
# 1 1522 231 334 411 456 46

screen_effort_summary <- reshape2::melt(screen_effort_summary)

mean(screen_effort_summary$value) # 500
sd(screen_effort_summary$value) # 521.6079
median(screen_effort_summary$value) # 372.5

rm(screen_effort_summary)

```

```{r summarise coding stats}
# because the final screening was done at coding -- the coding effort can be retreived from the screen database

# Coding effort
code_effort_summary <- allScreens %>%
  filter(include_screen == TRUE) %>% 
  summarise(
    DV = sum(grepl("devi", reviewer, ignore.case = TRUE)),
    JGP = sum(grepl("jean", reviewer, ignore.case = TRUE)),
    AC = sum(grepl("AC", reviewer, ignore.case = TRUE)),
    LB = sum(grepl("Laurent", reviewer, ignore.case = TRUE)),
    YS = sum(grepl("yunne", reviewer, ignore.case = TRUE)),
    FV = sum(grepl("Fred", reviewer, ignore.case = TRUE))
  )
code_effort_summary
#    DV JGP AC LB YS FV
# 1 655  31 93 81 33 42

code_effort_summary <- reshape2::melt(code_effort_summary)

mean(code_effort_summary$value) # 155.8333
sd(code_effort_summary$value) # 245.9044
median(code_effort_summary$value) # 61.5

rm(code_effort_summary)
```








# Make another screening dataframe that is by ORO_branch rather than inclusion vs exclusion

```{r}
## read in 

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
#src_dbi(all_db)

# combined screening table
allScreens <- tbl(all_db, "allScreen_afterCoding")
allScreens <- allScreens %>% collect()

# coding decisions
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables")
codebookCombinedSimplified <- codebookCombinedSimplified %>% collect()

RSQLite::dbDisconnect(all_db)


## for the inclusions, indicate which ORO branch it is
allScreens2 <- allScreens %>%
  select(sysrev_id, analysis_id, duplicate_id, reviewer, reviewer_2, title, abstract, keywords, 
         sample_screen, include_screen) %>%
  left_join(codebookCombinedSimplified %>% 
              select(duplicate_id, year, oro_branch.Mitigation,oro_branch.Nature, 
                     oro_branch.Societal, oro_branch.Unclear),
            by = "duplicate_id") %>%
  mutate(oro_present = rowSums(
    cbind(oro_branch.Mitigation,oro_branch.Nature,oro_branch.Societal, oro_branch.Unclear), 
    na.rm = TRUE)) 





# make sure there are no mis-matches -- there is a coding result for every inclusion
length(which(allScreens2$include_screen == FALSE & allScreens2$oro_present == 1))  
length(which(allScreens2$include_screen == TRUE & allScreens2$oro_present == 0))
#View(allScreens2[which(allScreens2$include_screen == TRUE & allScreens2$oro_present == 0),])
allScreens2$oro_present <- NULL





## Write for Vicky
write.table(
  allScreens,
  file = here::here("data","derived-data","screening","screened-records","all-screen-results_afterSupplementalCoding.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```


# Junk

```{r supplementary coding}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db)

# coding results
supCoding <- tbl(all_db, "supplementary_coding")
supCoding <- supCoding %>%
  select(-c(title, year, coder_1, coder_2))

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedups <- dedups %>% 
  select(analysis_id, title, abstract, keywords)

# join together
supCodingDf <- left_join(supCoding, dedups, by = "analysis_id") %>% collect() 

# check for NAs
sum(is.na(supCodingDf$abstract))
View(supCodingDf[is.na(supCodingDf$abstract),])

idCols <- c("duplicate_id","title","abstract","keywords")
supCodingDf <- supCodingDf[,c(idCols, colnames(supCodingDf)[which(!(colnames(supCodingDf) %in% idCols))])]
summary(supCodingDf[,idCols])


# but there are some which have not abstract matches
RSQLite::dbDisconnect(all_db)
```



```{r Using title to match supplemental screens metadata -- doesn't work well}

# Using title to match supplemental screens metadata -- doesn't work well
# supplemental coding
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"),
                                 create=FALSE)
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables")
codebookCombinedSimplified <- codebookCombinedSimplified %>% collect()
RSQLite::dbDisconnect(all_db)

supplementalScreen_inclusions <- subset(supplementalScreen, include_screen == TRUE)
  
matchesInc <- rep(NA, nrow(supplementalScreen_inclusions))

for(i in 1:length(matchesInc)){
  matchesInc[i] <- stringdist::amatch(tolower(supplementalScreen_inclusions$title[i]), 
                                   tolower(codebookCombinedSimplified$title), method = "osa", maxDist = 5, nomatch = NA)
}
sum(is.na(matchesInc))

supplementalScreen_inclusions <- cbind(
  codebookCombinedSimplified[matchesInc, c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen_inclusions[,c("reviewer", "reviewer_2","include_screen","sample_screen")]
)


# text used for supplemental search
supplementalScreen_exclusions <- subset(supplementalScreen, include_screen == FALSE)
my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))

# find title matches-- takes several minutes to run
matchesExc <- rep(NA, nrow(supplementalScreen_exclusions))

for(i in 1:length(matchesExc)){
  #yearInd <- which(my_text$year == supplementalScreen_exclusions$year[i])
  matchesExc[i] <- stringdist::amatch(tolower(supplementalScreen_exclusions$title[i]), tolower(my_text$title), method = "osa", maxDist = 5, nomatch = NA)
}
sum(is.na(matchesExc))

supplementalScreen_exclusions <- cbind(
  my_text[matches,c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen_exclusions[,c("reviewer", "reviewer_2","include_screen","sample_screen")]
)


supplementalScreen <- rbind(supplementalScreen_exclusions, supplementalScreen_inclusions)

save(supplementalScreen, matchesExc, matchesInc,
      file = here::here("data","derived-data","coding","keyword-matches-supplemental-coding","supplementalScreenMatches.RData"))
```
