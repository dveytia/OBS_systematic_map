---
title: "08-join-and-format-all-coding"
author: "Devi Veytia"
date: "2023-04-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
```



# Read in all coding results and format


```{r Source formatting function}

source(here::here("R","formatCoding2distilBert.R"))

# note that this function does not combine rows -- each row of output = each row of input

```


## Set-0 finalised


```{r combine set-0 into most common answer, eval=FALSE}

## Read in files
set0Dir <- here::here("data","derived-data","coding","manual-coding","set-0")
set0Files <- dir(set0Dir)
set0Files <- set0Files[grepl(".csv", set0Files)] # only include csvs because macro files don't read in all the multiple options
set0Files <- set0Files[!grepl("metadata", set0Files)] # remove metadata file
set0Files <- set0Files[!grepl("combined", set0Files)] # remove file if already formatted


## Using metadata, format the dataframe to fill
metadataTemplate <- readr::read_csv(file.path(set0Dir, "set-0-screening-metadata.csv"), show_col_types = FALSE)
metadataTemplate <- subset(metadataTemplate, select = c(sysrev_id, title, year))
metadataTemplate$sysrev_id <- as.character(metadataTemplate$sysrev_id)

## Format into binary responses for each variable x value combination
# bind into an array where each slice is a reviewer's response
for(f in 1:length(set0Files)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(set0Dir, set0Files[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = TRUE)
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$sysrev_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "sysrev_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("sysrev_id","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="sysrev_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("title","year","notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)
  # merge with template so formatting is the same
  df$data <- merge.data.frame(metadataTemplate, df$data, by="sysrev_id")
  
 
  # save
  if(f==1){
    variables <- df$variables
    responseArray <- df$data
  }else{
    responseArray <- abind::abind(responseArray, df$data, along=3)
  }
  
}



# # save
save(responseArray, file=file.path(set0Dir, "responseArray.RData"))
```

```{r calculate kappa statistic for each variable x value}
load(file.path(set0Dir, "responseArray.RData"))

# calculate
kappaStat <- apply(responseArray[,-c(1:5),], 2, irr::kappam.fleiss)

# summarise
kappaStatSummary <- unlist(lapply(kappaStat, function(x) x$value))
hist(kappaStatSummary)

kappaStatDF <- data.frame(VarVal = dimnames(responseArray)[[2]][-c(1:5)], Kappa = kappaStatSummary)
kappaStatDF$Variable <- unlist(lapply(strsplit(kappaStatDF$VarVal, "[.]"), function(x) x[1]))
kappaStatDF$Value <- unlist(lapply(strsplit(kappaStatDF$VarVal, "[.]"), function(x) x[2]))
kappaStatDF <- kappaStatDF %>% filter(!is.na(Kappa))
kappaStatDF <- kappaStatDF %>%
  group_by(Variable) %>%
  mutate(ValueNumber = as.integer(factor(Value))) %>%
  mutate(ValueNumber = ifelse(is.na(ValueNumber), 1, ValueNumber))



# plot
require(ggplot2)

ggplot(kappaStatDF, aes(x = ValueNumber, y = Variable, fill = Kappa, label = Variable))+
  geom_tile()+
  geom_text()



require(dplyr)
require(ggrepel)

kappaStatDF <- kappaStatDF %>% arrange(Kappa) 
kappaStatDF$KappaOrder <- seq(1:nrow(kappaStatDF))

ggplot(kappaStatDF %>% filter(Kappa > 0.5) %>% mutate(Variable = factor(Variable)), 
       aes(x=KappaOrder, y=Kappa, fill=Variable))+
  geom_point()+
  geom_hline(yintercept = 0.6, col="red")+
  geom_label_repel(aes(x=KappaOrder, y=Kappa, label = VarVal), max.overlaps = 15, size=3)+
  scale_x_continuous(name = "Variable x value")+
  theme(legend.position = "none")

sink(here::here("outputs","coding","coding_FleissKappa.txt"))
print(kappaStatDF$VarVal[order(kappaStatDF$Kappa, decreasing = TRUE)])
sink()

kappaStatDF %>%
  filter(Variable == "oro_type") %>%
  select(Value, Kappa)


```

However, it is difficult to quantify the kappa for multiple choice variables, so instead look at the each variable, and test how many coders agreed on at least one value. 

```{r format response Array for calculating kappa on multiple choice}
# required packages
require(dplyr)
require(ggplot2)
require(ggrepel)


# load in array of responses
load(file.path(set0Dir, "responseArray.RData"))


# create new variable: oro_branch
# function to calculate oro_branch from oro_type
source(here::here("R","oroBranchFn.R"))
# calculate oro_branch for each coder's response and then bind along 3rd dimension
oroBranchArray <- do.call(abind::abind,
                          list(
                            plyr::alply(
                              responseArray[,variableValues[grep("oro_type",variableValues)],],
                                3, oroBranchFn),
                            along = 3
                          ))


# list all the variables and values
variableValues <- c(dimnames(responseArray)[[2]], dimnames(oroBranchArray)[[2]])
values <- unlist(lapply(strsplit(variableValues, "[.]"), function(x) x[2]))
values[is.na(values)] <- variableValues[is.na(values)]
variables <- unlist(lapply(strsplit(variableValues, "[.]"), function(x) x[1]))
variables <- unique(variables)
variables <- variables[which(!(variables %in% c("sysrev_id","title","year","coder_1","coder_2","include_code")))]


# change the "1" in the data frame to the name of the value so that responses can be easily tallied by name 
temp <- abind::abind(responseArray, oroBranchArray, along = 2)
for(j in 1:dim(temp)[2]){
    for(s in 1:dim(temp)[3])
      temp[,j,s] <- ifelse(temp[,j,s] == 1, values[j], NA)
}

# check
temp[1,6:15,] 

```


```{r kappa for at least one value agreement per variable}
## functions to calculate agreement to the most common response   

# return the most common response(s)
commonResponseFn <- function(x){
    responseTab <- table(data.frame(factor(x)))
    maxTab <- responseTab[which.max(responseTab)]
    nMax <- which(responseTab == maxTab)
    commonResponse <- names(nMax)
    return(commonResponse)
}

# returns 1 if there is any agreement between a vector (representing all multiple choices of one coder for a variable) and the common response (the answer(s) for that variable)
agreeFn <- function(x, commonResponse){
  if(dim(x)[1] != length(commonResponse)){
    stop("number of articles in x does not match number of list items")
  }else{
    agreements <- rep(0, dim(x)[1])
    for(p in 1:dim(x)[1]){
      matches <- intersect(commonResponse[[p]], x[p,])
      if(1 <= length(matches)){ # if > 1 agreement/match
        agreements[p] <- 1
      }
    } # end looping through articles
    return(agreements)
  }
} # end function



## for each variable and publication, find the most common response

# empty data frame to store results
kappaResults <- data.frame(
  variable = variables,
  kappaValue = rep(NA, length(variables)),
  kappaPValue = rep(NA,length(variables)),
  kappaOutlierRmValue = rep(NA,length(variables)),
  kappaOutlierRmPValue = rep(NA,length(variables)),
  OutlierID = rep(NA,length(variables))
)

# Loop through the variables
for(v in 1:length(variables)){
  # which columns hold value responses for a given variable
  temp2 <- temp[,variableValues[grep(variables[v],variableValues)],]
  #temp2[is.na(temp2)] <- "0" # NA is still a choice, so change to 0 so that it doesn't get removed
  #dim(temp2)

  # If the variable is only one option (y/n), then a regular kappa test applies
  if(length(dim(temp2)) == 2){
    agreement <- temp2
    agreement[!is.na(agreement)] <- 1
    agreement[is.na(agreement)] <- 0
  }else{
    # If the variable is a multiple choice, 
    # for each coder, assess whether they agreed with the common response
    # returns list containing vectors of the most common responses where length == n articles
    commonResponse <- plyr::alply(temp2, 1, commonResponseFn) 
    
    # assess whether there was a match between each coder and any common response
    # returns a matrix with nrow = n articles, and 
    # ncol = n coders, indicating whether the coder had any response for the variable
    # that agreed with the most common choice
    agreement <- apply(temp2, 3, agreeFn, commonResponse=commonResponse)
    
  }
  
  # subset to only articles where there is complete agreement that there is inclusion, 
  # otherwise disagreement on the include_code would cause disagreement on all others
  # which rows (articles) have an exclusion?
  excl <- which(responseArray[,"include_code",] == "0", arr.ind = TRUE)
  # set NAs for exclusions
  for(i in 1:nrow(excl)){
    agreement[excl[i,1],excl[i,2]] <- NA
  }
  
  # calculate the kappa statistic
  kappa <- irr::kappam.fleiss(agreement) # automatically omits NA rows
  kappaResults$kappaValue[v] <- kappa$value
  kappaResults$kappaPValue[v] <- kappa$p.value
  
  
  # also remove outliers using clustering
  # fit two clusters, and determine if there is an outlier if one cluster only has one member
  
  # before clustering, set NAs back to 0
  agreement[is.na(agreement)] <- "0"
  
  # only do clustering if enough unique values 
  if(length(unique(c(agreement))) < 2){
    next
  }else{
    x <-kmeans(dist(t(agreement)), 2)
    
    # if there is one reviewer in a cluster (outlier)
    if(min(table(x$cluster)) == 1){ 
      clust <- names(which.min(table(x$cluster)))
      outlier <- which(x$cluster == names(which.min(table(x$cluster))))
      kappa <- irr::kappam.fleiss(agreement[,-outlier])
      kappaResults$kappaOutlierRmValue[v] <- kappa$value
      kappaResults$kappaOutlierRmPValue[v] <- kappa$p.value
      kappaResults$OutlierID[v] <- outlier
    }
  } # end of clustering
  
} # end of loop


reviewIDs <- c(AC = 1, FV = 2, LB = 3, DV = 4, JP = 5, YS=6)
kappaResults$OutlierID <- names(reviewIDs)[match(kappaResults$OutlierID, reviewIDs)]




## PLOTS

# plot the change in removing the outlier
kappaResults %>%
  filter(!is.na(kappaOutlierRmValue)) %>%
  select(variable, kappaValue, kappaOutlierRmValue, OutlierID) %>% 
  ggplot() + # aes(variable, Fleiss_Kappa, col = Method)
  geom_segment(aes(x=variable, y=kappaValue, xend = variable, yend = kappaOutlierRmValue),
               arrow = arrow(length = unit(0.25, "cm"))) +
  geom_text_repel(aes(x=variable, y= kappaValue, label = OutlierID))+
  theme(
    axis.text.x = element_text(angle = 45, hjust=1)
  )


# Plot the kappa values for each variable
kappaResults$variableGroup <- c(
  rep("Intervention", 5),
  rep("Method", 7),
  rep("Outcome", 7),
  rep("Outcome Dimension", 8),
  rep("Additional", 5),
  "Intervention" # add at the end for oro_branch
)
kappaResults$variableGroup <- factor(kappaResults$variableGroup,
                                     levels = rev(c(
                                       "Intervention","Method",
                                       "Outcome","Outcome Dimension", "Additional"
                                     )))
kappaResults <- kappaResults %>%
  group_by(variableGroup)%>%
  mutate(groupNumber = row_number())

kappaResults$variableNames <- lapply(strsplit(kappaResults$variable,"_"), paste, collapse = " ")

lims <- c(min(kappaResults$kappaValue, na.rm=T), (2*0.6)-min(kappaResults$kappaValue, na.rm=T))

ggplot(kappaResults, 
       aes(groupNumber, variableGroup, 
           label = stringr::str_wrap(variableNames, width=5), 
           fill = kappaValue))+
  geom_tile()+
  geom_text()+
  scale_fill_distiller(palette = "RdBu", limits = lims)


# plot as a cumulative graph
kappaResults <- kappaResults %>% arrange(kappaValue) 
kappaResults$KappaOrder <- seq(1:nrow(kappaResults))

kappaResults %>% 
  ggplot(aes(x=KappaOrder, y=kappaValue, fill=variableGroup, label = variable))+
  geom_point()+
  geom_hline(yintercept = 0.6, col="red")+
  geom_label_repel(max.overlaps = 15, size=3)+
  scale_x_continuous(name = "Variable x value")+
  theme(legend.position = "none")


```


```{r explore why Fleiss kappa is returning negative results}

  # create dummy dataframe with lots of agreement of one outcome
  test <- matrix(sample(as.character(c(rep("Yes", 83), rep("No", 1)))),
                 nrow=14, ncol=6)
  colnames(test) <- paste0("rater", 1:6)
  rownames(test) <- paste0("article", 1:14)
  test <- as.data.frame(test)
  for(c in 1:ncol(test)){
    test[,c]<- factor(test[,c], levels = c("Yes","No"))
  }
  
  View(test)
  irr::kappam.fleiss(test)
  
  
  
  # create dummy dataframe with agreement but more balanced examples of the two outcomes
  test <- rbind(
    matrix(sample(as.character(c(rep("Yes", 41), rep("No", 1)))),
                 nrow=7, ncol=6),
    matrix(sample(as.character(c(rep("No", 41), rep("Yes", 1)))),
                 nrow=7, ncol=6)
  )
  colnames(test) <- paste0("rater", 1:6)
  rownames(test) <- paste0("article", 1:14)
  test <- as.data.frame(test)
  for(c in 1:ncol(test)){
    test[,c]<- factor(test[,c], levels = c("Yes","No"))
  }
  
  View(test)
  irr::kappam.fleiss(test)

```


```{r add in a different type of oro_type to combine protection and restoration into conservation}

x <- temp[,variableValues[grep("oro_type",variableValues)],]
x <- x[,,1]

## Function to create new variable
oroType2Fn <- function(x){ # x is a matrix with n pubs x n variables
  x2 <- x[,-which(colnames(x) == "oro_type.N_Restoration")]
  colnames(x2)[which(colnames(x2) == "oro_type.N_Protection")] <- "oro_type.N_Conservation"
  colnames(x2) <- gsub("oro_type","oro_type2", colnames(x2))
  x2 <- gsub("oro_type","oro_type2", x2)
  
  x2[,"oro_type2.N_Conservation"] <- ifelse(
    !is.na(x[,"oro_type.N_Protection"]) | !is.na(x[,"oro_type.N_Restoration"]),
    "oro_type2.N_Conservation", NA
  )
  return(x2)
}


oroType2Array <- do.call(abind::abind,
                          list(
                            plyr::alply(
                              temp[,variableValues[grep("oro_type",variableValues)],],
                                3, oroType2Fn),
                            along = 3
                          ))

temp <- abind::abind(temp, oroType2Array, along = 2)



# list all the variables and values
variableValues <- dimnames(temp)[[2]]
values <- unlist(lapply(strsplit(variableValues, "[.]"), function(x) x[2]))
values[is.na(values)] <- variableValues[is.na(values)]
variables <- unlist(lapply(strsplit(variableValues, "[.]"), function(x) x[1]))
variablesAll <- variables
variables <- unique(variables)
variables <- variables[which(!(variables %in% c("sysrev_id","title","year","coder_1","coder_2")))]


```


```{r try Kramer 1980 kappa designed for multiple choice responses}

## Calculate kappa statistic for all the variables 
# kappa calculation method changes depending on whether the variable is multiple choice

# Function to calculate multiple choice version of kappa statistic
source(here::here("R/kramerKappa.R"))


# for each variable and publication, find the most common response
# empty data frame to store results
kappaResults <- data.frame(
  variable = variables,
  kappaValue = rep(NA, length(variables)),
  kappaPValue = rep(NA, length(variables)),
  kappaExclusionRmValue = rep(NA, length(variables)),
  kappaOutlierRmValue = rep(NA,length(variables)),
  kappaMethod = rep(NA,length(variables)),
  OutlierID = rep(NA,length(variables))
)

# which rows (articles) have an exclusion at the screening stage?
excl <- which(responseArray[,"include_code",] == "0", arr.ind = TRUE)


# Loop through all the variables
for(v in 1:length(variables)){
  # which columns hold value responses for a given variable
  temp2 <- temp[,variableValues[which(variablesAll == variables[v])],]
  temp2[is.na(temp2)] <- "No" # NA is still a choice, so change from an NA so it doesn't get removed
  
  
  # for inclusion, then most will be include so kappa test won't work
  # instead use the Kendall coefficient of concordance
  # still difficult as mostly one value
  if(variables[v] == "include_code"){
    kappaResults$kappaMethod[v] <- "Kendall's coeff of concordance"
    coeff <- irr::kendall(temp2)
    kappaResults$kappaValue[v] <- coeff$value
    kappaResults$kappaPValue[v] <- coeff$p.value
    # look for outliers
    temp2Mat <- ifelse(temp2 == "No" | is.na(temp2), 0, 1) #change to numeric so dist can be calc
    if(1 < length(unique(c(temp2Mat)))){
       x <- kmeans(dist(t(temp2Mat)),2)
       # if there is one reviewer in a cluster (outlier)
       if(min(table(x$cluster)) == 1){ 
         clust <- names(which.min(table(x$cluster)))
         outlier <- which(x$cluster == names(which.min(table(x$cluster))))
         coeff <- irr::kendall(temp2[,-outlier])
         kappaResults$kappaOutlierRmValue[v] <- coeff$value
         kappaResults$OutlierID[v] <- outlier
       }
    }
    next
  }
  
  
  # If the variable is only one option (y/n), then a regular kappa test applies
  if(length(dim(temp2)) == 2){
    
    kappaResults$kappaMethod[v] <- "Fleiss' Kappa"
    
    # first calculate regular kappa
    kappa <- irr::kappam.fleiss(temp2)
    kappaResults$kappaValue[v] <- kappa$value
    kappaResults$kappaPValue[v] <- kappa$p.value
    # then calculate kappa without exclusions
    # because the kappa statistic runs with an na.omit, 
    # equivalent to removing all rows with an NA
    # coders who excluded an article -- set all other responses to NA
    for(i in 1:nrow(excl)){
      temp2[excl[i,1],excl[i,2]] <- NA # all the variables coded for that article x coder
    }
    
    kappa <- irr::kappam.fleiss(temp2)
    kappaResults$kappaExclusionRmValue[v] <- kappa$value
    
    # then calculate kappa without outlier reviewers (if present)
    temp2Mat <- ifelse(temp2 == "No" | is.na(temp2), 0, 1) #change to numeric so dist can be calc
    if(1 < length(unique(c(temp2Mat)))){
       x <- kmeans(dist(t(temp2Mat)),2)
       # if there is one reviewer in a cluster (outlier)
       if(min(table(x$cluster)) == 1){ 
         clust <- names(which.min(table(x$cluster)))
         outlier <- which(x$cluster == names(which.min(table(x$cluster))))
         kappa <- irr::kappam.fleiss(temp2[,-outlier])
         kappaResults$kappaOutlierRmValue[v] <- kappa$value
         kappaResults$OutlierID[v] <- outlier
      }
    } # end calculating kappa for outliers
   
    
  }else{
    
    # If the variable has multiple choice, then use the Kramer (1980) method
    kappaResults$kappaMethod[v] <- "Kramer (1980)"
    
    # first calculate with all articles, just setting individual responses to NA
    # # coders who excluded an article -- set all other responses to NA
    for(i in 1:nrow(excl)){
      temp2[excl[i,1],,excl[i,2]] <- NA # all the variables coded for that article x coder
    }
    kappa <- kramerKappa(temp2)  
    kappaResults$kappaValue[v] <- kappa$k0
    kappaResults$kappaPValue[v] <- kappa$pValue
   
   
    # remove articles with disagreement in screening entirely
   kappa <- kramerKappa(temp2[-unique(excl[,1]),,]) 
   kappaResults$kappaExclusionRmValue[v] <- kappa$k0
   
   
   # then calculate kappa without outlier reviewers (if present)
   temp2melt <- reshape2::melt(temp2, varnames = c("article","variable","reviewer"))
   temp2melt$value[temp2melt$value == "No"] <- NA
   temp2melt$value <- ifelse(is.na(temp2melt$value), 0,1)
   temp2Mat <- reshape2::acast(temp2melt, reviewer~variable, sum)
   x <- kmeans(dist(temp2Mat),2)
   
   
    if(min(table(x$cluster)) == 1){ # if there is one reviewer in a cluster (outlier)
       clust <- names(which.min(table(x$cluster)))
       outlier <- which(x$cluster == names(which.min(table(x$cluster))))
       kappa <- kramerKappa(temp2[,,-outlier])
       kappaResults$kappaOutlierRmValue[v] <- kappa$k0
       kappaResults$OutlierID[v] <- outlier
    }
   
  } # end of multiple choice variables

} # end of loop

reviewIDs <- c(AC = 1, FV = 2, LB = 3, DV = 4, JP = 5, YS=6)
kappaResults$OutlierID <- names(reviewIDs)[match(kappaResults$OutlierID, reviewIDs)]


## Write table of kappa results ---------------------
writexl::write_xlsx(kappaResults,here::here("outputs","coding","codingKappa.xlsx"))


## PLOTS ------------------

# get a quick idea of what the kappa values are like
hist(kappaResults$kappaValue, breaks = seq(-0.1,1, by=0.1))


# plot the change in removing the outlier
kappaResults %>%
  filter(!is.na(kappaOutlierRmValue)) %>%
  select(variable, kappaValue, kappaOutlierRmValue, OutlierID) %>% 
  ggplot() + # aes(variable, Fleiss_Kappa, col = Method)
  geom_segment(aes(x=variable, y=kappaValue, xend = variable, yend = kappaOutlierRmValue),
               arrow = arrow(length = unit(0.25, "cm"))) +
  geom_text_repel(aes(x=variable, y= kappaValue, label = OutlierID))+
  theme(
    axis.text.x = element_text(angle = 45, hjust=1)
  )

ggsave(here::here("figures","supplementary","codingKappaOutlierChange.pdf"), width = 12, height = 12, units="cm")


# Plot the kappa values for each variable
kappaResults$variableGroup <- c(
  "Screen",
  rep("Intervention", 5),
  rep("Method", 7),
  rep("Outcome", 7),
  rep("Outcome Dimension", 8),
  rep("Additional", 5),
  rep("Intervention", 2) # add at the end for oro_branch and oro_type2
)

varGroupLevels <- rev(c("Intervention","Method",
                        "Outcome","Outcome Dimension", "Additional"))

# kappaResults$variableGroup <- factor(kappaResults$variableGroup,
#                                      levels = varGroupLevels)
kappaResults <- kappaResults %>%
  filter(variable != "include_code") %>%
  mutate(variableGroup = factor(variableGroup, levels = varGroupLevels)) %>%
  group_by(variableGroup)%>%
  mutate(groupNumber = row_number())

kappaResults$variableNames <- lapply(strsplit(kappaResults$variable,"_"), paste, collapse = " ")

lims <- c(min(kappaResults$kappaValue, na.rm=T), (2*0.6)-min(kappaResults$kappaValue, na.rm=T))

ggplot(kappaResults, 
       aes(groupNumber, variableGroup, 
           label = stringr::str_wrap(variableNames, width=5), 
           fill = kappaValue))+
  geom_tile()+
  geom_text()+
  scale_fill_distiller(palette = "RdBu", limits = lims)


# plot as a cumulative graph
kappaResults <- kappaResults %>% arrange(kappaValue) 
kappaResults$KappaOrder <- seq(1:nrow(kappaResults))

kappaResults %>% 
  filter(!is.na(kappaValue)) %>% # lead time removed
  ggplot(aes(x=KappaOrder, y=kappaValue, fill=variableGroup, label = variable))+
  geom_point()+
  geom_hline(yintercept = 0.6, col="red")+
  geom_label_repel(max.overlaps = 25, size=3)+
  scale_x_continuous(name = "Variable x value")+
  theme(legend.position = "none")

ggsave(here::here("figures","supplementary","codingKappa.pdf"), width = 20, height = 10, units="cm")
```



```{r combine set 0 into the most common answer, eval=FALSE}
load(file.path(set0Dir, "responseArray.RData"))

# function to find most frequently appearing value
fun <- function(x){
  as.numeric(names(which.max(table(x))))
}


responseDf <- apply(responseArray[,-c(1:5),], 1:2, FUN=fun) # condense to most frequently appearing value
responseDf <- as.data.frame(responseDf)
responseDf <- cbind(responseArray[,c(1:5),1], responseDf) # bind back in ID columns


# change name of coder
responseDf[,"coder_1"] <- rep("all_combined", nrow(responseDf))


# for all inclusions where common decision is to include, but could not agree on ORO, set to unclear
incl <- which(responseDf[,"include_code"] == 1 & 
               sum(responseDf[,which(colnames(responseDf) == "oro_type.M_Renewables"):which(colnames(responseDf) == "oro_type.Unclear")] == 0))
noORO <- which(rowSums(responseDf[,which(colnames(responseDf) == "oro_type.M_Renewables"):which(colnames(responseDf) == "oro_type.Unclear")], na.rm=T) == 0)

responseDf[intersect(incl, noORO), "oro_type.Unclear"] <- 1


# # save

# screening decisions
write.csv(responseDf[,1:6], 
          here::here("data","derived-data","screening","screened-records","coding-screening_set-0.csv"),row.names = FALSE)

# coding decisions
write.csv(responseDf %>% filter(include_code == 1) %>% select(-c(include_code)),
  file.path(set0Dir,"codebook_combined-formatted_set-0.csv"),row.names = FALSE)

```



## Initial sets finalised



```{r format initial sets into binary columns for predictions in distilBERT}
## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","sets")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = TRUE)
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$sysrev_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "sysrev_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("sysrev_id","title","year","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="sysrev_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)

  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

#View(responseDf)

```


Make sure that these column names are included: "impact_economy.Tourism__Recreation" "impact_economy.Informal"           
[7] "impact_economy.Livelihood"          "impact_economy.Other"

And not "impact_economy.Tourism" "impact_economy.other" --> then there is an error in the impact_economy column, row 1 of the .csv file
```{r check for consistency in column names}
colnames(responseDf)
```


```{r save screening decisions seperately, eval=FALSE}
write.csv(responseDf[,1:6], 
          here::here("data","derived-data","screening","screened-records","coding-screening_initialSets.csv"),
          row.names = FALSE)

# only keep inclusions
responseDf <- responseDf %>%
  filter(include_code == 1) %>%
  select(-c(include_code))

# responseDf <- subset(responseDf, include_code == 1)
# responseDf <- responseDf[,which(colnames(responseDf) != "include_code")]
```



```{r read in combined answers from set-0 and merge in with responses from other sets}
set0Responses <- readr::read_csv(
  here::here("data","derived-data","coding","manual-coding","set-0","codebook_combined-formatted_set-0.csv"),
  show_col_types = FALSE
  )

responseDf <- rbind(set0Responses, responseDf)

```

```{r use the lookup table to join with analysis_id duplicate_id abstract and keywords}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db) # look at all the tables in the database

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(analysis_id, duplicate_id, title, year, abstract, keywords)

# lookup table of the sysrev_id to duplicate_id
sysrev_id <- tbl(all_db, "sysrevid_2_analysisid_lookup")
sysrev_id <- sysrev_id %>% select(duplicate_id, sysrev_id)

# join to get the relevant ids
relevantData <- sysrev_id %>%
  left_join(dedup_sub, by = "duplicate_id") %>%
  collect()
relevantData$sysrev_id <- as.character(relevantData$sysrev_id)

# disconnect
RSQLite::dbDisconnect(all_db)

dim(responseDf)

# merge metadata in by sysrev_id and order columns
responseDf <- responseDf %>%
  select(-c(year)) %>%
  left_join(relevantData, by = "sysrev_id")

# compare titles to makes sure the articles are the same
print(cbind(responseDf$title.x[1:5], responseDf$title.y[1:5]))

# remove duplicate title
responseDf <- responseDf %>%
  rename(title = title.x) %>%
  select(-c(title.y))

# order columns
idCols <- c("sysrev_id","analysis_id", "duplicate_id","title","abstract","keywords","year","coder_1","coder_2")
responseDf <- responseDf[,c(idCols, colnames(responseDf)[which(!(colnames(responseDf) %in% idCols))])]

# check there are now NAs for analysis_id or duplicate_id
sum(is.na(responseDf$analysis_id))
sum(is.na(responseDf$duplicate_id))
length(unique(responseDf$duplicate_id)) == length(responseDf$duplicate_id) # check all unique ids are unique
```

```{r add responses as a table to the sqlite database}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

RSQLite::dbWriteTable(all_db, "initial_coding", responseDf, overwrite=TRUE) #

RSQLite::dbDisconnect(all_db)

```


## Supplementary sets

```{r format supplementary sets}

## READ IN FILES

# files of coding results
codingDir <- here::here("data","derived-data","coding","manual-coding","supplementary-sets")
codingFiles <- dir(codingDir)
codingFiles <- codingFiles[grepl(".csv", codingFiles)] # only include csv files

for(f in 1:length(codingFiles)){
  
  # determine whether to also produce a string of variable names 
  # only do this for the first loop
  if(f==1){
    returnVariableString = TRUE
  }else{
    returnVariableString = FALSE
  }
  
  # read in codebook and format
  df <- formatCoding2distilBert(
    codebookFp=file.path(codingDir, codingFiles[f]),
    skipLines=3,returnVariableString = returnVariableString,
    exclusions = FALSE)
  
  # change column name because actually these weren't screened in sysrev so they don't have a sysrev_id
  colnames(df$data)[1] <- "duplicate_id"
  
  
  # for now, just condense everything so it is 1 row x publication
  # columns 1:5 and last column are ID columns, and 6 onwards have data
  # condense data columns
  condensedDat <- apply(df$data[,7:ncol(df$data)-1], 2, FUN=function(x) tapply(X=x, INDEX=df$data$duplicate_id, FUN=sum, na.rm=T))
  condensedDat[condensedDat>1] <- 1 # reduce all sums to just p/a
  condensedDat <- as.data.frame(condensedDat, row.names = rownames(condensedDat))
  condensedDat <- tibble::rownames_to_column(condensedDat, "duplicate_id")
  
  # join back with corresponding id columns
  # I don't know why, but even with left_join duplciates in y cause duplication in x so need to rm duplicates first
  idCols <- df$data[,c("duplicate_id","title","year","coder_1","coder_2")]
  idCols <- idCols[!duplicated(idCols),]
  condensedDat <- condensedDat %>% left_join(idCols, by="duplicate_id")
  # get column names in right order
  colOrder <- colnames(df$data)
  colOrder <- colOrder[-which(colOrder %in% c("notes"))]
  condensedDat <- condensedDat[,colOrder]
  df$data <- condensedDat; rm(condensedDat)


  
  
  # save
  if(f==1){
    variables <- df$variables
    responseDf <- df$data
  }else{
    responseDf <- rbind(responseDf, df$data)
  }
  
}

#View(responseDf)

```

```{r check that ids are correct}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db) # look at all the tables in the database

# check for duplicates and filter out
responseDf %>%
  filter(duplicate_id == responseDf$duplicate_id[duplicated(responseDf$duplicate_id)]) %>%
  View
responseDf <- responseDf[!duplicated(responseDf$duplicate_id),]

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(analysis_id, duplicate_id, title, year, abstract, keywords) %>%
  collect()

RSQLite::dbDisconnect(all_db)

# look for title match when the dupid is matched
matches2 <- rep(NA, nrow(responseDf))

for(i in 1:length(matches2)){
  
  # what are all the possible matches?
  possibleMatches <- grep(responseDf$duplicate_id[i], dedup_sub$duplicate_id)
  
  # if only one, then simple, that's the match
  if(length(possibleMatches) == 1){
    matches2[i] <- possibleMatches
  }
  
  # if more than one possible match, look at the titles and match
  if(length(possibleMatches) > 1){
    titlematch <- stringdist::amatch(tolower(responseDf$title[i]), tolower(dedup_sub$title[possibleMatches]), 
                                     method = "osa", maxDist = 5, nomatch = NA)
    titlematch <- titlematch[!is.na(titlematch)]
    possibleMatches <- possibleMatches[titlematch[1]]
  }
  
  # assign the match id
  matches2[i] <- possibleMatches
}

# check for no NAs
sum(is.na(matches2))

# join with matches
responseDf2 <- cbind(
  dedup_sub[matches2,c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  responseDf[,-c(1:3)]
)

# check no additional duplicates were created
anyDuplicated(responseDf2)


# # text used for supplemental search
# my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))
# 
# # search for duplicate_id match
# matches2 <- rep(NA, nrow(responseDf))
# for(i in 1:length(matches2)){
#   
#   dupMatch <- which(my_text$duplicate_id == responseDf$duplicate_id[i])
#   
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"0"))
#   }
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"00"))
#   }
#   if(length(dupMatch)==0){
#     dupMatch <- which(my_text$duplicate_id == paste0(responseDf$duplicate_id[i],"000"))
#   }
#   
#   matches2[i] <- dupMatch
# }
# sum(is.na(matches2))


```



```{r add supplementary responses as a table to the sqlite database}
require(dbplyr)
require(R.utils)
require(RSQLite)

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

RSQLite::dbWriteTable(all_db, "supplementary_coding", responseDf2, overwrite=TRUE)



RSQLite::dbDisconnect(all_db)

```


# Merge all together and simplify columns


```{r merge with abstract and id metadata}

require(dbplyr)

all_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

#src_dbi(all_db) # look at all the tables in the database

# coding results from initial round
initialCoding <- tbl(all_db, "initial_coding")
initialCoding <- initialCoding %>% collect()

# coding results from supplemental round
supplementaryCoding <- tbl(all_db, "supplementary_coding")
supplementaryCoding <- supplementaryCoding %>% collect()
supplementaryCoding$sysrev_id <- rep(NA, nrow(supplementaryCoding)) # add a black sysrev_id

# check what columns they have in common and if any important ones are missing
cols <- intersect(colnames(initialCoding), colnames(supplementaryCoding))
colnames(initialCoding)[which(!(colnames(initialCoding) %in% colnames(supplementaryCoding)))]
colnames(supplementaryCoding)[which(!(colnames(supplementaryCoding) %in% colnames(initialCoding)))]

  
## Disconnect databases
dbDisconnect(all_db)


## append the data frames for initial and supplementary coding
codebookCombined <- rbind(initialCoding[,cols], supplementaryCoding[,cols])

# check IDs to make sure they are unique
length(unique(codebookCombined$duplicate_id))
length(codebookCombined$duplicate_id)
```


```{r tabulate all coding results}

# index of which columns belong to which variables
variableInd <- unlist(lapply(strsplit(colnames(codebookCombined), "[.]"), function(x) x[[1]]))

# identify variables to tabulate (i.e. not identification columns)
variables_to_tab <- unique(variableInd)
idCols <- c("sysrev_id","analysis_id","duplicate_id","title","abstract","keywords","year","coder_1","coder_2")
variables_to_tab <- variables_to_tab[which(!(variables_to_tab %in% idCols))]
  
# empty list to fill with tabulated tables (one item for each variable)
tabulated_variables <- list()

for(v in 1:length(variables_to_tab)){
  colsInd <- grep(variables_to_tab[v], variableInd, ignore.case = TRUE)
  
  if(is.character(codebookCombined[,colsInd])){next} # can't tabulate character vectors
  
  if(!is.null(dim(codebookCombined[,colsInd]))){
    tab <- colSums(codebookCombined[,colsInd], na.rm=TRUE)
    # if there are multiple values for each variable, get the variable name
    if(sum(grepl("[.]", colnames(codebookCombined)[colsInd])) > 0){
      names(tab) <- unlist(lapply(strsplit(colnames(codebookCombined)[colsInd], "[.]"), function(x) x[[2]]))
    }
  }else{
    tab <- tabulate(codebookCombined[,colsInd])
  }
  tabulated_variables[[v]] <- tab
  names(tabulated_variables)[v] <- variables_to_tab[v]
}


sink(here::here("outputs","coding","tabulated_coding_round1And2.txt"))
print(tabulated_variables)
sink()

```


```{r simplify coding variables and save as all-coding-format-distilBERT.txt}
idCols <- c("sysrev_id","analysis_id","duplicate_id","title","abstract","keywords","year","coder_1","coder_2")

source(here::here("R","simplifyCodebook.R"))

codebookCombinedSimplified <- simplifyCodebook(codebookCombined, idCols)

# check column names
colnames(codebookCombinedSimplified)

codebookCombinedSimplified <- codebookCombinedSimplified[!duplicated(codebookCombinedSimplified),]

# check for duplicates
anyDuplicated(codebookCombinedSimplified)


## Save 
# write for Vicky & save
write.table(
  codebookCombinedSimplified,
  file = here::here("data","derived-data","coding", "all-coding-format-distilBERT.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)



# save to database as well
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
src_dbi(all_db)
RSQLite::dbWriteTable(all_db, "allCodingSimplifiedVariables", codebookCombinedSimplified, overwrite=TRUE) 

RSQLite::dbDisconnect(all_db)

```



# Compare these with all the articles that were screened to come up with updated screening decisions data


```{r compile final screening decisions from both initial and supplementary coding rounds}

## Coding from initial round 
codingScreen0 <- read.csv(
  here::here("data","derived-data","screening","screened-records","coding-screening_set-0.csv"))
codingScreen <- read.csv(
  here::here("data","derived-data","screening","screened-records","coding-screening_initialSets.csv"))

# join together
codingScreen <- rbind(codingScreen, codingScreen0)
rm(codingScreen0)
codingScreen <- codingScreen %>%
  rename(reviewer = coder_1, reviewer_2 = coder_2, include_screen = include_code) %>%
  select(sysrev_id, reviewer, reviewer_2, include_screen, title)


## Add with exclusions from initial screening round
# now in screen results merged, remove articles that were in the coding screen, 
# so that it is just composed of entries that were excluded from the screening round 
# and never made it to coding

# load screen_results_merged
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData")) 
# subset to those that weren't evaluated during coding (exclusions)
initialScreen <- subset(screen_results_merged, !(sysrev_id %in% c(codingScreen$sysrev_id)))
# format column names
initialScreen <- screen_results_merged %>%
  filter(!(sysrev_id %in% codingScreen$sysrev_id)) %>%
  rename(reviewer = screener) %>%
  mutate(reviewer_2 = as.character(Double_Blind)) %>%
  select(sysrev_id, reviewer, reviewer_2, include_screen, sample_screen, title) %>%
  mutate(sample_screen = as.character(sample_screen))

# need to get sample_screen from initial screening results
codingScreen <- codingScreen %>%
  left_join(screen_results_merged %>% select(sysrev_id, sample_screen, title), by = "sysrev_id") %>%
  mutate(sample_screen = as.character(sample_screen))

# check title.x and title.y match before re-assigning title
# View(codingScreen)
codingScreen <- codingScreen %>%
  rename(title = title.x) %>%
  select(-c(title.y))


## join inital coding results with initial screening and get metadata
cols <- intersect(colnames(codingScreen), colnames(initialScreen))
codingScreen <- rbind(initialScreen[,cols], codingScreen[,cols])

# use the lookup table to join sysrev_id to analysis_id and duplicate_id
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"),
                                 create=FALSE)

sysrevIDLookup <- tbl(all_db, "sysrevid_2_analysisid_lookup")

dedups <- tbl(all_db,"uniquerefs")

RSQLite::dbDisconnect(all_db)

codingScreen.test <- codingScreen %>%
  left_join(sysrevIDLookup %>% select(sysrev_id, duplicate_id, title.x), by="sysrev_id", copy=TRUE) %>%
  left_join(dedups %>% select(duplicate_id, analysis_id, title, abstract, keywords), by = "duplicate_id", copy=TRUE)

# Check all titles match
View(codingScreen.test)
codingScreen <- codingScreen.test
# delete redundant titles
codingScreen <- codingScreen %>%
  rename(title = title.y) %>% 
  select(-c(title.x, title.x.x))


# format columns
codingScreen$include_screen <- codingScreen$include_screen == 1

```


```{r From the supplementary round}

## From the supplementary round

# get all screening results
supplementalScreenDir <- here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screening")
supplementalScreenFiles <- dir(supplementalScreenDir)

for(f in 1:length(supplementalScreenFiles)){
  temp <- read.csv(file.path(supplementalScreenDir, supplementalScreenFiles[f]))
  temp <- subset(temp, !is.na("screened_abstracts"))
  if(f==1){
    supplementalScreen <- temp
  }else{
    supplementalScreen <- rbind(supplementalScreen, temp)
  }
}
# clean
supplementalScreen <- subset(supplementalScreen, !is.na(screened_abstracts)) 
supplementalScreen <- supplementalScreen %>% 
  filter(!is.na(screened_abstracts)) # remove skipped articles
  

# add columns to be consistent with above
supplementalScreen$reviewer <- rep("Devi", nrow(supplementalScreen))
supplementalScreen$reviewer_2 <- rep(NA, nrow(supplementalScreen))
supplementalScreen$include_screen <- supplementalScreen$screened_abstracts == "selected"
supplementalScreen$sample_screen <- "supplemental coding"


## join with metadata id information

# unique reference metadata
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

dedups <- tbl(all_db, "uniquerefs")
dedup_sub <- dedups %>% 
  select(analysis_id, duplicate_id, title, year, abstract, keywords) %>%
  collect()

RSQLite::dbDisconnect(all_db)

# check for any duplicates
anyDuplicated(supplementalScreen$label)

# look for title match when the dupid is matched
matches2 <- rep(NA, nrow(supplementalScreen))

for(i in 1:length(matches2)){
  
  # what are all the possible matches?
  possibleMatches <- grep(supplementalScreen$label[i], dedup_sub$duplicate_id)
  
  # if only one, then simple, that's the match
  if(length(possibleMatches) == 1){
    matches2[i] <- possibleMatches
  }
  
  # if more than one possible match, look at the titles and match
  if(length(possibleMatches) > 1){
    titlematch <- stringdist::amatch(tolower(supplementalScreen$title[i]), tolower(dedup_sub$title[possibleMatches]), 
                                     method = "osa", maxDist = 5, nomatch = NA)
    titlematch <- titlematch[!is.na(titlematch)]
    possibleMatches <- possibleMatches[titlematch[1]]
  }
  
  # assign the match id
  matches2[i] <- possibleMatches
}

# check for no NAs
sum(is.na(matches2))


# join with matches 
supplementalScreen2 <- cbind(
  dedup_sub[matches2, c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen[,c("reviewer","reviewer_2","include_screen","sample_screen")]
)

# # remove a couple articles that were accidentially marked as include
# supplementalScreen2 <- supplementalScreen2 %>%
#   filter(!(duplicate_id %in% c("NA.217","2017.14740","2014.14080"))) 

#supplementalScreen2[which(supplementalScreen2$label == "1985.216"),]

# check that there are no nas/duplicates
supplementalScreen2$sysrev_id <- rep(NA, nrow(supplementalScreen2))
sum(is.na(supplementalScreen2$sample_screen))
length(unique(supplementalScreen2$duplicate_id)) == length(supplementalScreen2$duplicate_id)

# check titles match
left_join(supplementalScreen2, dedup_sub, by= "analysis_id") %>% 
  select(analysis_id, duplicate_id.x, duplicate_id.y, title.x, title.y) %>% View()
```

```{r join intitial and supplemental screens together into allScreens}
## Join both data frames together
# cols <- intersect(colnames(supplementalScreen2), colnames(codingScreen))
# cols <- cols[order(cols)]
# cols <- cols[c(2,3,9,6,7,8,4,10,1,5)] # re-order so its more sensible
cols <- c("analysis_id","duplicate_id","sysrev_id","reviewer","reviewer_2","sample_screen","include_screen","title","abstract","keywords")
allScreens <- rbind(
  supplementalScreen2[,cols], codingScreen[,cols]
)

# format
allScreens$reviewer <- as.factor(allScreens$reviewer)
allScreens$reviewer_2 <- as.factor(allScreens$reviewer_2)
allScreens$sample_screen <- as.factor(allScreens$sample_screen)
summary(allScreens)

# check for duplicates
anyDuplicated(allScreens$duplicate_id)
```


```{r compare screening to coding results to make sure no articles were included by accident}
# if an article is screened as include but not coded, likely an accident -- just remove to be safe ratehr than set to exclude

# load in all coding results
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables")  %>% collect()

RSQLite::dbDisconnect(all_db)


# compare screening decisions to coding

# are there any exclusions in the coding book?
which(allScreens$duplicate_id[which(allScreens$include_screen == FALSE)] 
             %in% codebookCombinedSimplified$duplicate_id)


# are there any inclusions NOT coded?
ind <- which(!(allScreens$duplicate_id[which(allScreens$include_screen == TRUE)] 
               %in% codebookCombinedSimplified$duplicate_id))
length(ind) 
# if so, filter out
if(length(ind) > 0){
  allScreens <- subset(allScreens, duplicate_id != allScreens$duplicate_id[which(allScreens$include_screen == TRUE)][ind])
}



## Do they match up by the different ids?
testMerge <- merge(allScreens, codebookCombinedSimplified, by = "duplicate_id", all.x = FALSE, all.y = TRUE)
sum(is.na(testMerge$include_screen))
nrow(testMerge) == nrow(codebookCombinedSimplified) # make sure they all have a match

testMerge2 <- merge(allScreens, codebookCombinedSimplified, by = "analysis_id", all.x = FALSE, all.y = TRUE)
sum(is.na(testMerge2$include_screen))
nrow(testMerge2) == nrow(codebookCombinedSimplified) # make sure they all have a match

dupids <- testMerge2$duplicate_id.y[which(is.na(testMerge2$include_screen))]

# View to check the titles correspond
testMerge2 %>% select(analysis_id, duplicate_id.x, duplicate_id.y, title.x, title.y) %>% View

# if there are some id's that don't have a match, investigate...
if(length(dupids)>0){
  View(testMerge2[which(is.na(testMerge2$include_screen)),c(1,10:13)])
  View(testMerge[which(testMerge$duplicate_id %in% dupids),c(1:15)])
}


# # reassign so that they match 
# testMerge <- allScreens
# matches <- match(allScreens$duplicate_id, codebookCombinedSimplified$duplicate_id)
# matches <- matches[!is.na(matches)]
# testMerge$analysis_id[matches] <- codebookCombinedSimplified$analysis_id[matches]
# # check first -- yes
# testMerge3 <- merge(testMerge, codebookCombinedSimplified, by = "analysis_id", all.x = FALSE, all.y = TRUE)
# sum(is.na(testMerge3$include_screen))
# allScreens <- testMerge

rm(testMerge, testMerge2)
```



```{r save all compiled screening results to database}
## Save to database
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
#src_dbi(all_db)
RSQLite::dbWriteTable(all_db, "allScreen_afterCoding", allScreens, overwrite=TRUE)

RSQLite::dbDisconnect(all_db)

```


```{r summarise screening stats}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
allScreens <- tbl(all_db, "allScreen_afterCoding")
allScreens <- allScreens %>% collect()

RSQLite::dbDisconnect(all_db)

# how many inclusions vs exclusions
table(allCodingJoin$include_screen)
# FALSE  TRUE 
#  2056   961 


# Coding effort
screen_effort_summary <- allScreens %>%
  filter(sample_screen != "test list") %>%
  summarise(
    DV = sum(grepl("devi", reviewer, ignore.case = TRUE)),
    JGP = sum(grepl("jean", reviewer, ignore.case = TRUE)),
    AC = sum(grepl("adrien", reviewer, ignore.case = TRUE)),
    LB = sum(grepl("bopp", reviewer, ignore.case = TRUE)),
    YS = sum(grepl("yunne", reviewer, ignore.case = TRUE)),
    FV = sum(grepl("Fred", reviewer, ignore.case = TRUE))
  )
screen_effort_summary
#     DV JGP  AC  LB  YS FV
# 1 1521 231 334 411 456 46

screen_effort_summary <- reshape2::melt(screen_effort_summary)

mean(screen_effort_summary$value) # 499.8333
sd(screen_effort_summary$value) # 521.216
median(screen_effort_summary$value) # 372.5

rm(screen_effort_summary)

```

```{r summarise coding stats}
# because the final screening was done at coding -- the coding effort can be retreived from the screen database

# Coding effort
code_effort_summary <- allScreens %>%
  filter(include_screen == TRUE) %>% 
  summarise(
    DV = sum(grepl("devi", reviewer, ignore.case = TRUE)),
    JGP = sum(grepl("jean", reviewer, ignore.case = TRUE)),
    AC = sum(grepl("AC", reviewer, ignore.case = TRUE)),
    LB = sum(grepl("Laurent", reviewer, ignore.case = TRUE)),
    YS = sum(grepl("yunne", reviewer, ignore.case = TRUE)),
    FV = sum(grepl("Fred", reviewer, ignore.case = TRUE))
  )
code_effort_summary
#    DV JGP AC LB YS FV
# 1 657  31 93 81 33 42

code_effort_summary <- reshape2::melt(code_effort_summary)

mean(code_effort_summary$value) # 156.1667
sd(code_effort_summary$value) # 246.7164
median(code_effort_summary$value) # 61.5

rm(code_effort_summary)
```








# Make another screening dataframe that is by ORO_branch rather than inclusion vs exclusion

```{r}
## read in 

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)
#src_dbi(all_db)

# combined screening table
allScreens <- tbl(all_db, "allScreen_afterCoding") %>% collect()

# coding decisions
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables") %>% collect()

RSQLite::dbDisconnect(all_db)


# check that the two datasets line up
testMerge <- allScreens %>%
  select(analysis_id, duplicate_id, title, 
         sample_screen, include_screen) %>%
  right_join(codebookCombinedSimplified %>% 
              select(analysis_id, duplicate_id, title),
            by = "analysis_id")
summary(testMerge)
testMerge %>% arrange(analysis_id) %>% View()
nrow(testMerge) == nrow(codebookCombinedSimplified)



## for the inclusions, indicate which ORO branch it is
allScreens2 <- allScreens %>%
  select(sysrev_id, analysis_id, duplicate_id, reviewer, reviewer_2, title, abstract, keywords, 
         sample_screen, include_screen) %>%
  left_join(codebookCombinedSimplified %>% 
              select(duplicate_id, year, oro_branch.Mitigation,oro_branch.Nature, 
                     oro_branch.Societal, oro_branch.Unclear),
            by = "duplicate_id") %>%
  mutate(oro_present = rowSums(
    cbind(oro_branch.Mitigation,oro_branch.Nature,oro_branch.Societal, oro_branch.Unclear), 
    na.rm = TRUE)) 





# make sure there are no mis-matches -- there is a coding result for every inclusion
length(which(allScreens2$include_screen == FALSE & allScreens2$oro_present == 1))  
length(which(allScreens2$include_screen == TRUE & allScreens2$oro_present == 0))
#View(allScreens2[which(allScreens2$include_screen == TRUE & allScreens2$oro_present == 0),])
allScreens2$oro_present <- NULL

# make sure all unique values
anyDuplicated(allScreens2$duplicate_id)


## Write for Vicky
write.table(
  allScreens2,
  file = here::here("data","derived-data","screening","screened-records","all-screen-results_afterSupplementalCoding.txt"),
  row.names=F, col.names=TRUE, sep='\t', quote=FALSE
)

```

# Checks

```{r check that the dataframes match up}





```




# Junk

```{r supplementary coding}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                 create=FALSE)

src_dbi(all_db)

# coding results
supCoding <- tbl(all_db, "supplementary_coding")
supCoding <- supCoding %>%
  select(-c(title, year, coder_1, coder_2))

# unique reference metadata
dedups <- tbl(all_db, "uniquerefs")
dedups <- dedups %>% 
  select(analysis_id, title, abstract, keywords)

# join together
supCodingDf <- left_join(supCoding, dedups, by = "analysis_id") %>% collect() 

# check for NAs
sum(is.na(supCodingDf$abstract))
View(supCodingDf[is.na(supCodingDf$abstract),])

idCols <- c("duplicate_id","title","abstract","keywords")
supCodingDf <- supCodingDf[,c(idCols, colnames(supCodingDf)[which(!(colnames(supCodingDf) %in% idCols))])]
summary(supCodingDf[,idCols])


# but there are some which have not abstract matches
RSQLite::dbDisconnect(all_db)
```



```{r Using title to match supplemental screens metadata -- doesn't work well}

# Using title to match supplemental screens metadata -- doesn't work well
# supplemental coding
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"),
                                 create=FALSE)
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables")
codebookCombinedSimplified <- codebookCombinedSimplified %>% collect()
RSQLite::dbDisconnect(all_db)

supplementalScreen_inclusions <- subset(supplementalScreen, include_screen == TRUE)
  
matchesInc <- rep(NA, nrow(supplementalScreen_inclusions))

for(i in 1:length(matchesInc)){
  matchesInc[i] <- stringdist::amatch(tolower(supplementalScreen_inclusions$title[i]), 
                                   tolower(codebookCombinedSimplified$title), method = "osa", maxDist = 5, nomatch = NA)
}
sum(is.na(matchesInc))

supplementalScreen_inclusions <- cbind(
  codebookCombinedSimplified[matchesInc, c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen_inclusions[,c("reviewer", "reviewer_2","include_screen","sample_screen")]
)


# text used for supplemental search
supplementalScreen_exclusions <- subset(supplementalScreen, include_screen == FALSE)
my_text <- readRDS(here::here("data","derived-data","coding","keyword-matches-supplemental-coding","screeningText.rds"))

# find title matches-- takes several minutes to run
matchesExc <- rep(NA, nrow(supplementalScreen_exclusions))

for(i in 1:length(matchesExc)){
  #yearInd <- which(my_text$year == supplementalScreen_exclusions$year[i])
  matchesExc[i] <- stringdist::amatch(tolower(supplementalScreen_exclusions$title[i]), tolower(my_text$title), method = "osa", maxDist = 5, nomatch = NA)
}
sum(is.na(matchesExc))

supplementalScreen_exclusions <- cbind(
  my_text[matches,c("analysis_id","duplicate_id","title","abstract","keywords","year")],
  supplementalScreen_exclusions[,c("reviewer", "reviewer_2","include_screen","sample_screen")]
)


supplementalScreen <- rbind(supplementalScreen_exclusions, supplementalScreen_inclusions)

save(supplementalScreen, matchesExc, matchesInc,
      file = here::here("data","derived-data","coding","keyword-matches-supplemental-coding","supplementalScreenMatches.RData"))
```
