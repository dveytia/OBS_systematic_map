---
title: "04-organizing_all_search_results"
author: "Devi Veytia"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r load libraries}
library(dplyr)
library(R.utils)
library(RSQLite)
library(revtools)

```

```{r source functions}
source(here::here("R","cleaning-and-formatting-bib-dataframes.R"))
```

```{r set up inputs}
##  set up inputs

# Do you want to overwrite the existing sql databases?
sqlOverwrite = TRUE


## Directory where the folders containing the results for the different blocks 
resultsPath <- here::here("data","raw-data") # where the database folders are located
dbFolderNames <- c("WOS_downloads", "Scopus_downloads") # names of the database folders


## Sql file for the sampling of the records for screening
sqlite_file <- file.path(resultsPath, dbFolderNames[1],"sql-databases","sample-wos-refs.sqlite")
table_name <- "wosrefs"


## SQL files for de-duplication

# all references before duplicate identification
sqlite_all_ref_file <- file.path(resultsPath,"sql-databases","all-refs.sqlite")
sqlite_all_ref_table_name <- "allrefs" 
# # version of the database file
v = "v2"
sqlite_all_ref_file_version <- paste0(gsub(".sqlite","",sqlite_all_ref_file),"_",v,".sqlite")

# joined with duplicate id
sqlite_all_ref_file_version_dedupid <- paste0(gsub(".sqlite","",sqlite_all_ref_file_version),
                                              "_join-duplicate_id.sqlite")
sqlite_allRefDedupID_table_name <- "allrefs_join"

# just the unique references
sqlite_unique_ref_file <- file.path(resultsPath,"sql-databases", paste0("unique-refs_",v,".sqlite"))
sqlite_unique_ref_table_name <- "uniquerefs"


## number of articles of include in double blibd screening
nDoubBlind <- 300



## Coding pdf writing 

# whether to overwrite set folder for coding pdfs
codingPdfSetFolderOverwrite <- TRUE

```


The idea is to download all the search records from WOS and Scopus, de-duplicate, then merge with the screening results so that we don't screen the same articles twice. 
- screen subsets (yes, sort by relevance in colandr is better)
- download decisions that have already been made in pilot testing
- de-duplicate search results (doi and title)
- remove records that have already been screened(?,yes) (maybe by merging based on doi?)
- format and export
- 



Note for Colandr:
* can upload .ris, .bib, .txt (need to have the information that an .ris or .bib file would have). Might be the best option because then studies will have unique IDs? 
* imported citaions cannot be deleted
* max single import size is 40MB (15-20k of citations)


Note for sysrev:
* import in .ris
* max file size is 7MB - best to upload ~2000 refs at a time

When exporting search results from WOS, do it as a .bib file, then de-duplicate using revtools. Note I get an error when trying to download the complete view, can only do standard


# Subsetting results for screening

It would take too long to de-duplicate all references, so what I will do instead is randomly select .bib files from each search string block, write to an sql database to sample from.

From this sampled database, de-duplicate based on title and year
* load in all .bib files into R: only columns for doi and title
* create unique ID based on fuzzy title and year (Callaghan et al 2021)
* grouping by unique ID, combine data from different rows to create a single row containing all the available data see: https://stackoverflow.com/questions/44206714/remove-least-complete-duplicate-rows-in-r-or-sql

Then divvy them up into screening sets:
* 300 refs for double blind screening
* 4000 for the first set of individual screening -- more might be needed depending on the proportion of relevant to irrelevant.




```{r load in randomly selected bib files from each block and write to an sql database to sample from}
## Set up inputs

# max number of record files to read per block
# if number of files > maxFileNo, then maxFileNo will be sampled from available files
maxFileNo <- 10


# Colnames to extract
colnames2extract <- c("type","author","title","journal","year","volume","number","pages","abstract","publisher","address","affiliation","doi","issn","eissn","keywords","keywords_plus","research_areas","web_of_science_categories","author_email","affiliations","orcid_numbers", "web_of_science_index", "unique_id", "funding_acknowledgement", "funding_text", "article_number", "researcherid_numbers","editor", "booktitle","isbn", "book_group_author", "organization", "series")



## use inputs to extract file paths
resultsFolders <- dir(file.path(resultsPath, dbFolderNames[1]))
resultsFolders <- resultsFolders[grep("23", resultsFolders)]
resultsFolders <- resultsFolders[order(resultsFolders)]

# get names for the different blocks
blockNames <- unlist(lapply(str_split(resultsFolders,"_"), function(x) x[[1]]))



## loop through all the files to write to an sql database
start <- Sys.time()

for(i in 1:length(resultsFolders)){
  
  cat(paste(blockNames[i], i,"/of", length(blockNames), "\n"))
  
  blockFiles <- dir(file.path(resultsPath, dbFolderNames[1], resultsFolders[i]))
  
  if(length(blockFiles) > maxFileNo){
    blockFiles <- blockFiles[sample(1:length(blockFiles),size = maxFileNo, replace = FALSE)]
  }
  
  # for each of the wos record files in the directory
  for(f in 1:length(blockFiles)){
    
    cat(paste("   file", f, "of", length(blockFiles),"\n"))
    
    # read in bibtex file -- if there is an error skip to the next
    skip_to_next <- FALSE
    tryCatch(
      tempDf <- revtools::read_bibliography(here::here(resultsPath,dbFolderNames[1], resultsFolders[i], blockFiles[f])),
      error = function(e){skip_to_next <<- TRUE}
    )
    if(skip_to_next){next}
    
    # Ensure order and naming of columns is consistent
    # create empty columns if standard columns are missing
    if(sum(!(colnames2extract %in% colnames(tempDf))) > 0){
      tempDf[,colnames2extract[which(!(colnames2extract %in% colnames(tempDf)))]] <- NA
    }
    # subset to only specific columns 
    tempDf <- tempDf[,colnames2extract]
    
    # add a column ID to indicate which block it came from
    tempDf$source_string <- paste(blockNames[i])
    
    # set overwrite commands and appending commands
    if(i==1 & f==1 & sqlOverwrite){
      overwritel = TRUE; appendl = FALSE
    }else{
      overwritel = FALSE; appendl = TRUE
    }
    
    # write to database
    db <- RSQLite::dbConnect(RSQLite::SQLite(), dbname = sqlite_file)
    dbWriteTable(db, "refs", tempDf, append=appendl, overwrite = overwritel)
    dbDisconnect(db)
    
    
  } # end processing the .bib file
  
} # end looping through the block folder

end <- Sys.time()

start-end
```


```{r sample references for each substring and de-duplicate}
set.seed(123)

wos_id_db <- src_sqlite(sqlite_file, create=FALSE)
wos_ids <- tbl(wos_id_db, "refs")

# select 1500 references from each block to give 7500 to work with
wos_id_results <- wos_ids %>%
  filter(!is.na(doi), !is.na(title), !is.na(abstract)) %>%
  #select(unique_id, source_string) %>%
  group_by(source_string)%>%
  slice_sample(n=1500)

wos_id_results <- as.data.frame(wos_id_results)
dim(wos_id_results)

wos_id_results$title_id <- revtools::find_duplicates(wos_id_results, match_variable = "title",
                                             match_function = "stringdist", method = "osa", threshold = 5,
                                             remove_punctuation = TRUE, to_lower = TRUE)

# remove title and year duplicates
# for each unique combination of title and year, 
# combine data from different rows to create a single row containing all the available data
# note if there are conflicting data for a column with the same ID slice will take the first value
wos_id_results2 <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep =".")) %>%
  group_by(title_year_id) %>%
  slice(which.min(rowSums(is.na(.)))) %>%
  as.data.frame()

nrow(wos_id_results2)


wos_id_results <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep ="."))

dupedIds <- wos_id_results$title_year_id[duplicated(wos_id_results$title_year_id)]
for(i in 1:length(dupedIds)){
  tempDf <- wos_id_results %>%
   filter(title_year_id %in% dupedIds[i]) %>% 
   slice(which.min(rowSums(is.na(.)))) %>%
   as.data.frame()
  if(i==1){
    dedupedResults <- tempDf
  }else{
    dedupedResults <- rbind(dedupedResults, tempDf)
  }
}

# join together into one dataframe with unique references
nondupedResults <- wos_id_results %>%
  filter(!(title_year_id %in% dupedIds)) %>%
  as.data.frame()
unique_wos_results <- rbind(dedupedResults, nondupedResults)
rm(dedupedResults, nondupedResults)

# number of duplicates removed -- 121
nrow(wos_id_results)-nrow(unique_wos_results)

# add a column indicating that it is a random sample
unique_wos_results$random_sample <- paste("yes")

# ensure that title and abstract are not na
unique_wos_results <- unique_wos_results %>%
  filter(!is.na(title), !is.na(abstract))

# # write to a new file
# saveRDS(unique_wos_results,
#         here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sample.rds"))

```

Depending on how the records need to be formatted, save as appropriate file type

```{r using the dedeuplicated sample of references create a subsample of articles to upload for screening}
set.seed(123)

# load in the random sample of deduplicated wos results
unique_wos_results <- readRDS(
  here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sample.rds")
)

# from this, divide into sets of articles to screen with proportions based on block
# because renewable options compose 1/3 of the mitigation typology, the 1000 references for mitigation options is distributed accordingly-
# 1/3 are retrieved from the renewable block, and 2/3 from the mitigation block
sampleInstructions <- data.frame(
  source_string = unique(unique_wos_results$source_string),
  n = c(1000, round(1000*2/3), 1000, round(1000*1/3), 1000)
)

# using the sampling instructions create a dataframe with a random sample
for(i in 1:nrow(sampleInstructions)){
  
  tempDf <- unique_wos_results %>%
    filter(source_string == sampleInstructions$source_string[i]) %>%
    sample_n(size = sampleInstructions$n[i])
  
  if(i==1){
    sampleDf <- tempDf
  }else{
    sampleDf <- rbind(sampleDf, tempDf)
  }
  
}


# seperate out into double blind and regular sets 
indDoubBlind <- sample(1:nrow(sampleDf), nDoubBlind, replace=FALSE)
sampleDf$Double_Blind <- "no"
sampleDf$Double_Blind[indDoubBlind] <- "yes"

sampleDf_singleScreen <- sampleDf %>% filter(Double_Blind == "no")

# ## Write files
# # save sampled Df
# saveRDS(sampleDf,
#         here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sub-sample1_Df.rds"))



# # format files for upload into screening platform and write
# # potential to group by source to better help with the relevance ranking algorithm -- but decided not to
# 
# revtools::write_bibliography(x = sampleDf_singleScreen[1:round(nrow(sampleDf_singleScreen)/2),],
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample_1.1.ris"),
#                             format="ris")
# 
# revtools::write_bibliography(x = sampleDf_singleScreen[(round(nrow(sampleDf_singleScreen)/2)+1):nrow(sampleDf_singleScreen),],
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample_1.2.ris"),
#                             format="ris")
# 
# revtools::write_bibliography(x = sampleDf %>% filter(Double_Blind == "yes"),
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample1_double-blind.ris"),
#                             format="ris")

```


```{r add in test list items to random screening subsample}

## load in articles

# test list articles
test_list_short <- readxl::read_excel(here::here("data", "derived-data", "screen_test_list", "test-list_2023-01-19.xlsx"))

test_list <- revtools::read_bibliography(
  file = here::here("data", "derived-data", "screening","records-to-screen",
                   "test-list_indexedInWos.bib"))



# subset to the test list articles that have been shortened based on revised relevance
test_list <- test_list[which(test_list$doi %in% test_list_short$doi),]

# remove duplicate doi that causes unrelated article
test_list <- test_list %>%
  filter(tolower(language) == "english")

# subset already being screened
sampleDf <- readRDS(here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sub-sample1_Df.rds"))



## make sure there are no duplicates with subset already being screened
# check for duplicates and remove if present
dups <- which(test_list$doi %in% sampleDf$doi)
if(length(dups) > 0){
  test_list <- test_list[-dups,]
}



# write this to a new bib file to be screened with the random subset
revtools::write_bibliography(x = test_list,
                             filename = here::here("data","derived-data","screening","records-to-screen","add_test-list-articles.ris"),
                            format="ris")


```


```{r missed a couple articles because the test list file was incorrect, so add in}
## Load in files

# load FULL test list
test_list <- revtools::read_bibliography(
  file = here::here("data", "derived-data", "screen_test_list", 
                   "test-list_indexed_in_WOS_2023-02-07.bib")
)
# subset to the test list articles that have been shortened based on revised relevance
test_list_short <- readxl::read_excel(
  here::here("data", "derived-data", "screen_test_list", "test-list_2023-01-19.xlsx"))
test_list <- test_list[which(test_list$doi %in% test_list_short$doi),]

# remove duplicate doi that causes unrelated article
test_list <- test_list %>%
  filter(tolower(language) == "english")

# list of titles of test list articles already added
# test_list_already_added <- screen_results%>% filter(sample_screen == "test list") %>% select(title) %>% as.vector()
# saveRDS(test_list_already_added, here::here("data","derived-data","screening",
#                                             "records-to-screen","50testListTitlesAlreadyAdded.rds"))
test_list_already_added <- readRDS(here::here("data","derived-data","screening","records-to-screen",
                                              "50testListTitlesAlreadyAdded.rds"))



## remove the already added articles

# create empty dataframe with just already added titles
tempDf <- matrix(nrow=length(test_list_already_added$title), ncol = ncol(test_list))
colnames(tempDf) <- colnames(test_list)
tempDf <- as.data.frame(tempDf)
tempDf$title <- test_list_already_added$title

# merge with full test list
missedTestList <- rbind(tempDf, test_list)

# then dedup by title
missedTestList$dups <- revtools::find_duplicates(missedTestList, "title")
missedTestList <- missedTestList %>%
  filter(n_duplicates == 1)
dim(missedTestList) # check that it is the right number of articles


## write to a file again to be uploaded to sysrev
revtools::write_bibliography(x = missedTestList,
                             filename = here::here("data","derived-data","screening","records-to-screen","add_test-list-articles_2.ris"),
                            format="ris")

```


# Organizing screened results for coding

```{r read in screened results}

## Load in csv export of screened records from sysrev
screenedRecs_dir <- here::here("data","derived-data","screening","screened-records")
screenedRecs_files <- dir(screenedRecs_dir)
screenedRecs_files <- screenedRecs_files[grepl("screen-decisions",screenedRecs_files)]


## colnames to get for standard formatting
screenColnamesIn <- c("Article.ID","Title","Journal","Authors","Test.list.article","Users", 
                           "Include","reason_excluded","Double_Blind","Filtered",
                      "X..1.ORO","Outcome.section","blue.carbon.flux.storage","User.Notes")
screenColnamesOut <- c("sysrev_id","title","source_title","author", "test_list_article","screener",
                       "include_screen","reason_excluded_screen",
                       "Double_Blind","filtered_screen","multiple_oro_screen", 
                       "outcome_section","blue_carbon_flux_storage","notes")
screenColnames <- cbind(screenColnamesIn, screenColnamesOut)
screenColnames # check they correspond

# which columns are to be formatted as a boolean vector
BooleanColnames <- c("test_list_article", "include_screen", "filtered_screen", "multiple_oro_screen", 
                     "outcome_section","blue_carbon_flux_storage")



## Read in files and bind together into one dataframe with desired columns

for(f in 1:length(screenedRecs_files)){
  tempDf <- read.csv(file.path(screenedRecs_dir, screenedRecs_files[f]))
  
  # filter to only consistent or resolved records, no conflicts
  tempDf <- subset(tempDf, Status %in% c("consistent", "resolved","single"))
  
  # add a column indicating reason for exclusion
  tempDf$reason_excluded <- NA
  for(i in 1:nrow(tempDf)){
    if(tempDf$Relevant.Population[i] == "false"){
      tempDf$reason_excluded[i] <- "Population"
    }else if(tempDf$Relevant.Intervention[i] == "false"){
      tempDf$reason_excluded[i] <- "Intervention"
    }else if(tempDf$Relevant.Outcome[i] == "false"){
      tempDf$reason_excluded[i] <- "Outcome"
    }else{
      next
    }
  }
  
  # extract relevant columns
  tempDf <- tempDf[,which(colnames(tempDf) %in% screenColnamesIn)]
  # add missing columns
  missingCols <- screenColnamesIn[!(screenColnamesIn %in% colnames(tempDf))]
  tempDf[,missingCols] <- NA
  
  # order columns into standard order
  tempDf <- tempDf[,c(screenColnamesIn)]
  
  # rename columns
  colnames(tempDf) <- screenColnamesOut
  
  # identify if it was double blind
  tempDf$Double_Blind <- rep(grepl("double-blind", screenedRecs_files[f]), nrow(tempDf))
  
  # bind results together
  if(f==1){
    screen_results <- tempDf
  }else{
    screen_results <- rbind(screen_results, tempDf)
  }
  
  if(f==length(screenedRecs_files)){rm(tempDf)}
}



## Format Columns

# boolean column types
for(c in 1:length(BooleanColnames)){
  ind <- which(colnames(screen_results) == BooleanColnames[c])
  screen_results[,ind] <- ifelse(screen_results[,ind] == "true", TRUE, FALSE)
  screen_results[,ind] <- ifelse(is.na(screen_results[,ind]), FALSE, screen_results[,ind])
  
}


# make one column to indicate how the record was sampled (e.g. random, test list, other)

screen_results$sample_screen <- ifelse(screen_results$filtered_screen, "relevance sort", "random")
screen_results$sample_screen <- ifelse(screen_results$test_list_article, "test list", screen_results$sample_screen)
screen_results$sample_screen <- factor(screen_results$sample_screen, 
                                          levels = c("random","relevance sort","test list"))

# factor reason excluded
screen_results$reason_excluded_screen <- factor(
  screen_results$reason_excluded_screen, levels = c("Population","Intervention","Outcome",NA)
)

# now reorder columns
screen_results$test_list_article <- NULL
screen_results$filtered_screen <- NULL
screenColnamesOut2 <- screenColnamesOut[-which(screenColnamesOut == "filtered_screen")]
screenColnamesOut2[which(screenColnamesOut == "test_list_article")] <- "sample_screen"
screen_results <- screen_results[,c(screenColnamesOut2)]





# produce summary of counts of different categories
summary(screen_results)

```


```{r merge with other metadata because some was lost in sysrev export}

## load in file with metadata of the records I screened
sampleDf <- readRDS(here::here("data","derived-data",
                               "screening","records-to-screen","de-duplicated_wos_sub-sample1_Df.rds"))
sampleDf <- revtools::extract_unique_references(sampleDf, "title_year_id")
# add_testlist <- revtools::read_bibliography(
#   here::here("data", "derived-data", "screening","records-to-screen", 
#                    "test-list_indexedInWos.bib")
# )
sampleDf$source <- "sampleDf"
add_testlist <- revtools::read_bibliography(
  file = here::here("data", "derived-data", "screen_test_list", 
                   "test-list_indexed_in_WOS_2023-02-07.bib")
)
add_testlist$source <- "testlist"
sampleDf_addTestList <- rbind(
  add_testlist[,intersect(colnames(sampleDf), colnames(add_testlist))],
  sampleDf[,intersect(colnames(sampleDf), colnames(add_testlist))]
)



## dataframe to find matches between screen results and dataframe with metadata
# just use title, but keep author for manual checks of dups
screen_results2 <- screen_results[,c("title","author")] 
screen_results2$source <- "screen_result"
sampleDf2 <- sampleDf_addTestList[,c("title","author","source")]
# sampleDf2$source <- "whole_ref"
matchingDf <- rbind(screen_results2, sampleDf2)


# merge in columns with metadata I want in my coding form
matchingDf$titleMatch <- revtools::find_duplicates(matchingDf, "title")



##  join matches to the original dataframe

# for screening results
screen_results3 <- merge(screen_results, matchingDf[which(matchingDf$source == "screen_result"),])
screen_results3$source <- NULL

# for full record
sampleDf3 <- merge(sampleDf_addTestList, matchingDf[which(matchingDf$source != "screen_result"),])
sampleDf3$source <- NULL
View(sampleDf3[which(duplicated(sampleDf3$titleMatch)),]) # check any remaining duplicates
sampleDf3 <- sampleDf3[-which(duplicated(sampleDf3$titleMatch))[1:2],] # remove duplicates manually
sampleDf3 <- revtools::extract_unique_references(sampleDf3, "titleMatch") # or can also automate 

# merge dataframes together and remove duplicate columns that I am not using to merge
removeCols <- intersect(colnames(screen_results3), colnames(sampleDf3))
removeCols <- removeCols[-which(removeCols == "titleMatch")]
keepCols <- colnames(sampleDf3)[which(!(colnames(sampleDf3) %in% removeCols))]
screen_results_merged <- merge(
  screen_results3, sampleDf3[,keepCols], by=c("titleMatch"), all.y = FALSE, all.x = TRUE
)

# check there are no nas in doi so that I can match easily with deduplicated database
sum(is.na(screen_results_merged$doi))




# clean confusing columns
screen_results_merged <- screen_results_merged %>% select(-titleMatch, -n_duplicates)

# double check for duplicates
dups <- revtools::find_duplicates(screen_results_merged, "title", group_variables = "year")
if(anyDuplicated(dups)){
  screen_results_merged <- extract_unique_references(screen_results_merged, dups)
  screen_results_merged <- screen_results_merged %>% select(-n_duplicates)
}



# order columns
firstCols <- c("sysrev_id","title","year","doi","source_title","author", "screener","include_screen","multiple_oro_screen","outcome_section","blue_carbon_flux_storage","notes")
lastCols <- colnames(screen_results_merged)[which(!(colnames(screen_results_merged) %in% firstCols))]
screen_results_merged <- screen_results_merged[,c(firstCols, lastCols)]


# summarise
summary(screen_results_merged)
# note that not all test list articles were screened because while they were in the sample, we did not get to screening them


# save
write.csv(screen_results_merged, 
     file= here::here("data","derived-data","screening","screened-records","screen_results_merged.csv"))
save(screen_results_merged,
     file = here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))
```

```{r make a lookup table between sysrev_id and analysis_id}
# load in the data of screening results and metadata
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))

# connect to database with metadata from full corpus
# metadata from full corpus
dbcon <- dbConnect(RSQLite::SQLite(), 
                   here::here("data","raw-data","sql-databases","all_tables_v1.sqlite"), 
                   create = FALSE)
src_dbi(dbcon)
allrefs_join <- tbl(dbcon, "allrefs_join")

allrefs_join <- allrefs_join %>% 
  filter(!is.na(doi)) %>%
  collect() 

dbDisconnect(dbcon)

## Join with the screened results
id_lookup <- merge.data.frame(
  x = screen_results_merged %>% select(sysrev_id, title, year, doi),
  y = allrefs_join %>% select(analysis_id, duplicate_id, doi, title),
  by = "doi",
  all.x = TRUE, all.y=FALSE
)
id_lookup <- id_lookup[!duplicated(id_lookup[,c("sysrev_id")]),]

nrow(id_lookup)
nrow(screen_results_merged)

```



```{r summarise screening results}
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))

screen_effort_summary <- screen_results_merged %>%
  filter(sample_screen != "test list") %>%
  summarise(
    DV = sum(grepl("devi", screener)),
    JGP = sum(grepl("jean", screener)),
    AC = sum(grepl("adrien", screener)),
    LB = sum(grepl("bopp", screener)),
    YS = sum(grepl("yunne", screener))
  )
screen_effort_summary

#     DV JGP  AC  LB  YS
# 1 1170 245 454 527 536

screen_effort_summary <- reshape2::melt(screen_effort_summary)

mean(screen_effort_summary$value) # 586.4
sd(screen_effort_summary$value) # 346.6775
median(screen_effort_summary$value) # 527

rm(screen_effort_summary)

```

# Write included articles to pdfs for coding


```{r write the articles that need to be screened into pdf files}
require(rmarkdown)
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))

# only write the inclusions to pdf
write2pdf <- subset(screen_results_merged, include_screen == "TRUE")


# write into set folders with a max number of references
setSize <- 50
newSet <- seq(1, nrow(write2pdf), by=setSize)


# within each set, randomly select articles to double check
set.seed(100)
n_dbCheck <- 2 # number of articles per set to double check

for(s in 1:length(newSet)){
  
  # where the set starts
  start <- newSet[s] 
  
  # where the set ends
  end <- newSet[s]+setSize-1
  if(end > nrow(write2pdf)){
    end <- nrow(write2pdf)
  }
  
  # randomly sample within that range
  result <- sample(start:end, n_dbCheck)
  
  # save to a vector
  if(s==1){
    dbCheck <- result
  }else{
    dbCheck <- c(dbCheck, result)
  }
  
}


# filepath
codingpdf.fp <- here::here("data","derived-data","coding","coding-pdfs")

# # compile a list of files that are already written to pdf so I can skip writing
# already_written <- dir(here::here("data","derived-data","coding","coding-pdfs-training"))



## Loop through and write all files to pdf
for(i in 1:nrow(write2pdf)){
  
  
  # For each set of 50, 
  # write a new folder if the first file of a group of 50
  if(i %in% newSet){
    
    setNo <- which(newSet == i)
    setDirName <- paste("set", setNo, sep="-")
    
    # create directory to write to
    if(dir.exists(file.path(codingpdf.fp, setDirName)) & codingPdfSetFolderOverwrite){
      dir.create(file.path(codingpdf.fp, setDirName))
    }else if(!dir.exists(file.path(codingpdf.fp, setDirName))){
      dir.create(file.path(codingpdf.fp, setDirName))
    }else if(dir.exists(file.path(codingpdf.fp, setDirName)) & isFalse(codingPdfSetFolderOverwrite)){
      print("error: directory exists and overwrite is set to false")
      break
    }
    
    # write the metadata to a csv file inside the directory
    endSet <- newSet[setNo]+setSize-1
    if(endSet > nrow(write2pdf)){endSet <- nrow(write2pdf)}
    metadataTemp <- write2pdf[i:endSet,]
    write.csv(
      metadataTemp,
      file.path(codingpdf.fp, setDirName, paste0(setDirName,"-screening-metadata.csv"))
    )
    
  }
  
  
  # For each file,
  # get filename for the pdf
  # the title_year
  filename <- paste(gsub(" ","-",
                         stringr::str_trim(
                      gsub('[[:punct:] ]+',' ',
                           tolower(
                             stringr::str_trunc(write2pdf$title[i], width = 40))))),write2pdf$year[i],sep="_")
  
  # if the file needs to be double checked, prefix with sysrevid_X_, else sysrevid
  if(i %in% dbCheck){
    filename <- paste(write2pdf$sysrev_id[i],"X",filename, sep="__")
  }else{
    filename <- paste(write2pdf$sysrev_id[i],filename, sep="_")
  }
  
  # append with .pdf
  filename <- paste0(filename,".pdf")
  filename
  
  
  #if(filename %in% already_written){next}
  
  # write text to an .Rmd file
  sink("temp.txt")
  cat("#",write2pdf$title[i])
  cat("\n","\n")
  cat(write2pdf$author[i])
  cat("\n","\n")
  cat(write2pdf$year[i])
  cat("\n","\n")
  cat(write2pdf$source_title[i])
  cat("\n","\n")
  cat("###",write2pdf$keywords[i])
  cat("\n","\n","\n","\n")
  cat(write2pdf$abstract[i])
  cat("\n","\n")
  cat("*Research areas: ",write2pdf$research_areas[i],"*")
  sink()
  
  # Render to a pdf
  my_text <- readLines("temp.txt")
  cat(my_text, sep="  \n", file = "my_text.Rmd")
  render("my_text.Rmd", pdf_document(),
         output_file = file.path(codingpdf.fp, setDirName, filename) 
         ) #)
  # if the last entry, clean up temporary files that aren't needed
  if(i==nrow(write2pdf)){
    file.remove("temp.txt")
    file.remove("my_text.Rmd")
  }
}


```

```{r get column data for coding training}

already_written <- dir(here::here("data","derived-data","coding","coding-pdfs-training"))
trainIDs <- strsplit(already_written,"_")
trainIDs <- lapply(trainIDs, function(x) x[[1]])
trainIDs <- unlist(trainIDs)

codingTrainingMetadata <- subset(screen_results_merged, sysrev_id %in% trainIDs)

writexl::write_xlsx(codingTrainingMetadata, 
                    here::here("data","derived-data","coding","coding-pdfs-training_metadata.xlsx"))
```


# Reformat all the WOS and Scopus records into standard formatting

Things to include in the dataframe:
* Which database it came from
* database unique identifier
* Other metadata: colnames2extract <- c("type","author","title","journal","year","volume","number","pages","abstract","publisher","address","affiliation","doi","issn","eissn","keywords","keywords_plus","research_areas","web_of_science_categories","author_email","affiliations","orcid_numbers", "web_of_science_index", "unique_id", "funding_acknowledgement", "funding_text", "article_number", "researcherid_numbers","editor", "booktitle","isbn", "book_group_author", "organization", "series")
* unique id based on title and year

Then create another database that has collapsed all duplicates based on title and year into the most complete record possible.

```{r IS THIS NEEDED  set up folders filepaths containing records}

## use inputs to extract file paths

# combine into one list so that it can be looped through
database_resultsFolders <- vector("list", length(dbFolderNames))
names(database_resultsFolders) <-  dbFolderNames


## get the file names located within each block folder and store in a list

for(d in 1:length(dbFolderNames)){ # for each of the databases searched
  
  # select only the folders with record results (not sql databases)
  resultsFolders <- dir(file.path(resultsPath, dbFolderNames[d]))
  resultsFolders <- resultsFolders[grep("23", resultsFolders)]
  resultsFolders <- resultsFolders[order(resultsFolders)] 
  
  # create a list to store these file names
  database_resultsFolders[[d]] <- vector("list", length(resultsFolders))
  names(database_resultsFolders[[d]]) <- resultsFolders
  
  # get names for the different blocks -- should be the same for all so only do this once
  if(d==1){
    blockNames <- unlist(lapply(strsplit(resultsFolders,"_"), function(x) x[[1]]))
    blockNames <- stringr::str_remove(blockNames, "-block")
  }
  
  
  # for each folder containing results for a search string
  # get a list of the file names
  for(s in 1:length(resultsFolders)){ 
     database_resultsFolders[[d]][[s]] <- dir(file.path(resultsPath, dbFolderNames[d], resultsFolders[s]))
  }
  
}



# check
str(database_resultsFolders)


```


```{r compare the formatting of the wos references and the scopus references to figure out how to merge them}
# import sample df from .bib files
wosFormat <- revtools::read_bibliography(
  file.path(resultsPath, 
            names(database_resultsFolders)[1], 
            names(database_resultsFolders[[1]])[1],
            database_resultsFolders[[1]][[1]][1]))

scopusFormat <- revtools::read_bibliography(
  file.path(resultsPath, 
            names(database_resultsFolders)[2], 
            names(database_resultsFolders[[2]])[1],
            database_resultsFolders[[2]][[1]][1]))

# # check which column names are shared
# colnames(wosFormat)[(colnames(wosFormat) %in% colnames(scopusFormat))]
```

```{r Vector of names of columns I want to include in the standard dataframe when all is combined }

standardColnames <- c(
  "analysis_id", # space to fill in the unique id of the record as it has been downloaded in the map
  "duplicate_id", # space to fill in the "unique id" based on de-duplication protocol
  "source_database", # the name of the database e.g. wos or scopus
  "search_substring", # the string(s) the record was retrieved, 
  "database_article_id", # if the source has a unique id, use (e.g. eid for scopus, and unique_id in wos)
  "type", # the type of publication, e.g. article, proceedings - make all lower case
  "author", # the authors of the publication
  "title", # the title of the publication
  "source_title", # "journal" column, which in scopus is actually source_title, so could be book,etc, in wos, if journal is absent, use booktitle  
  "year",
  "volume",
  "number",
  "pages",
  "abstract",
  "keywords", # merge author keywords and keywords
  "keywords_other", # any additional keywords, e.g. author keywords or keywords_plus
  "publisher",
  "editor",
  "language",
  "affiliation",
  "funding", # funding_acknowledgement in wos, "funding_details in scopus
  "doi",
  "issn",
  "isbn",
  "journal", # wos-only columns ------------
  "research_areas",
  "web_of_science_categories",
  "booktitle",
  "book_group_author",
  "book_author",
  "organization",
  "series",
  "da" # access date
)

```


```{r loop through all the saved .bib files and reformat and save into sql database}




# for each of the databases searched

for(d in 1:length(dbFolderNames)){ 
  
  # track progress
  cat(paste(dbFolderNames[d], "\n"))
  
  # name of the database
  database <- dbFolderNames[d]
  
  # names of all the folders holding results
  resultsFolders <- dir(file.path(resultsPath, dbFolderNames[d])) #names(database_resultsFolders[[d]])
  resultsFolders <- resultsFolders[grepl("23", resultsFolders)]
  resultsFolders <- resultsFolders[order(resultsFolders)]
  
  # get the names for the sub-string blocks
  blockNames <- unlist(lapply(strsplit(resultsFolders,"_"), function(x) x[[1]]))
  blockNames <- stringr::str_remove(blockNames, "-block")
  
  # for each folder containing results for a search string
  for(s in 1:length(resultsFolders)){ 
    
    # track progress
    cat(paste("  ",blockNames[s], d,"/of", length(blockNames), "\n"))
    
    # get all the files in the folder
    files <- dir(file.path(resultsPath, database, resultsFolders[s]))
    
    # for each file in the folder read in the .bib file
    # if there is an error because wrong file type is in folder, skip to next
    for(f in 1:length(files)){
      
      # track progress
      cat(paste("      file", f,"/of", length(files), "\n"))
      
      # # Read in file
      # skip_to_next <- FALSE
      # tryCatch(
      #   tempDf <- revtools::read_bibliography(here::here(resultsPath, database, resultsFolders[s], files[f])),
      #   error = function(e){skip_to_next <<- TRUE}
      # )
      # if(skip_to_next){next}
      tempDf <- revtools::read_bibliography(file.path(resultsPath, database, resultsFolders[s], files[f]))
      
      
      # Format to common formatting
      # determine what function needs to be used to format
      if(grepl("wos", tolower(database))){
        format.fn <- wos2standard
      }else{
        format.fn <- scopus2standard
      }
      # use that function to format the data frame
      tempDf <- format.fn(
        standardColnames = standardColnames, inputDf = tempDf, search_substring = blockNames[s] 
      )
      
      
      ## fill in the analysis_id
      # if the first iteration, start from 1
      if(d==1 & s==1 & f==1){start <- 1}
      end <- start+nrow(tempDf)-1
      tempDf$analysis_id <- seq(start, end)
      # once filled set where the next iteration should start
      start <- end+1
      
      
      # print progress
      if(f==1){
        runningtotal <- nrow(tempDf)
      }else{
        runningtotal <- runningtotal + nrow(tempDf)
      }
      print(paste(nrow(tempDf), "records added /", runningtotal, "total"))
      
      
      # write to the sql database
      # set overwrite commands and appending commands
      if(d==1 & s==1 & f==1 & sqlOverwrite){
        overwritel = TRUE; appendl = FALSE
      }else{
        overwritel = FALSE; appendl = TRUE
      }
      
      # # write to database
      db <- RSQLite::dbConnect(RSQLite::SQLite(), dbname = sqlite_all_ref_file_version)
      dbWriteTable(db, "refs", tempDf, append=appendl, overwrite = overwritel)
      dbDisconnect(db)
      
      
      
    } # end processing one file
    
     
  } # end looping through all files in a block folder
  
} # end looping through all databases




```


# Deduplicate pooled records

```{r searching through the sql database, make a lookup table with id, title and year duplicates}

db <- src_sqlite(sqlite_all_ref_file_version, create=FALSE)
allrefs <- tbl(db, "refs")

# find number of records in total
nrecstotal <- allrefs %>% summarise(n())
print(nrecstotal) # 392111

# number of records by database
nrecsbydb <- allrefs %>% group_by(source_database) %>% summarise(n())
print(nrecsbydb)
#   source_database  `n()`
#   <chr>            <int>
# 1 Scopus          140955
# 2 Web Of Science  251156

# also by substring
allrefs %>% group_by(source_database, search_substring) %>% summarise(n())
#    source_database search_substring  `n()`
#    <chr>           <chr>             <int>
#  1 Scopus          General-options     221
#  2 Scopus          Mitigation        55607
#  3 Scopus          Nature             1442
#  4 Scopus          Renewables        83197
#  5 Scopus          Societal            488
#  6 Web Of Science  General-options   14069
#  7 Web Of Science  Mitigation        47245
#  8 Web Of Science  Nature           128492
#  9 Web Of Science  Renewables        42677
# 10 Web Of Science  Societal          18673




# extract the id, the title and the year for de-duplication
dedupInfo <- allrefs %>%
  select(analysis_id, title, year, doi)
dedupInfo %>% head
dedupInfo <- as.data.frame(dedupInfo)

# # write the file of de-dupe info
# save(dedupInfo, file = file.path(resultsPath, "deduplication-files","columns-for-deduplication.RData"))
```


Next the deduplication is run in rossi using the script analyses/ross_dedup.R


```{r read in the results from the ross deduplication}

## Old column IDs
load(file.path(resultsPath, "deduplication-files","columns-for-deduplication.RData"))
columns_for_deduplication <- dedupInfo
rm(dedupInfo)


## this data contains columns for the unique id of the reference, the title and the year, as well as a duplicate id indicating if there was a duplicate based on title and year
load(here::here(resultsPath,"deduplication-files","title_dedup_ids.RData"))
dedupInfo <- subset(dedupInfo,!is.na(dedupInfo$analysis_id)) 

# because the de-dup was run in parallel by year, need to attach back the year id to make it unique, 
# otherwise there is duplication in ids between years
dedupInfo$title_year_dup_id <- paste(dedupInfo$year, dedupInfo$duplicate_id, sep=".")

# investigate which analysis records are not in the deup dataframe
ind <- which(!(columns_for_deduplication$analysis_id %in% dedupInfo$analysis_id))
#View(columns_for_deduplication[ind,])

# the deduplication code did not run properly when the year was NA, so run on this subset
noYear <- columns_for_deduplication[ind,]
noYear$duplicate_id <- revtools::find_duplicates(noYear,
                            match_variable = "title", 
                            match_function = "stringdist", 
                            method = "osa", threshold = 5,
                            remove_punctuation = TRUE, to_lower = TRUE)
noYear$title_year_dup_id <- paste("NA", noYear$duplicate_id, sep=".")

# join in with rossi dedup results
dedupInfo <- rbind(dedupInfo, noYear)
dim(dedupInfo) # check dims -- correct.
length(unique(dedupInfo$analysis_id)) == nrow(dedupInfo) # check each row has its own unique id

# order by analysis_id
dedupInfo <- dedupInfo %>%
  arrange(analysis_id)
dedupInfo[1:3,]

# the number of unique references
length(dedupInfo$title_year_dup_id)-sum(duplicated(dedupInfo$title_year_dup_id)) # 269481

# rename column to duplicate_id
dedupInfo$duplicate_id <- dedupInfo$title_year_dup_id
dedupInfo$title_year_dup_id <- NULL


# # save as a .sqlite
# db <- RSQLite::dbConnect(RSQLite::SQLite(), 
#                          dbname = here::here(resultsPath, "sql-databases", "title_dedup_ids.sqlite"))
# dbWriteTable(db, "title_dedup_ids", dedupInfo, append=FALSE, overwrite = TRUE)
# dbDisconnect(db)

```


```{r summarise the duplicate information using join}
library(dbplyr)

## connect to the database
allrefs_db <- dbConnect(RSQLite::SQLite(), sqlite_all_ref_file_version, create = FALSE)
src_dbi(allrefs_db)
allrefs <- tbl(allrefs_db, "refs")

allrefs %>% head %>% View

dedup_db <- dbConnect(RSQLite::SQLite(), 
                      here::here(resultsPath, "sql-databases", "title_dedup_ids.sqlite"), 
                      create = FALSE)
src_dbi(dedup_db)
dedups <- tbl(dedup_db, "title_dedup_ids")
# I only need two columns for the join, so collect those
dedups <- dedups %>%
  select(analysis_id, duplicate_id) %>%
  collect()

# a hack to get join to work
copy_to(allrefs_db, dedups)
src_dbi(allrefs_db)
dbDisconnect(dedup_db)



## join the datasets with their duplicate info
# do left join so that all the references are maintained
dedups <- tbl(allrefs_db, "dedups")
allrefs_join <- left_join(allrefs %>% select(-duplicate_id), dedups, by = "analysis_id")



## Summarise

# numbers of duplicates by database source
allrefs_join %>%
  group_by(source_database) %>%
  summarize(n_unique_refs = n_distinct(duplicate_id))
#   source_database n_unique_refs
#   <chr>                   <int>
# 1 Scopus                 130492
# 2 Web Of Science         205420
  

# in total
allrefs_join %>%
  summarise(n_unique_refs = n_distinct(duplicate_id))
#   n_unique_refs
#           <int>
# 1        269481



## Write
require(RSQLite)
allrefs_join <- allrefs_join %>% collect() # collect all the values from the database into local memory

# new_db <- RSQLite::dbConnect(RSQLite::SQLite(),sqlite_all_ref_file_version_dedupid, create=TRUE)
# dbWriteTable(conn=new_db, name=sqlite_allRefDedupID_table_name, value=allrefs_join, append=FALSE, overwrite = TRUE)
# DBI::dbDisconnect(new_db)

# check that the writing works
new_db <- dbConnect(RSQLite::SQLite(), sqlite_all_ref_file_version_dedupid, create = FALSE)
src_dbi(new_db)
allrefs_join <- tbl(new_db, sqlite_allRefDedupID_table_name)

allrefs_join %>% summarise(n=n()) # check all the rows are there
DBI::dbDisconnect(new_db)


## Disconnect from active databases
DBI::dbDisconnect(allrefs_db)
```



Now that the duplicates are identified, deduplicate the database using Rossi


```{r summarise the deduplicated database}
library(dbplyr)

# open the connection
db <- dbConnect(RSQLite::SQLite(),sqlite_unique_ref_file, create=FALSE)
src_dbi(db)
dedupRefs <- tbl(db, sqlite_unique_ref_table_name)

## Summarise

# summarize how many unique references in total -- yes this matches what it should be
dedupRefs %>% summarise(n_unique_refs=n())
#   n_unique_refs
#           <int>
# 1        269481

# number of unique records by database -- picture these results as a venn diagram
dedupRefs %>% group_by(source_database) %>% summarise(n=n())
#   source_database              n
#   <chr>                    <int>
# 1 Scopus                   64061
# 2 Web Of Science          138989
# 3 Web Of Science | Scopus  66431


# number of abstracts missing
dedupRefs %>% summarise(num_na = sum(is.na(abstract)))
#   num_na
#    <int>
# 1   4045




# number of unique records by search string
dedupRefs %>% group_by(search_substring) %>% summarise(n=n()) 

search_string_n <- dedupRefs %>% 
  summarise(
    General = sum(grepl("General", search_substring)),
    Mitigation = sum(sum(grepl("Mitigation", search_substring)), sum(grepl("Renewable", search_substring))),
    Natural = sum(grepl("Natur", search_substring)),
    Societal = sum(grepl("Societal", search_substring))
  ) 
search_string_n <- reshape2::melt(search_string_n, variable.name = "Search_string", value.name = "n_records")
#   Search_string n_records
# 1       General     14144
# 2    Mitigation    156995
# 3       Natural    129083
# 4      Societal     18867


require(ggplot2)

ggplot(search_string_n, aes(Search_string, n_records)) +
  geom_col()+
  theme_classic()



## Write to a tab delimited .txt file for Vicky to process?
dedupRefs <- dedupRefs %>% collect()
dim(dedupRefs)

# write.table(dedupRefs,
#             file = file.path(resultsPath, "deduplication-files","unique_references.txt"),
#             row.names=F, col.names=TRUE, sep='\t', quote=FALSE)


# close connection
dbDisconnect(db)
```




# Check against other benchmark articles

```{r}
#install.packages("citationchaser")
```

But the API is only a 14 day free trial, so use the shiny app: https://estech.shinyapps.io/citationchaser/


## Galparsoro 2022 -- reviewing ecological impacts of offshore wind farms

DOI searched: https://doi.org/10.1038/s44183-022-00003-5

```{r}
library(dbplyr)


## References retrieved from backwards citation chasing

# read in
galRef <- revtools::read_bibliography(here::here("data","raw-data","citation-chasing","Galparsoro_etal_2002_references.ris"))

# only search by doi, so remove if doi is na
galRef <- subset(galRef, !is.na(doi))

# load database with all references in our database (not deduplicated)
dbcon <- DBI::dbConnect(RSQLite::SQLite(),
                        here::here("data","raw-data","sql-databases","all-refs_v2.sqlite"),
                        create=FALSE)
dbplyr::src_dbi(dbcon)
allrefs <- tbl(dbcon, "refs")

# join the two dataframes to find matches
allrefs <- allrefs %>%
  select(doi, analysis_id, duplicate_id)

galRefRetreived <- galRef %>%
  left_join(allrefs, by = "doi", copy = TRUE)

# note there are duplicates because some dois match with multiple entries, so remove
galRefRetreived <- revtools::extract_unique_references(galRefRetreived, matches = "doi")



## Were the missed references indexed in WOS?

# string to search in WOS
cat(paste0(galRefRetreived$doi[which(is.na(galRefRetreived$analysis_id))], sep=" OR "))
# only 26 indexed in WOS

# read in wos results
indexWos <- revtools::read_bibliography(here::here("data","raw-data","citation-chasing","missed-Galparsoro-indexed-in-wos-zotero.ris"))

# join with the retreived dataset
galRefRetreived <- galRefRetreived %>%
  left_join(indexWos %>% select(doi,accession), by = "doi")%>%
  rename(wos_id = accession)

## None were indexed in Scopus



## Summaries

# how many are retreived
sum(!is.na(galRefRetreived$analysis_id))

# out of how many are cited
nrow(galRef)

# how many were missed that were indexed?
galRefRetreived %>%
  filter(is.na(analysis_id), !is.na(wos_id)) %>%
  nrow

## Export for Fred
writexl::write_xlsx(galRefRetreived %>% select(-duplicate_id, -n_duplicates), 
                    path = here::here("data","raw-data","citation-chasing","Galparsoro_etal_2002_retrieved-references.xlsx"))

``` 




