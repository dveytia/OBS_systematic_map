---
title: "04-organizing_all_search_results"
author: "Devi Veytia"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r load libraries}
library(dplyr)
library(R.utils)
library(RSQLite)
library(revtools)

```

```{r source functions}
source(here::here("R","cleaning-and-formatting-bib-dataframes.R"))
```

```{r set up inputs}
##  set up inputs

# Do you want to overwrite the existing sql databases?
sqlOverwrite = TRUE


## Directory where the folders containing the results for the different blocks 
resultsPath <- here::here("data","raw-data") # where the database folders are located
dbFolderNames <- c("WOS_downloads", "Scopus_downloads") # names of the database folders


## Sql file for the sampling of the records for screening
sqlite_file <- file.path(resultsPath, dbFolderNames[1],"sql-databases","sample-wos-refs.sqlite")
table_name <- "wosrefs"


## SQL files for de-duplication
sqlite_all_ref_file <- file.path(resultsPath,"sql-databases","all-refs.sqlite")
sqlite_all_ref_table_name <- "allrefs" <- "allrefs"

sqlite_unique_ref_file <- file.path(resultsPath,"sql-databases","unique-refs.sqlite")
sqlite_unique_ref_table_name <- "uniquerefs"



## number of articles of include in double blibd screening
nDoubBlind <- 300

```


The idea is to download all the search records from WOS and Scopus, de-duplicate, then merge with the screening results so that we don't screen the same articles twice. 
- screen subsets (yes, sort by relevance in colandr is better)
- download decisions that have already been made in pilot testing
- de-duplicate search results (doi and title)
- remove records that have already been screened(?,yes) (maybe by merging based on doi?)
- format and export
- 



Note for Colandr:
* can upload .ris, .bib, .txt (need to have the information that an .ris or .bib file would have). Might be the best option because then studies will have unique IDs? 
* imported citaions cannot be deleted
* max single import size is 40MB (15-20k of citations)


Note for sysrev:
* import in .ris
* max file size is 7MB - best to upload ~2000 refs at a time

When exporting search results from WOS, do it as a .bib file, then de-duplicate using revtools. Note I get an error when trying to download the complete view, can only do standard


# Subsetting results for screening

It would take too long to de-duplicate all references, so what I will do instead is randomly select .bib files from each search string block, write to an sql database to sample from.

From this sampled database, de-duplicate based on title and year
* load in all .bib files into R: only columns for doi and title
* create unique ID based on fuzzy title and year (Callaghan et al 2021)
* grouping by unique ID, combine data from different rows to create a single row containing all the available data see: https://stackoverflow.com/questions/44206714/remove-least-complete-duplicate-rows-in-r-or-sql

Then divvy them up into screening sets:
* 300 refs for double blind screening
* 4000 for the first set of individual screening -- more might be needed depending on the proportion of relevant to irrelevant.




```{r load in randomly selected bib files from each block and write to an sql database to sample from}
## Set up inputs

# max number of record files to read per block
# if number of files > maxFileNo, then maxFileNo will be sampled from available files
maxFileNo <- 10


# Colnames to extract
colnames2extract <- c("type","author","title","journal","year","volume","number","pages","abstract","publisher","address","affiliation","doi","issn","eissn","keywords","keywords_plus","research_areas","web_of_science_categories","author_email","affiliations","orcid_numbers", "web_of_science_index", "unique_id", "funding_acknowledgement", "funding_text", "article_number", "researcherid_numbers","editor", "booktitle","isbn", "book_group_author", "organization", "series")



## use inputs to extract file paths
resultsFolders <- dir(file.path(resultsPath, dbFolderNames[1]))
resultsFolders <- resultsFolders[grep("23", resultsFolders)]
resultsFolders <- resultsFolders[order(resultsFolders)]

# get names for the different blocks
blockNames <- unlist(lapply(str_split(resultsFolders,"_"), function(x) x[[1]]))



## loop through all the files to write to an sql database
start <- Sys.time()

for(i in 1:length(resultsFolders)){
  
  cat(paste(blockNames[i], i,"/of", length(blockNames), "\n"))
  
  blockFiles <- dir(file.path(resultsPath, dbFolderNames[1], resultsFolders[i]))
  
  if(length(blockFiles) > maxFileNo){
    blockFiles <- blockFiles[sample(1:length(blockFiles),size = maxFileNo, replace = FALSE)]
  }
  
  # for each of the wos record files in the directory
  for(f in 1:length(blockFiles)){
    
    cat(paste("   file", f, "of", length(blockFiles),"\n"))
    
    # read in bibtex file -- if there is an error skip to the next
    skip_to_next <- FALSE
    tryCatch(
      tempDf <- revtools::read_bibliography(here::here(resultsPath,dbFolderNames[1], resultsFolders[i], blockFiles[f])),
      error = function(e){skip_to_next <<- TRUE}
    )
    if(skip_to_next){next}
    
    # Ensure order and naming of columns is consistent
    # create empty columns if standard columns are missing
    if(sum(!(colnames2extract %in% colnames(tempDf))) > 0){
      tempDf[,colnames2extract[which(!(colnames2extract %in% colnames(tempDf)))]] <- NA
    }
    # subset to only specific columns 
    tempDf <- tempDf[,colnames2extract]
    
    # add a column ID to indicate which block it came from
    tempDf$source_string <- paste(blockNames[i])
    
    # set overwrite commands and appending commands
    if(i==1 & f==1 & sqlOverwrite){
      overwritel = TRUE; appendl = FALSE
    }else{
      overwritel = FALSE; appendl = TRUE
    }
    
    # write to database
    db <- RSQLite::dbConnect(RSQLite::SQLite(), dbname = sqlite_file)
    dbWriteTable(db, "refs", tempDf, append=appendl, overwrite = overwritel)
    dbDisconnect(db)
    
    
  } # end processing the .bib file
  
} # end looping through the block folder

end <- Sys.time()

start-end
```


```{r sample references for each substring and de-duplicate}
set.seed(123)

wos_id_db <- src_sqlite(sqlite_file, create=FALSE)
wos_ids <- tbl(wos_id_db, "refs")

# select 1500 references from each block to give 7500 to work with
wos_id_results <- wos_ids %>%
  filter(!is.na(doi), !is.na(title), !is.na(abstract)) %>%
  #select(unique_id, source_string) %>%
  group_by(source_string)%>%
  slice_sample(n=1500)

wos_id_results <- as.data.frame(wos_id_results)
dim(wos_id_results)

wos_id_results$title_id <- revtools::find_duplicates(wos_id_results, match_variable = "title",
                                             match_function = "stringdist", method = "osa", threshold = 5,
                                             remove_punctuation = TRUE, to_lower = TRUE)

# remove title and year duplicates
# for each unique combination of title and year, 
# combine data from different rows to create a single row containing all the available data
# note if there are conflicting data for a column with the same ID slice will take the first value
wos_id_results2 <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep =".")) %>%
  group_by(title_year_id) %>%
  slice(which.min(rowSums(is.na(.)))) %>%
  as.data.frame()

nrow(wos_id_results2)


wos_id_results <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep ="."))

dupedIds <- wos_id_results$title_year_id[duplicated(wos_id_results$title_year_id)]
for(i in 1:length(dupedIds)){
  tempDf <- wos_id_results %>%
   filter(title_year_id %in% dupedIds[i]) %>% 
   slice(which.min(rowSums(is.na(.)))) %>%
   as.data.frame()
  if(i==1){
    dedupedResults <- tempDf
  }else{
    dedupedResults <- rbind(dedupedResults, tempDf)
  }
}

# join together into one dataframe with unique references
nondupedResults <- wos_id_results %>%
  filter(!(title_year_id %in% dupedIds)) %>%
  as.data.frame()
unique_wos_results <- rbind(dedupedResults, nondupedResults)
rm(dedupedResults, nondupedResults)

# number of duplicates removed -- 121
nrow(wos_id_results)-nrow(unique_wos_results)

# add a column indicating that it is a random sample
unique_wos_results$random_sample <- paste("yes")

# ensure that title and abstract are not na
unique_wos_results <- unique_wos_results %>%
  filter(!is.na(title), !is.na(abstract))

# # write to a new file
# saveRDS(unique_wos_results,
#         here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sample.rds"))

```

Depending on how the records need to be formatted, save as appropriate file type

```{r using the dedeuplicated sample of references create a subsample of articles to upload for screening}
set.seed(123)

# load in the random sample of deduplicated wos results
unique_wos_results <- readRDS(
  here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sample.rds")
)

# from this, divide into sets of articles to screen with proportions based on block
# because renewable options compose 1/3 of the mitigation typology, the 1000 references for mitigation options is distributed accordingly-
# 1/3 are retrieved from the renewable block, and 2/3 from the mitigation block
sampleInstructions <- data.frame(
  source_string = unique(unique_wos_results$source_string),
  n = c(1000, round(1000*2/3), 1000, round(1000*1/3), 1000)
)

# using the sampling instructions create a dataframe with a random sample
for(i in 1:nrow(sampleInstructions)){
  
  tempDf <- unique_wos_results %>%
    filter(source_string == sampleInstructions$source_string[i]) %>%
    sample_n(size = sampleInstructions$n[i])
  
  if(i==1){
    sampleDf <- tempDf
  }else{
    sampleDf <- rbind(sampleDf, tempDf)
  }
  
}


# seperate out into double blind and regular sets 
indDoubBlind <- sample(1:nrow(sampleDf), nDoubBlind, replace=FALSE)
sampleDf$Double_Blind <- "no"
sampleDf$Double_Blind[indDoubBlind] <- "yes"

sampleDf_singleScreen <- sampleDf %>% filter(Double_Blind == "no")

# ## Write files
# # save sampled Df
# saveRDS(sampleDf,
#         here::here("data","derived-data", "screening","records-to-screen","de-duplicated_wos_sub-sample1_Df.rds"))


# # format files for upload into screening platform and write
# # potential to group by source to better help with the relevance ranking algorithm -- but decided not to
# 
# revtools::write_bibliography(x = sampleDf_singleScreen[1:round(nrow(sampleDf_singleScreen)/2),],
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample_1.1.ris"),
#                             format="ris")
# 
# revtools::write_bibliography(x = sampleDf_singleScreen[(round(nrow(sampleDf_singleScreen)/2)+1):nrow(sampleDf_singleScreen),],
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample_1.2.ris"),
#                             format="ris")
# 
# revtools::write_bibliography(x = sampleDf %>% filter(Double_Blind == "yes"),
#                              filename = here::here("data","derived-data", "screening","records-to-screen","wos_sub-sample1_double-blind.ris"),
#                             format="ris")

```


```{r add in test list items to random screening subsample}

# load in test list articles
test_list <- readxl::read_excel(here::here("data", "derived-data", "screen_test_list",
                    "test-list_2023-01-19.xlsx"))



```



# Reformat all the WOS and Scopus records into standard formatting

Things to include in the dataframe:
* Which database it came from
* database unique identifier
* Other metadata: colnames2extract <- c("type","author","title","journal","year","volume","number","pages","abstract","publisher","address","affiliation","doi","issn","eissn","keywords","keywords_plus","research_areas","web_of_science_categories","author_email","affiliations","orcid_numbers", "web_of_science_index", "unique_id", "funding_acknowledgement", "funding_text", "article_number", "researcherid_numbers","editor", "booktitle","isbn", "book_group_author", "organization", "series")
* unique id based on title and year

Then create another database that has collapsed all duplicates based on title and year into the most complete record possible.

```{r IS THIS NEEDED  set up folders filepaths containing records}

## use inputs to extract file paths

# combine into one list so that it can be looped through
database_resultsFolders <- vector("list", length(dbFolderNames))
names(database_resultsFolders) <-  dbFolderNames


## get the file names located within each block folder and store in a list

for(d in 1:length(dbFolderNames)){ # for each of the databases searched
  
  # select only the folders with record results (not sql databases)
  resultsFolders <- dir(file.path(resultsPath, dbFolderNames[d]))
  resultsFolders <- resultsFolders[grep("23", resultsFolders)]
  resultsFolders <- resultsFolders[order(resultsFolders)] 
  
  # create a list to store these file names
  database_resultsFolders[[d]] <- vector("list", length(resultsFolders))
  names(database_resultsFolders[[d]]) <- resultsFolders
  
  # get names for the different blocks -- should be the same for all so only do this once
  if(d==1){
    blockNames <- unlist(lapply(strsplit(resultsFolders,"_"), function(x) x[[1]]))
    blockNames <- stringr::str_remove(blockNames, "-block")
  }
  
  
  # for each folder containing results for a search string
  # get a list of the file names
  for(s in 1:length(resultsFolders)){ 
     database_resultsFolders[[d]][[s]] <- dir(file.path(resultsPath, dbFolderNames[d], resultsFolders[s]))
  }
  
}



# check
str(database_resultsFolders)


```


```{r compare the formatting of the wos references and the scopus references to figure out how to merge them}
# import sample df from .bib files
wosFormat <- revtools::read_bibliography(
  file.path(resultsPath, 
            names(database_resultsFolders)[1], 
            names(database_resultsFolders[[1]])[1],
            database_resultsFolders[[1]][[1]][1]))

scopusFormat <- revtools::read_bibliography(
  file.path(resultsPath, 
            names(database_resultsFolders)[2], 
            names(database_resultsFolders[[2]])[1],
            database_resultsFolders[[2]][[1]][1]))

# # check which column names are shared
# colnames(wosFormat)[(colnames(wosFormat) %in% colnames(scopusFormat))]
```

```{r Vector of names of columns I want to include in the standard dataframe when all is combined }

standardColnames <- c(
  "analysis_id", # space to fill in the unique id of the record as it has been downloaded in the map
  "duplicate_id", # space to fill in the "unique id" based on de-duplication protocol
  "source_database", # the name of the database e.g. wos or scopus
  "search_substring", # the string(s) the record was retrieved, 
  "database_article_id", # if the source has a unique id, use (e.g. eid for scopus, and unique_id in wos)
  "type", # the type of publication, e.g. article, proceedings - make all lower case
  "author", # the authors of the publication
  "title", # the title of the publication
  "source_title", # "journal" column, which in scopus is actually source_title, so could be book,etc, in wos, if journal is absent, use booktitle  
  "year",
  "volume",
  "number",
  "pages",
  "abstract",
  "keywords", # merge author keywords and keywords
  "keywords_other", # any additional keywords, e.g. author keywords or keywords_plus
  "publisher",
  "editor",
  "language",
  "affiliation",
  "funding", # funding_acknowledgement in wos, "funding_details in scopus
  "doi",
  "issn",
  "isbn",
  "journal", # wos-only columns ------------
  "research_areas",
  "web_of_science_categories",
  "booktitle",
  "book_group_author",
  "book_author",
  "organization",
  "series",
  "da" # access date
)

```


```{r loop through all the saved .bib files and reformat and save into sql database}

# # version of the database file
v = "v2"
sqlite_all_ref_file_version <- paste0(gsub(".sqlite","",sqlite_all_ref_file),"_",v,".sqlite")


# for each of the databases searched

for(d in 1:length(dbFolderNames)){ 
  
  # track progress
  cat(paste(dbFolderNames[d], "\n"))
  
  # name of the database
  database <- dbFolderNames[d]
  
  # names of all the folders holding results
  resultsFolders <- dir(file.path(resultsPath, dbFolderNames[d])) #names(database_resultsFolders[[d]])
  resultsFolders <- resultsFolders[grepl("23", resultsFolders)]
  resultsFolders <- resultsFolders[order(resultsFolders)]
  
  # get the names for the sub-string blocks
  blockNames <- unlist(lapply(strsplit(resultsFolders,"_"), function(x) x[[1]]))
  blockNames <- stringr::str_remove(blockNames, "-block")
  
  # for each folder containing results for a search string
  for(s in 1:length(resultsFolders)){ 
    
    # track progress
    cat(paste("  ",blockNames[s], d,"/of", length(blockNames), "\n"))
    
    # get all the files in the folder
    files <- dir(file.path(resultsPath, database, resultsFolders[s]))
    
    # for each file in the folder read in the .bib file
    # if there is an error because wrong file type is in folder, skip to next
    for(f in 1:length(files)){
      
      # track progress
      cat(paste("      file", f,"/of", length(files), "\n"))
      
      # # Read in file
      # skip_to_next <- FALSE
      # tryCatch(
      #   tempDf <- revtools::read_bibliography(here::here(resultsPath, database, resultsFolders[s], files[f])),
      #   error = function(e){skip_to_next <<- TRUE}
      # )
      # if(skip_to_next){next}
      tempDf <- revtools::read_bibliography(file.path(resultsPath, database, resultsFolders[s], files[f]))
      
      
      # Format to common formatting
      # determine what function needs to be used to format
      if(grepl("wos", tolower(database))){
        format.fn <- wos2standard
      }else{
        format.fn <- scopus2standard
      }
      # use that function to format the data frame
      tempDf <- format.fn(
        standardColnames = standardColnames, inputDf = tempDf, search_substring = blockNames[s] 
      )
      
      
      ## fill in the analysis_id
      # if the first iteration, start from 1
      if(d==1 & s==1 & f==1){start <- 1}
      end <- start+nrow(tempDf)-1
      tempDf$analysis_id <- seq(start, end)
      # once filled set where the next iteration should start
      start <- end+1
      
      
      # print progress
      if(f==1){
        runningtotal <- nrow(tempDf)
      }else{
        runningtotal <- runningtotal + nrow(tempDf)
      }
      print(paste(nrow(tempDf), "records added /", runningtotal, "total"))
      
      
      # write to the sql database
      # set overwrite commands and appending commands
      if(d==1 & s==1 & f==1 & sqlOverwrite){
        overwritel = TRUE; appendl = FALSE
      }else{
        overwritel = FALSE; appendl = TRUE
      }
      
      # # write to database
      db <- RSQLite::dbConnect(RSQLite::SQLite(), dbname = sqlite_all_ref_file_version)
      dbWriteTable(db, "refs", tempDf, append=appendl, overwrite = overwritel)
      dbDisconnect(db)
      
      
      
    } # end processing one file
    
     
  } # end looping through all files in a block folder
  
} # end looping through all databases




```



```{r searching through the sql database, make a lookup table with id, title and year duplicates}
# make a unique id which is the number of the record in the review
# also make a 


db <- src_sqlite(sqlite_all_ref_file_version, create=FALSE)
allrefs <- tbl(db, "refs")

# find number of records in total
nrecstotal <- allrefs %>% summarise(n())
print(nrecstotal) # 392111

# number of records by database
nrecsbydb <- allrefs %>% group_by(source_database) %>% summarise(n())
print(nrecsbydb)
#   source_database  `n()`
#   <chr>            <int>
# 1 Scopus          140955
# 2 Web Of Science  251156

# also by substring
allrefs %>% group_by(source_database, search_substring) %>% summarise(n())
#    source_database search_substring  `n()`
#    <chr>           <chr>             <int>
#  1 Scopus          General-options     221
#  2 Scopus          Mitigation        55607
#  3 Scopus          Nature             1442
#  4 Scopus          Renewables        83197
#  5 Scopus          Societal            488
#  6 Web Of Science  General-options   14069
#  7 Web Of Science  Mitigation        47245
#  8 Web Of Science  Nature           128492
#  9 Web Of Science  Renewables        42677
# 10 Web Of Science  Societal          18673




# extract the id, the title and the year for de-duplication
dedupInfo <- allrefs %>%
  select(analysis_id, title, year, doi)
dedupInfo %>% head
dedupInfo <- as.data.frame(dedupInfo)

# write the file of de-dupe info
save(dedupInfo, file = file.path(resultsPath, "deduplication-files","columns-for-deduplication.RData"))
```

```{r}

# read in file
dedupInfo2 <- read.table(file = file.path(resultsPath, "deduplication-files","columns-for-deduplication.txt"),
                        header = TRUE, sep = '\t')

# run code for deduplication
unique_title_id <- revtools::find_duplicates(dedupInfo,
    match_variable = "title", 
    group_variables = c("year"),
    match_function = "stringdist", method = "osa", threshold = 5,
    remove_punctuation = TRUE, to_lower = TRUE
  )

dedupInfo$duplicate_id <- unique_title_id

saveRDS(dedupInfo, here::here("data","derived-data","title_dedup_ids.rds"))





# ------


# remove title and year duplicates
# for each unique combination of title and year, 
# combine data from different rows to create a single row containing all the available data
# note if there are conflicting data for a column with the same ID slice will take the first value
wos_id_results2 <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep =".")) %>%
  group_by(title_year_id) %>%
  slice(which.min(rowSums(is.na(.)))) %>%
  as.data.frame()

nrow(wos_id_results2)


wos_id_results <- wos_id_results %>%
  mutate(title_year_id = paste(title_id, year, sep ="."))

dupedIds <- wos_id_results$title_year_id[duplicated(wos_id_results$title_year_id)]
for(i in 1:length(dupedIds)){
  tempDf <- wos_id_results %>%
   filter(title_year_id %in% dupedIds[i]) %>% 
   slice(which.min(rowSums(is.na(.)))) %>%
   as.data.frame()
  if(i==1){
    dedupedResults <- tempDf
  }else{
    dedupedResults <- rbind(dedupedResults, tempDf)
  }
}


```


# untested code ----


```{r deduplicate the sql database}

db <- src_sqlite(sqlite_file, create=FALSE)
tempDf <- tbl(db, "refs")
recs <- tempDf %>%
  mutate(unique_ID)


```

```{r de duplicate code}
start <- Sys.time()
tempDf$title_id <- revtools::find_duplicates(tempDf, match_variable = "title",
                                             match_function = "stringdist", method = "osa", threshold = 5,
                                             remove_punctuation = TRUE, to_lower = TRUE)
end <- Sys.time()

diff <- end-start
diff*300*1000/60/60/24 # the amount of time it would take in days to run for 300k ref

# if the title and year are a match, combine for the most complete reference

``` 



# PRISMA SUMMARY

check this out: PRISMA2020 R Package https://www.youtube.com/watch?v=IySomccfnFE&list=PLt4D0fZLWzSlVzzhZTeyOCJDyMoDHNEDK&index=18


# Junk code
```{r using rscopus}
## Set API key

Sys.setenv(Elsevier_API = "f4e391f48f6cf6a2e335a3d56d1afc86")
# # or
# rscopus::set_api_key("5dccf61efd84c23b9fe877da0d73d5e5")

## keys:
# linked to hotmail acct: f4e391f48f6cf6a2e335a3d56d1afc86
# linked to fondationbiodiversite acct: 5dccf61efd84c23b9fe877da0d73d5e5


# # prove that I have the API key set
# print(rscopus::get_api_key(), reveal=TRUE)
rscopus::have_api_key()

## Example

# create query
Query <- paste0("AF-ID(60006514) AND SUBJAREA(", 
                rscopus::subject_areas(), 
                ") AND PUBYEAR = 2018 AND ACCESSTYPE(OA)")


# run query


if (rscopus::have_api_key()) {
  
  # define function if the api key is present, build query based on subject area
  make_query = function(subj_area) {
    paste0("AF-ID(60006514) AND SUBJAREA(", 
           subj_area,
           ") AND PUBYEAR = 2018 AND ACCESSTYPE(OA)")
  }
  
  # which subject area to subset the search to
  i = 3
  subj_area = rscopus::subject_areas()[i]
  print(subj_area)
  
  # search for query
  completeArticle <- rscopus::scopus_search(
    query = make_query(subj_area), 
    view = "COMPLETE", 
    count = 25)
  print(names(completeArticle))
  total_results = completeArticle$total_results
  total_results = as.numeric(total_results)
} else {
  total_results = 0
}

df <- gen_entries_to_df(completeArticle$entries)

```


```{r subset results from WOS for screening not using sql}
## use inputs to extract folder paths for where the wos results are stored
resultsFolders <- dir(wos_resultsDir)[grep("23", dir(resultsDir))] # results extracted in '23

# get names for the different blocks
blockNames <- unlist(lapply(str_split(resultsFolders,"_"), function(x) x[[1]]))

# create list to store results into different sets
setsList <- vector("list", length(blockNames)+1)

for(i in 1:length(resultsFolders)){
  
  # select one of the block directories and extract all the file names within
  blockFiles <- dir(file.path(wos_resultsDir, resultsFolders[i]))
  
  # # order files based on dowload order
  # ord <- stringr::str_extract_all(blockFiles,"\\d", simplify = TRUE)
  # ord <- as.numeric(paste0(ord[,1], ord[,2]))
  # ord[which(is.na(ord))] <- 0
  # blockFiles <- blockFiles[order(ord)]
  # 
  # # select files in three slices: first, middle and last
  # # since I sorted based on relevance, this should give a good representation of varying levels of relevance
  # blockFiles <- blockFiles[c(1,round(length(blockFiles)/2), length(blockFiles))]
  
  
  
  # for each of the slices, read out the bibliographic file
  for(f in 1:length(blockFiles)){
    
    tempDf <- revtools::read_bibliography(
      here::here(resultsDir, resultsFolders[i], blockFiles[f])
    ) 
    
    tempDf$source_string <- paste()
    
    # make into a big dataframe
    if(i==1 & f==1){
      
    }
    
    
  } # end processing the .bib file
  
}

```




