---
title: "03-review_team_screening_training"
author: "Devi Veytia"
date: "2023-01-24"
output: html_document
---

```{r load libraries}
library(dplyr)
library(ggplot2)
```


This file organizes the results and decisions for the review team training for screening


# Organise search results into training sets for export


In order to start the team training in a timely manner, I download a sample of the search results from WOS to divide into training sets. To aim to get a distribution of relevant and irrelevant articles, the search string was run in WOS, and then sorted by relevance. The first 3000 and last 3000 records were downloaded. From these 6000 records, I was able to randomly sample the training sets for pilot testing, as well as the sample for training and testing the model (3000 records total). Query link: https://www.webofscience.com/wos/woscc/summary/c04c0155-d6fa-4cf5-9b1d-e00408b0bb81-6b7bdddc/relevance/1


```{r read in sample of search records from WOS and deduplicate}

## read in the sample of the search records downloaded from WOS
recs <- litsearchr::import_results(
  directory = here::here("data","raw-data","sample_WOS_download_2023-01-20")
)



## De-duplicate

# based on doi -- 0 duplicates
unique_id <- revtools::find_duplicates(recs, to_lower = TRUE)
sum(duplicated(unique_id))


# assign the unique ids to the records and filter for only unique entries
recs$unique_id <- unique_id

# based on title
unique_id_title <- revtools::find_duplicates(recs, to_lower = TRUE, match_variable = "title",
                                            match_function = "stringdist", method = "osa",remove_punctuation = TRUE)
recs$unique_id_title <- unique_id_title

# how many duplicates
sum(duplicated(unique_id_title))

# view title duplicates
View(recs[which(unique_id_title %in% unique_id_title[duplicated(unique_id_title)]),c("unique_id","unique_id_title","author","title","year","doi")])

# remove duplicates
ids2remove <- c(105, 762,1113, 1015, 1015, 1392, 2576, 3567)
recs <- subset(recs, !(unique_id %in% ids2remove))



## Examine dataset
summary(recs)



## save the records
# saveRDS(recs, here::here("data", 
#                                        "derived_data", 
#                                        "review_team_train_screen",
#                                        "sample_WOS_download_2023-01-20_deduplicated.rds"))


# revtools::write_bibliography(recs, 
#                              here::here("data", 
#                                        "derived_data", 
#                                        "review_team_train_screen",
#                                        "sample_WOS_download_2023-01-20_deduplicated.bib"))

```

```{r randomly select 4 training sets}

## Randomly select articles for training sets
set.seed(123)
nSets <- 4
setSize <- 30
nRecs <- nSets*setSize



## Allocate references randomly into training sets and save .bib files and .txt files for Colandr and Abstrackr respectively

# read in de-duplicated references
recsSub <- readRDS(here::here("data","derived_data","review_team_train_screen","sample_WOS_download_2023-01-20_deduplicated.rds"))

# remove articles that don't have titles or abstracts
recsSub <- subset(recsSub, !is.na(abstract))
recsSub <- subset(recsSub, !is.na(title))

# pull out the articles I need randomly
recsSub <- recsSub[sample(nrow(recsSub), nRecs, replace=FALSE),]

# Explore data
recsSub %>%
  select(author, title, year, abstract, doi, keywords) %>%
  View

# Set a column to ID what set it belongs to
recsSub$Set <- rep(1:nSets, setSize)




# ## Write each set to a seperate file
# for(i in 1:nSets){
#   temp <- subset(recsSub, Set == i)
#   temp$Set <- NA
#   
#   # write files
#   tempFileName <- file.path(scopeSearchDir, 
#                                        "derived_data", 
#                                        "review_team_train_screen",
#                                        paste0("pilot_screen_",i))
#   revtools::write_bibliography(temp, 
#                              paste0(tempFileName,".ris"))
#   
#   temp <- temp %>%
#     select(unique_id, title, abstract, keywords) %>%
#     rename(id = unique_id)
#   
#   write.table(temp,
#              file = paste0(tempFileName,".txt"),
#              row.names=F, col.names=TRUE, sep='\t', quote=FALSE)
# }

```


# Set 1: Inter-reviewer agreement

Using this function: https://search.r-project.org/CRAN/refmans/irr/html/kappam.fleiss.html

```{r names of screeners}
screeners <- c("dveytia", "YunneShin", "bopp", "gattuso", "adriencomte")
idColumns <- c("title", "abstract", "consensus")
```

```{r read in csv of set 1 screening results}

## Read in and format
set1 <- read.csv(here::here("data","derived-data","review-team-train-screen", "set1_screening_results.csv"),
                 row.names = 2, strip.white = TRUE)
# truncate number of columns
set1 <- set1[,c(idColumns, screeners)]
# calculate the proportion of agreement -- values of 0 and 1 indicate agreement, and in between are levels of disagreement with 0.5 being the highest
set1$propPos <- apply(set1[,screeners], 1, function(x) sum(x>0))/5



## Calculate the kappa statistic
set1KappaTest <- irr::kappam.fleiss(set1[,c(screeners)])
print(set1KappaTest)
# Fleiss' Kappa for m Raters
# 
#  Subjects = 30 
#    Raters = 5 
#     Kappa = 0.573 
# 
#         z = 9.92 
#   p-value = 0 


## Visualize

set1_long <- reshape2::melt(set1, 
                            measure.vars = c(screeners),
                            variable.name = "screener",
                            value.name = "screen_decision")



agreement <- expand.grid(set1$title, screeners)
colnames(agreement) <- c("title","screener")
agreement <- merge(agreement, set1[,c("title","propPos")], by=c("title"))


ggp1 <- ggplot(set1_long, aes(screener, stringr::str_trunc(title, 30, "right"), 
                      fill=factor(screen_decision, levels = c(1,-1), labels = c("incl","excl"))))+
  geom_tile()+
  scale_fill_manual(values = c("chartreuse4","darksalmon"))+
  labs(y = "Title", fill = "Decision")+
  labs(caption = paste("Fleiss' Kappa statistic = ", signif(set1KappaTest$value, 2)))+
  # geom_tile(data=agreement, aes(
  #   x=screener, y=stringr::str_trunc(title, 30, "right"),colour = propPos), 
  #   fill="transparent", inherit.aes = FALSE)+
  # scale_colour_gradientn(colours = c("transparent","red","transparent"))+
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    legend.position = "bottom"
  )


ggp2 <- ggplot(set1, aes("agreement", stringr::str_trunc(title, 30, "right"),
                 fill = propPos))+
  geom_tile()+
  scale_fill_gradientn(colours = c("transparent","red","transparent"), name="Agreement")+
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.background = element_rect(fill = "white"),
    axis.ticks = element_blank()
  )


# pdf(here::here("figures","screening_training_sets","screening_set1.pdf"),
#     width = 7, height = 5)
# egg::ggarrange(ggp1, ggp2, nrow=1, ncol=2, widths = c(3,1))
# 
# dev.off()
```






# Set 2: Inter-reviewer agreement

Using this function: https://search.r-project.org/CRAN/refmans/irr/html/kappam.fleiss.html

```{r names of screeners}
screeners <- c("dveytia", "YunneShin", "bopp", "gattuso", "adriencomte")
idColumns <- c("title", "abstract", "consensus")
```

```{r read in csv of set 2 screening results}

## Read in and format
set2 <- read.csv(here::here("data","derived-data","review-team-train-screen", "set2_screening_results.csv"),
                 row.names = 2, strip.white = TRUE)
# truncate number of columns
set2 <- set2[,c(idColumns, screeners)]
# calculate the proportion of agreement -- values of 0 and 1 indicate agreement, and in between are levels of disagreement with 0.5 being the highest
set2$propPos <- apply(set2[,screeners], 1, function(x) sum(x>0))/5



## Calculate the kappa statistic
set2KappaTest <- irr::kappam.fleiss(set2[,c(screeners)])
set2KappaTest <- irr::kappam.fleiss(set2[,c("dveytia", "YunneShin", "bopp", "adriencomte")])
print(set2KappaTest)

 # Fleiss' Kappa for m Raters
 # 
 # Subjects = 30 
 #   Raters = 5 
 #    Kappa = 0.773 
 # 
 #        z = 13.4 
 #  p-value = 0 


## The number of includes
sum(set2$consensus == 1)


## Visualize

set2_long <- reshape2::melt(set2, 
                            measure.vars = c(screeners),
                            variable.name = "screener",
                            value.name = "screen_decision")



agreement <- expand.grid(set2$title, screeners)
colnames(agreement) <- c("title","screener")
agreement <- merge(agreement, set2[,c("title","propPos")], by=c("title"))


ggp1 <- ggplot(set2_long, aes(screener, stringr::str_trunc(title, 30, "right"), 
                      fill=factor(screen_decision, levels = c(1,-1), labels = c("incl","excl"))))+
  geom_tile()+
  scale_fill_manual(values = c("chartreuse4","darksalmon"))+
  labs(y = "Title", fill = "Decision")+
  labs(caption = paste("Fleiss' Kappa statistic = ", signif(set2KappaTest$value, 2)))+
  # geom_tile(data=agreement, aes(
  #   x=screener, y=stringr::str_trunc(title, 30, "right"),colour = propPos), 
  #   fill="transparent", inherit.aes = FALSE)+
  # scale_colour_gradientn(colours = c("transparent","red","transparent"))+
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    legend.position = "bottom"
  )


ggp2 <- ggplot(set2, aes("agreement", stringr::str_trunc(title, 30, "right"),
                 fill = propPos))+
  geom_tile()+
  scale_fill_gradientn(colours = c("transparent","red","transparent"), name="Agreement")+
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.background = element_rect(fill = "white"),
    axis.ticks = element_blank()
  )


# pdf(here::here("figures","screening_training_sets","screening_set2.pdf"),
#     width = 7, height = 5)
# egg::ggarrange(ggp1, ggp2, nrow=1, ncol=2, widths = c(3,1))
# 
# dev.off()
```


```{r}
require(irr)

# needs a dataframe where each row is an article, and each column is a reviewer. The values represent the decision (can also be multi-categorical)


```





