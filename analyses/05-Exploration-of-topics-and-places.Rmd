---
title: "05-Exploration-of-topics-and-places"
author: "Devi Veytia"
date: "2023-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(reticulate)

```


# Set up data to work with

Right now I will use unique references, but ideally this should be only the included screens
```{r set up dataset}
## inputs (need to change)
sqlite.fp <- here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite")
tbl.name <- "uniquerefs"


## Get records from database
dbcon <- dbConnect(RSQLite::SQLite(), sqlite.fp, create = FALSE)
recs <- tbl(dbcon, tbl.name)


## perhaps just to on a subsample
my_text <- recs %>%
  select(analysis_id, duplicate_id, title, abstract) %>%
  slice_sample(n=100) %>%
  collect()

dim(my_text)
head(my_text)


## Clost database connection
DBI::dbDisconnect(dbcon)
```


```{r load in predicted relevance and convert to sqlite}

## Write xlsx file to a sqlite database for easy joining

predRel <- readxl::read_excel(here::here("data","raw-data","relevance-predictions","1_document_relevance_28022023.xlsx"))

# rename columns
colnames(predRel) <- c("analysis_id","relevance_mean","relevance_std","relevance_lower","relevance_upper")

predRel_db <- dbConnect(SQLite(),
                        dbname = here::here("data","raw-data","sql-databases","relevance-predictions.sqlite"),
                        create = TRUE)

dbWriteTable(predRel_db, "predRel", predRel, overwrite = TRUE)
dbDisconnect(predRel_db)
```


# Summarise relevance predictions

```{r}

## Database connection
predRel_db <- dbConnect(SQLite(), dbname = here::here("data","raw-data","sql-databases","relevance-predictions.sqlite"),create = FALSE)
predRel <- tbl(predRel_db, "predRel")


## Summaries
predRel %>%
  summarise(
    mean = sum(0.5 <= relevance_mean),
    lower = sum(0.5 <= relevance_lower),
    upper = sum(0.5 <= relevance_upper)
  )

## close database
dbDisconnect(predRel_db)

```



# Extracting occurrence of key terms from articles

## Process text

```{r source functions}
functionsToSource <- c("clean_string.R", "screen.R","tokenization.R","utils.R")
for(i in 1:length(functionsToSource)){
  source(here::here("R", functionsToSource[i]))
}

```


```{r get corresponding title and abstract info for predicted relevance greater than 0.5}
require(dbplyr)

## Get tables from databases
dedup_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite"), 
                                 create=FALSE)
src_dbi(dedup_db)
dedups <- tbl(dedup_db, "uniquerefs")

predRel_db <- dbConnect(SQLite(), dbname = here::here("data","raw-data","sql-databases","relevance-predictions.sqlite"),create = FALSE)
predRel <- tbl(predRel_db, "predRel")



## Filter and join

# subset to the records I want to join (pred rel => 0.5)
relRefs <- predRel %>%
  select(analysis_id, relevance_mean) %>%
  filter(0.5 <= relevance_mean) 

# use join to retreive metadata for those records
relRefs_join <- left_join(relRefs, dedups %>% select(analysis_id,title, abstract), by = "analysis_id", copy=TRUE)
relRefs_join <- relRefs_join %>%
  collect()


## Disconnect databases
dbDisconnect(dedup_db)
dbDisconnect(predRel_db)

```


I assembled a list of keywords grouped by different descriptive factors we would like to extract information about. We then scanned the article title, abstract and keywords and counted the occurrences of matches to these terms (boolean response of "yes" for at least one match/article).

```{r clean text}

## Load spreadsheet of keywords to extract and clean the text
nlp_search_terms <- readxl::read_excel(here::here("data","derived-data","coding","keyword-search-tokens.xlsx"))
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling

# separate out into single terms and expressions
single_words <- nlp_search_terms$Term[which(nlp_search_terms[,4] == "single")]
expressions <- nlp_search_terms$Term[which(nlp_search_terms[,4] == "expression")]
# name them by their corresponding group
names(single_words) <- nlp_search_terms$`Group name`[which(nlp_search_terms[,4] == "single")]
names(expressions) <- nlp_search_terms$`Group name`[which(nlp_search_terms[,4] == "expression")]



## Process the text to screen

# group title, abstract together
nlptxt <- my_text %>%
  mutate(text = paste(title, abstract))%>%
  select(duplicate_id, text)

# Lemmitization and clean string to remove extra spaces, numbers and punctuation
nlptxt$text <- clean_string(nlptxt$text)
nlptxt$text <- textstem::lemmatize_strings(nlptxt$text)

```


## Screen for keywords

```{r  screen for matches to make a document feature matrix takes a while to run}
start <- Sys.time()

screens_swd <- screen(nlptxt$text, single_words)
save(screens_swd, file=here::here("data","derived-data","coding","keyword-matches","single-word-matches.RData"))

screens_expr <- screen(nlptxt$text, expressions)
save(screens_expr, file=here::here("data","derived-data","coding","keyword-matches","expression-matches.RData"))

end <- Sys.time()

diff <- end-start
as.numeric(diff)*(300000/100)/3600 # number of hours it would take to process all records, about 2.5 hours


# # remove files
rm(screens_swd, screens_expr)

```


## Visualizing keyword search results

identify numbers of papers relevant to each topic

```{r load in screening results}
## Load and format matching results
load(here::here("data","derived-data","coding","keyword-matches","single-word-matches.RData"))
load(here::here("data","derived-data","coding","keyword-matches","expression-matches.RData"))

# order dataset according to spreadsheet
screens_all <- cbind(screens_expr, screens_swd)
screens_all <- screens_all[,match(gsub(" ","_", nlp_search_terms$Term), colnames(screens_all))]

# remove separate matrices
rm(screens_swd, screens_expr)



# some concepts require co-occurrences between two terms (i.e. AND statements)
# so make a new matrix with these values
complexTerms <- matrix(nrow=nrow(screens_all),
                       ncol = 2,
                       dimnames = list(
                         nlptxt$duplicate_id,
                         c("carbon removal or storage","coastal communities and safe space or fish")
                       ))


# for carbon AND removal

# indices of which columns contain synonyms matching relevant keywords
carb_ind <- which(nlp_search_terms$`Group name` == "carbon")
rem_ind <- which(nlp_search_terms$`Group name` == "removal")

# a y/n response for whether any of the synonymns are found
carb <- ifelse(1 <= screens_all[,carb_ind], 1,0)
rem <- ifelse(rowSums(1 <= screens_all[,rem_ind]), 1,0) 

# whether there are words matching both of the keyword groups for carbon AND removal
complexTerms[,1] <- ifelse(carb == 1 & rem == 1, 1, 0)



# for coasts and communities
coast_ind <- which(nlp_search_terms$`Group name` == "coasts")
comm_ind <- which(nlp_search_terms$`Group name` == "communities")
coast <- ifelse(rowSums(1 <= screens_all[,coast_ind]), 1,0)
comm <- ifelse(rowSums(1 <= screens_all[,comm_ind]), 1,0)
coastAndComm <- ifelse(coast == 1 & comm == 1, 1, 0)

# for safe space or fish -- i.e. other Group names that aren't coasts or communities
rest_ind <- which(nlp_search_terms$Group == "Paper4")
rest_ind <- rest_ind[which(!(rest_ind %in% c(coast_ind, comm_ind)))]
rest <- ifelse(rowSums(1 <= screens_all[,rest_ind]), 1, 0)

# for coasts and communities and safe space or fish
complexTerms[,2] <- ifelse(coastAndComm ==1 & rest == 1, 1,0)

# tablulate totals
complexTerms <- colSums(complexTerms)

## Add these new concepts in to the tabulated matrix and convert to data frame
tab_screens <- colSums(screens_all[,-c(carb_ind, rem_ind, coast_ind, comm_ind, rest_ind)])
rm(screens_all)
tab_screens <- c(tab_screens, complexTerms)
tab_screens <- as.data.frame(tab_screens)
colnames(tab_screens) <- c("n_matches")
# new vector of which paper each column belongs to
tab_screens$Topic <- c(nlp_search_terms$Group[-c(carb_ind, rem_ind, coast_ind, comm_ind, rest_ind)], "Paper1","Paper4")
tab_screens$Keyword <- rownames(tab_screens)
tab_screens$Keyword_group <- c(
  nlp_search_terms$`Group name`[-c(carb_ind, rem_ind, coast_ind, comm_ind, rest_ind)], 
  names(complexTerms)
  )
rownames(tab_screens) <- NULL
```


```{r plot the number of matches}
library(ggplot2)

# change groupings of mitigation keywords
tab_screens$Keyword_group[
  which(tab_screens$Keyword_group %in% c("CO2 removal from seawater",
                                   "OIF","blue carbon","bioenergy"))
] <- "carbon removal or storage"

topics <- unique(tab_screens$Topic)
topicNames <- c(
  "R&D vs implementation of mitigation OROs",
  "Biodiv & NCP outcomes",
  "Restoration practices",
  "Coastal community adaptation portfolio"
)


# ymax <- tab_screens %>%
#   group_by(Topic) %>%
#   summarise(n = sum(n_matches, na.rm=T))
# ymax <- ymax$n[which.max(ymax$n)]


ggp_list <- list()

for(i in 1:length(topics)){
  
  if(i %in% c(1,3)){
    yaxis_title <- "Number of matches"
  }else{
    yaxis_title <- ""
  }
  
  tempTitle <- paste0(letters[i],". Paper ", i, ": ",topicNames[i], collapse="")
  
  
  ggp_list[[i]] <- tab_screens %>%
    filter(Topic == topics[i]) %>%
    ggplot(aes(Topic, n_matches, fill = Keyword_group)) +
    geom_col()+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    #ylim(0, ymax)+
    scale_fill_brewer(type="qual", palette = i)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
    
}


egg::ggarrange(
  plots = ggp_list,
  nrow=2
)


```


```{r clean environment}
rm(list=ls())
```


# Geoparsing text


Repeat the set up chunk from the beginning of the doc to load files
```{r set up dataset}

```

```{r combine text for geoparsing}

vec2geocode <- paste(my_text$title, my_text$abstract)
vec2geocode_duplicate_id <- my_text$duplicate_id

rm(my_text)

```



```{r clean text and remove stopwords}
#vec2geocode <- c('I went to Ottawa and then London.','Pacific northwest')


# # clean 
# vec2geocode <- clean_string(vec2geocode[1:2])
# 
# # remove stopwords
# 
# # identify stopwords
# my_stopwords <- unique(c(quanteda::stopwords(),revtools::revwords(), litsearchr::get_stopwords(), "sea", "ocean", "marine", "coast", "south", "part"))
# 
# # remove
# for(i in 1:length(vec2geocode)){
#   vec2geocode[i] <- paste(quanteda::tokens_remove(quanteda::tokens(vec2geocode[i]),
#                                                   my_stopwords),collapse = " ")
# }


```



```{r}
use_virtualenv(here::here("spaCy_env"))
#use_virtualenv(here::here("localGeocode_env"))
#use_python("C:\\Users\\deviv\\AppData\\Local\\Programs\\Python\\PYTHON~1\\python.exe")
```

Use spacy to extract components of sentences that contain locations.

Types of entity labels available:
PERSON:      People, including fictional.
NORP:        Nationalities or religious or political groups.
FAC:         Buildings, airports, highways, bridges, etc.
ORG:         Companies, agencies, institutions, etc.
GPE:         Countries, cities, states.
LOC:         Non-GPE locations, mountain ranges, bodies of water.
PRODUCT:     Objects, vehicles, foods, etc. (Not services.)
EVENT:       Named hurricanes, battles, wars, sports events, etc.
WORK_OF_ART: Titles of books, songs, etc.
LAW:         Named documents made into laws.
LANGUAGE:    Any named language.
DATE:        Absolute or relative dates or periods.
TIME:        Times smaller than a day.
PERCENT:     Percentage, including ”%“.
MONEY:       Monetary values, including unit.
QUANTITY:    Measurements, as of weight or distance.
ORDINAL:     “first”, “second”, etc.
CARDINAL:    Numerals that do not fall under another type.

attributes can be displayed using dir(doc)
```{python}

import spacy
nlp = spacy.load("en_core_web_sm")


Entarr = [] ;
Datearr = [] ;


for i in range(len(r.vec2geocode)):
  doc = nlp(r.vec2geocode[i])
  
  for entity in doc.ents:
    
    if entity.label_ in ('GPE' 'LOC'):
      ents = [r.vec2geocode_duplicate_id[i], entity.label_, entity.text]
      Entarr.append(ents)
      
    if entity.label_ == 'DATE':
      ents = [r.vec2geocode_duplicate_id[i], entity.label_, entity.text]
      Datearr.append(ents)


print(Entarr[0])
print(Datearr[0])



import pickle
import os

pickle.dump(Entarr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "wb"))
pickle.dump(Datearr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Dates.pickle"]), "wb"))

del Datearr
del Entarr


```

note there will likely be duplicates in place names within a duplicate_id, so make sure to clean afterwards. Columns are: duplicate_id, entity_label, entity_text


call the output from location tagging in spacy directly to retreive metadata about the location
```{python}
from geocode.geocode import Geocode
gc = Geocode()
gc.load()

import numpy as np


Entarr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "rb"))
Entarr_numpy = np.array(Entarr)

Geocodearr = [] ;

for i in range(len(Entarr)):
  location = gc.decode(Entarr_numpy[i,2])
  Geocodearr.append(location)
  

pickle.dump(Geocodearr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Geocode-Metadata.pickle"]), "wb"))



del Geocodearr
del Entarr


```

Note also the option of gc.decode_parallel in https://github.com/mar-muel/local-geocode



Map the distribution of locations mentioned

```{python}
Entarr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "rb"))
Geocodearr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"geocode-metadata.pickle"]), "rb"))

quit
```

```{r}
require(tidyverse)

# add the duplicate id to each entry
geoarr <- list()

for(i in 1:length(py$Entarr)){
  # append each element of the list with the corresponding duplicate id -- sometimes multiple entries for each entity because returns
  # multiple matches
  tmp <- lapply(py$Geocodearr[[i]], function(x) append(x, py$Entarr[[i]]))
  # Add names to these new columns
  tmp <- lapply(tmp, function(x) {names(x)[9:11] <- c("duplicate_id","entity_label","entity_text"); x})
  geoarr[[i]] <- tmp
}

locationsDf <- flatten(geoarr)
rm(geoarr)

locationsDf <- lapply(locationsDf, function(x) as.data.frame(x))

locationsDf <- do.call(rbind.data.frame, locationsDf)


# remove any duplicated identifiations within each article
# sometimes there will also be multiple values for an entity label -- in this case take the first?
locationsDf <- locationsDf %>%
  distinct(geoname_id, duplicate_id, entity_text, .keep_all=TRUE) 

## BUT geolocating only really works on GPE, not LOC types, so maybe seperate as several columns in the metadata


save(locationsDf, file = here::here("data","derived-data","coding","nlp-extracted","geocode-metadata.RData"))
```

```{r}
library(maps)

load(here::here("data","derived-data","coding","nlp-extracted","geocode-metadata.RData"))

world_map <- map_data("world")

# # add the sizes of the different location_types
# location_size_lookup <- data.frame(
#   location_type = c('city', 'place', 'country', 'admin1', 'admin2', 'admin3', 'admin4', 'admin5', 'admin6', 'admin_other', 'continent', 'region'),
#   area  = c()
# )

ggplot()+
  geom_polygon(data = world_map, aes(x=long, y=lat, group=group), fill="grey")+

  geom_point(data=locationsDf %>% filter(entity_label == "GPE") %>% arrange(population), 
             aes(longitude, latitude, size = log(population), color = log(population)), alpha=0.3)+
  
  coord_sf()+
  scale_size_continuous()+
  scale_color_viridis_c(option = "magma")+
  theme_void()+
  guides(color = guide_legend())+
  theme(
    legend.position = c(0.1, 0.5),
      plot.background = element_rect(fill = "#f5f5f2", color = NA), 
      panel.background = element_rect(fill = "#f5f5f2", color = NA), 
      legend.background = element_rect(fill = "#f5f5f2", color = NA)
  )
  

  
```


# Junk code

Unfortunately couldn't get spacyr to work -- got stuck on spcay_install()

using: https://spacyr.quanteda.io/index.html
```{r install spaCy, eval=FALSE}
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)

library("spacyr")
spacy_install()

```


```{r use spacy to parse text and identify nouns}

library("spacyr")
spacy_initialize(model = "en_core_web_sm")

entities <- spacy_extract_entity(vec2geocode, output = "data.frame", type = "GPE")

entities

# finish spacy session
spacy_finalize()
```

