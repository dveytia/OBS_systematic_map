---
title: "09-Relevance-predictions-after-coding"
author: "Devi Veytia"
date: "2023-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(reticulate)
library(ggplot2)
```


This code explores relevance predictions after coding 

# Add predictions to sqlite database

```{r load in predicted relevance and convert to sqlite, eval=FALSE}

## Open sqlite database
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

src_dbi(all_db)


## Write include/exclude relevance predictions to a sqlite database for easy joining

# read in latest file
relDir <- here::here("data","raw-data","relevance-predictions")
relFiles <- dir(relDir)
relFiles <- relFiles[grep(".csv", relFiles)]
if(sum(grepl("Branch", relFiles, ignore.case = T))>0){
  relFiles <- relFiles[-grep("Branch", relFiles, ignore.case = T)]
}

dates<- gsub("[^0-9-]", "", relFiles)
dates <- stringr::str_sub(dates, 2)
dates <- as.Date(dates, format="%d%m%Y")
predRel <- read.csv(file.path(relDir, relFiles[which.max(dates)]))

# rename columns
colnames(predRel) <- c("analysis_id","relevance_mean","relevance_std","relevance_lower","relevance_upper")

# check for duplicates
if(sum(duplicated(predRel$analysis_id)) > 0){
  print("duplicates present")
  dupIds <- predRel$analysis_id[duplicated(predRel$analysis_id)]
  #View(predRel[which(predRel$analysis_id %in% dupsIds),])
  
  # # are the dupliate ids articles that have been "seen"? Yes
  # screens <- tbl(all_db, "allScreen_afterCoding") %>% 
  #   select(analysis_id, sample_screen, include_screen) %>%
  #   collect()
  # sum(dupIds %in% screens$analysis_id)
  # sum(screens$analysis_id %in% dupsIds)
  # test <- predRel[which(predRel$analysis_id %in% dupIds),] %>%
  #   inner_join(screens, by="analysis_id")
  # summary(test)
  # 
  # summary(predRelMeta[which(predRel$analysis_id %in% dupIds),])
  # nrow(predRelMeta[which(predRel$analysis_id %in% dupIds),])
  
  ## remove dupliate ids
  
  for(i in 1:length(dupsIds)){
    temp <- predRel[predRel$analysis_id == dupIds[i],]
    temp <- subset(temp, 0 < relevance_mean & relevance_mean < 1)
    if(i==1)
      temp2 <- temp
    else
      temp2 <- rbind(temp2, temp)
  }
  predRel <- rbind(
    predRel[-dupIds,], temp2
  )
  rm(temp2, temp)
}



## Multilabel screen using ORO_branch
relFiles <- dir(relDir)
relFiles <- relFiles[grep("Branch", relFiles, ignore.case = T)]
dates<- gsub("[^0-9-]", "", relFiles)
dates <- stringr::str_sub(dates, 2)
dates <- as.Date(dates, format="%d%m%Y")
ind <- which.max(dates)
if(grepl(".csv", relFiles[ind])){
  predBranch <- read.csv(file.path(relDir, relFiles[ind]))
}else{
  predBranch <- readxl::read_excel(file.path(relDir, relFiles[ind]))
}
# rename columns
colnames(predBranch) <- c("analysis_id",
                          paste("Mitigation", c("mean","std","lower","upper"), sep="_"),
                          paste("Nature", c("mean","std","lower","upper"), sep="_"),
                          paste("Societal", c("mean","std","lower","upper"), sep="_"),
                          paste("Unclear", c("mean","std","lower","upper"), sep="_")
                          )

# check for duplicates
if(anyDuplicated(predBranch$id)){
  print("duplicates present")
}




## Write to sql database
dbWriteTable(all_db, "predRel2", predRel, overwrite=TRUE)
dbWriteTable(all_db, "predBranch", predBranch, overwrite=TRUE)


dbDisconnect(all_db)
```


# Compare different methods of relevance predictions


## Summarise overall relevance numbers


```{r load all relevance predictions}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
predRel0 <- tbl(all_db, "predRel") %>% collect()
predRel1 <- tbl(all_db, "predRel1") %>% collect()
predRel <- tbl(all_db, "predRel2") %>% collect()
predBranch <- tbl(all_db, "predBranch") %>% collect()

dbDisconnect(all_db)
```

```{r plot numbers of relevant predictions}
## Format dataframes
# combine predictions from the three different branches into just the max relevance across any branch
predBranch <- predBranch[,-grep("Unclear",colnames(predBranch))] # remove unclear from predictions
predBranchMax <- data.frame(
  analysis_id = predBranch$analysis_id,
  relevance_mean = apply(predBranch[,grep("mean", colnames(predBranch))],1, max, na.rm=T),
  relevance_lower = apply(predBranch[,grep("lower", colnames(predBranch))],1,max, na.rm=T),
  relevance_upper = apply(predBranch[,grep("upper", colnames(predBranch))],1,max, na.rm=T)
)

# combine predictions from all the models
df <- predRel0 %>%
  select(analysis_id, relevance_mean, relevance_lower, relevance_upper) %>%
  mutate(model = "Binary after screen") %>%
  rbind(predRel1 %>% 
              select(analysis_id, relevance_mean, relevance_lower, relevance_upper) %>%
          mutate(model = "Binary after code")) %>%
  rbind(predRel %>% 
              select(analysis_id, relevance_mean, relevance_lower, relevance_upper) %>%
          mutate(model = "Binary excl screen, incl code")) %>%
  rbind(predBranchMax %>% mutate(model = "Binary ORO_branch after code")) %>%
  mutate(include = ifelse(0.5 <= relevance_mean, "include","exclude"))%>%
  mutate(include = factor(include,levels = c("include","exclude"))) %>%
  mutate(model = factor(model, levels=c(
    "Binary after screen", "Binary after code","Binary excl screen, incl code",
    "Binary ORO_branch after code"
  )))



## Summarise numbers of relevant predictions
relevanceSummaries <- df %>%
  group_by(model) %>%
  summarise(include_mean = sum(0.5 <= relevance_mean),
            include_lower = sum(0.5 <= relevance_lower),
            include_upper = sum(0.5 <= relevance_upper))
relevanceSummaries



## fit loess smoothers to make a line for mean, lower and upper
smoothFitFn <- function(dfSub, subSamp=100){
  dfSub <- dfSub[order(dfSub$relevance_mean),]
  dfSub$ID <- seq(1:nrow(dfSub))
  # sub-sample
  dfSub <- dfSub[seq(1,nrow(dfSub), subSamp),]
  # mean
  meanSmooth <- loess(relevance_mean~ID, data=dfSub)
  meanPred <- predict(meanSmooth,
                 data.frame(ID=seq(1, max(dfSub$ID), 100)))
  # lower
  lowerSmooth <- loess(relevance_lower~ID, data=dfSub)
  lowerPred <- predict(lowerSmooth,
                 data.frame(ID=seq(1, max(dfSub$ID), 100)))
  # upper
  upperSmooth <- loess(relevance_upper~ID, data=dfSub)
  upperPred <- predict(upperSmooth,
                 data.frame(ID=seq(1, max(dfSub$ID), 100)))
  # return
  out <- data.frame(
    analysis_id = seq(1, max(dfSub$ID), 100),
    mean_smooth = meanPred,
    lower_smooth = lowerPred,
    upper_smooth = upperPred
  )
  return(out)
}

mods <- levels(df$model)
for(m in 1:length(mods)){
  dfSub <- subset(df, model == mods[m])
  temp <- smoothFitFn(dfSub, subSamp = 75)
  temp$model <- mods[m]
  if(m==1){
    predSmoothDf = temp
  }else{
    predSmoothDf <- rbind(predSmoothDf, temp)
  }
  if(m==length(m)){
    predSmoothDf$model <- factor(predSmoothDf$model, levels=mods)
  }
}





ggplot(predSmoothDf)+
  geom_ribbon(aes(x=analysis_id, ymin=lower_smooth, ymax=upper_smooth), fill="pink")+
  geom_hline(yintercept = 0.5, linewidth = 0.5, col="red", linetype="dashed")+
  geom_path(aes(x=analysis_id, y = mean_smooth), linewidth=0.75)+
  geom_text(data=relevanceSummaries, x=0,y=1.25, aes(label = paste("mean", include_mean)), hjust=0, size=3)+
  geom_text(data=relevanceSummaries, x=0,y=1.15, aes(label = paste0("(", include_lower,"-",include_upper,")")), hjust=0, size=3)+
  facet_wrap(vars(model))+
  labs(x="article","predicted relevance smooth")+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )


```


## Assess mean relevance against seen articles


```{r load all relevance predictions and accompanying metadata}
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)
predRel0 <- tbl(all_db, "predRel") %>% collect()
predRel1<- tbl(all_db, "predRel1") %>% collect()
predRel <- tbl(all_db, "predRel2") %>% collect()
predBranch <- tbl(all_db, "predBranch") %>%
  select(analysis_id, Mitigation_mean, Nature_mean, Societal_mean) %>%
  collect()
screens <- tbl(all_db, "allScreen_afterCoding") %>% collect()
codebookCombinedSimplified <- tbl(all_db, "allCodingSimplifiedVariables") %>% 
  select(analysis_id, oro_branch.Mitigation,oro_branch.Nature,oro_branch.Societal) %>%
  collect()
dedupsSub <- tbl(all_db, "uniquerefs") %>% 
  select(analysis_id, duplicate_id, title) %>%
  collect()

dbDisconnect(all_db)
```


```{r merge dataframes so I can compare relevance predictions side by side}

df <- screens %>%
  select(analysis_id, sample_screen, include_screen) %>%
  left_join(predRel %>% select(analysis_id, relevance_mean)) %>%
  rename(relevance_mean2 = relevance_mean) %>%
  left_join(predRel0 %>% select(analysis_id, relevance_mean), by="analysis_id") %>%
  rename(relevance_mean1 = relevance_mean) %>%
  left_join(predBranch) %>%
  left_join(codebookCombinedSimplified, by="analysis_id")%>%
  #left_join(testListBranch %>% select(analysis_id, ORO_branch))%>%
  mutate(include_screen = factor(include_screen, 
                                 levels = c(1,0), labels = c("include","exclude")))%>% #ORO_branch = factor(ORO_branch, levels = c("Mitigate","Natural","Societal","Other"))
  left_join(dedupsSub %>% select(analysis_id, duplicate_id))

# add a column for the maximum prediction for any branch, and what the name of the branch was
df$branchMaxPred <- apply(df[,c("Mitigation_mean","Nature_mean","Societal_mean")],
                        1, max, na.rm=T)
df$branchMax <- apply(df[,c("Mitigation_mean","Nature_mean","Societal_mean")],
                        1, which.max)
df$branchMax <- c("Mitigation","Nature","Societal")[df$branchMax]

# summarise
summary(df)
```

### compare all models at predicting seen inclusions and exclusions

```{r summary statistics}
## Summary statistics

## Model performance metrics : TP, FP, FN
predRel1Summary <-  df %>%
  mutate(predIncl = ifelse(0.5 <= relevance_mean1, 1, 0),
         TP = ifelse(0.5 <= relevance_mean1 & include_screen == "include", 1,0),
         FP = ifelse(0.5 <= relevance_mean1 & include_screen == "exclude", 1,0),
         FN = ifelse(relevance_mean1 < 0.5 & include_screen == "include", 1,0)) 

predRel2Summary <-  df %>%
  mutate(predIncl = ifelse(0.5 <= relevance_mean2, 1, 0),
         TP = ifelse(0.5 <= relevance_mean2 & include_screen == "include", 1,0),
         FP = ifelse(0.5 <= relevance_mean2 & include_screen == "exclude", 1,0),
         FN = ifelse(relevance_mean2 < 0.5 & include_screen == "include", 1,0)) 

predBranchSummary <- df
predBranchSummary$maxPred <- apply(
  df[,c("Mitigation_mean", "Nature_mean", "Societal_mean")], 1, max)
predBranchSummary <-  predBranchSummary %>%
  mutate(predIncl = ifelse(0.5 <= maxPred, 1, 0),
         TP = ifelse(0.5 <= maxPred & include_screen == "include", 1,0),
         FP = ifelse(0.5 <= maxPred & include_screen == "exclude", 1,0),
         FN = ifelse(maxPred < 0.5 & include_screen == "include", 1,0))
    

performanceTabs <- rbind(
  colSums(predRel1Summary[,c("TP","FP","FN")]),
  colSums(predRel2Summary[,c("TP","FP","FN")]),
  colSums(predBranchSummary[,c("TP","FP","FN")]) 
)
rownames(performanceTabs) <- c("Binary after screen","Binary after code","Binary ORO_branch after code")
performanceTabs <- as.data.frame(performanceTabs)
performanceTabs <- performanceTabs %>%
  mutate(precision = TP/(TP+FP), recall = TP/(TP+FN),
         Fratio = ((2)*precision*recall)/(precision+recall))
print(signif(performanceTabs, 3))



# F-beta scores
FBetaFunction <- function(TP,FP, FN, Beta){
  precision = TP/(TP+FP)
  recall = TP/(TP+FN)
  Beta2 <- Beta^2
  FBeta = ((1+Beta2)*precision*recall)/(Beta2*precision+recall)
  names(FBeta) <- paste0("Beta=", Beta)
  return(FBeta)
}

Betas <- seq(1, 2, length.out = 5)
FBeta <- vector("numeric", length=5)

for(i in 1:length(Betas)){
  FBeta[i] <- FBetaFunction(performanceTabs[1], performanceTabs[2], performanceTabs[3], Betas[i])
}
FBeta


## Seperated by sample_screen method
summaryStats <- predRelSummary %>%
  group_by(sample_screen) %>%
  summarise(TP = sum(TP), FP = sum(FP), FN = sum(FN)) %>%
  mutate(precision = TP/(TP+FP), recall = TP/(TP+FN))
summaryStats

#   sample_screen          TP    FP    FN precision recall
#   <chr>               <dbl> <dbl> <dbl>     <dbl>  <dbl>
# 1 random                401    70    26     0.851  0.939
# 2 relevance sort        185    36     4     0.837  0.979
# 3 supplemental coding   253    60     1     0.808  0.996
# 4 test list              86     6     0     0.935  1 

predBranchSummary %>%
  group_by(sample_screen) %>%
  summarise(TP = sum(TP), FP = sum(FP), FN = sum(FN)) %>%
  mutate(precision = TP/(TP+FP), recall = TP/(TP+FN))

#   sample_screen          TP    FP    FN precision recall
#   <chr>               <dbl> <dbl> <dbl>     <dbl>  <dbl>
# 1 random                250   731   177     0.255  0.585
# 2 relevance sort        134   142    55     0.486  0.709
# 3 supplemental coding    73    47   181     0.608  0.287
# 4 test list              46     7    40     0.868  0.535


# distribution of predicted values for test list inclusions
quantile(test$relevance_mean2[test$sample_screen =="test list" &
                       test$include_screen == "include"])


# distribution of branch predictions for test list -- very low
test %>%
  filter(!is.na(ORO_branch)) %>%
  reshape2::melt(measure.vars = c("Mitigation_mean","Nature_mean","Societal_mean")) %>%
  ggplot(aes(x=ORO_branch, y=value))+
  geom_violin()+
  facet_wrap(vars(variable))

# distribution of overall branch predictions
test %>%
  reshape2::melt(measure.vars = c("Mitigation_mean","Nature_mean","Societal_mean")) %>%
  ggplot(aes(variable, y=value))+
  geom_violin()


```


```{r plots of predicted relevance for known screening decisions}
# compare distributions between inclusions and exclusions
df %>%
  select(include_screen, relevance_mean1, relevance_mean2, branchMaxPred) %>%
  reshape2::melt(measure.vars = c("relevance_mean1", "relevance_mean2","branchMaxPred")) %>% 
  ggplot()+
  geom_hline(yintercept = 0.5, linetype="dashed", col="red")+
  geom_violin(aes(x=variable, y=value, fill = variable))+
  labs(y="predicted relevance", x="iteration")+
  scale_fill_discrete(name="Model")+
  facet_wrap(vars(include_screen))+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "bottom"
  )

# facet by sampling strategy as well
df %>%
  select(include_screen, sample_screen, relevance_mean1, relevance_mean2, branchMaxPred) %>%
  reshape2::melt(measure.vars = c("relevance_mean1", "relevance_mean2","branchMaxPred")) %>% 
  ggplot()+
  geom_hline(yintercept = 0.5, linetype="dashed", col="red")+
  geom_violin(aes(x=variable, y=value, fill = variable), trim=T)+
  labs(y="predicted relevance", x="iteration")+
  facet_grid(sample_screen~include_screen)+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "bottom"
  )

# predictions for articles that were first predicted include but are now exclude
# -- no pattern is apparent
df %>%
  filter(0.5 <= relevance_mean1 & relevance_mean2 < 0.5) %>%
  ggplot(aes(relevance_mean1, relevance_mean2))+
  geom_point()+
  facet_wrap(vars(include_screen))
# maybe if we took the relevance down to ~0.3 we would include most of them, but is it worth it?

# predictions for articles that were first predicted exclude but are now include
# -- no pattern is apparent
df %>%
  filter(0.5 <= relevance_mean2 & relevance_mean1 < 0.5) %>%
  ggplot(aes(relevance_mean1, relevance_mean2))+
  geom_point()


# what does the old relevance predictions say for the new additions from supplementary coding?
# as a violin plot
df %>%
  filter(sample_screen == "supplemental coding") %>%
  reshape2::melt(measure.vars = c("relevance_mean1","relevance_mean2","branchMaxPred"),
                 variable.name = "model")%>% 
  ggplot(aes(x=model, y=value, fill = model)) +
  geom_hline(yintercept = 0.5, linetype="dashed", col="red")+
  ggtitle("supplemental coding articles")+
  geom_violin()+
  labs(y = "predicted relevance")+
  facet_wrap(vars(include_screen))+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
# as density plots
df %>%
  filter(sample_screen == "supplemental coding") %>%
  reshape2::melt(measure.vars = c("relevance_mean1","relevance_mean2","branchMaxPred"),
                 variable.name = "model")%>% 
  ggplot(aes(x=value, fill = include_screen)) +
  geom_vline(xintercept = 0.5, linetype="dashed", col="red")+
  ggtitle("supplemental coding articles")+
  geom_density(alpha=0.5)+
  scale_fill_discrete(name = "include screen")+
  labs(x = "predicted relevance")+
  facet_wrap(vars(model))



## For ORO_branch, can break this down further by ORO_branch classification

# melt the dataframe by the seen values
branchDf <- df %>%
  select(-c(relevance_mean1, relevance_mean2)) %>%
  reshape2::melt(measure.vars = colnames(df)[grep("oro_branch", colnames(df))],
                 value.name = "seen_branch",variable.name = "oro_branch")%>%
  mutate(oro_branch = factor(oro_branch, 
                             c("oro_branch.Mitigation","oro_branch.Nature",
                               "oro_branch.Societal"),
                             c("Mitigation","Nature","Societal")))%>%
  mutate(seen_branch = ifelse(seen_branch==1, as.character(oro_branch), NA)) %>%
  select(-c(oro_branch)) %>%
  filter(!is.na(seen_branch))
# branchDf %>% filter(is.na(seen_branch)) %>% View # NB seen_branch will == NA when include_screen == exclude 

# check all inclusions have an oro_branch
if(sum(!(branchDf$analysis_id %in% df$analysis_id[df$include_screen == "include"])) >0){
  print("inclusions with no ORO branch assignment")
  View(df[which(!(branchDf$analysis_id %in% df$analysis_id[df$include_screen == "include"])),])
}

# add a column for the predictions for the branch that was picked when seen
pred <- rep(NA, nrow(branchDf))
for(i in 1:length(pred)){
  if(branchDf$seen_branch[i] == "Mitigation"){
    pred[i] <- branchDf[i, grep("Mitigation", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Nature"){
    pred[i] <- branchDf[i, grep("Natur", colnames(branchDf), ignore.case = T)]
  }else if(branchDf$seen_branch[i] == "Societal"){
    pred[i] <- branchDf[i, grep("Societal", colnames(branchDf), ignore.case = T)]
  }else{
    pred[i] <- NA
  }
}
branchDf$seen_branch_pred <- pred; rm(pred)

# plot what the predicted relevance is for the branch that should have been chosen
ggplot(branchDf, aes(x=seen_branch, y=seen_branch_pred))+
  geom_violin()


# facetted by the branch seen, what branch had the highest prediction? 
branchDf %>%
  select(analysis_id, Mitigation_mean, Societal_mean, Nature_mean, seen_branch) %>%
  reshape2::melt(measure.vars = c("Mitigation_mean", "Societal_mean", "Nature_mean"),
                 variable.name = "predictedBranch", value.name = "predictedRelevance")%>%
  mutate(predictedRelevance = ifelse(0.5 <= predictedRelevance, predictedRelevance, NA)) %>%
  ggplot(aes(x=predictedBranch, y= predictedRelevance, fill=predictedBranch))+
  geom_violin()+
  facet_wrap(vars(seen_branch))+
  labs(y="predicted relevance", x = "predicted ORO branch", fill = "predicted ORO branch",
       caption="Facetted by the branch seen, what branches were predicted to be relevant?")+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )



# tabulate the actual and predicted Branch classifications (just compare selections to the maximum predicted relevance branch, even though there may be multiple predictions/ analysis)
lev <- c("Mitigation","Nature","Societal")
# get all the branches predicted for each article
predictedBranch <- df %>%
  mutate(Mitigation = ifelse(0.5 <=Mitigation_mean, Mitigation_mean, NA),
         Nature = ifelse(0.5 <= Nature_mean, Nature_mean, NA),
         Societal = ifelse(0.5 <= Societal_mean, Societal_mean, NA)) %>%
  reshape2::melt(measure.vars = lev, variable.name = "predictedBranch",
                 value.name = "predictedBranchRelevance") %>%
  filter(!is.na(predictedBranchRelevance), include_screen == "include") %>% 
  select(analysis_id, predictedBranch, predictedBranchRelevance)
# tabulate a confusion matrix
seenBranch <- branchDf %>%
  select(analysis_id, seen_branch)
analysis_ids <- unique(branchDf$analysis_id)
tempTab <- array(dim = c(length(lev),length(lev),length(analysis_ids)),
                 dimnames = list(seen = lev, predicted = lev, analysis_id = analysis_ids))
for(o in 1:length(analysis_ids)){
  seenClasses <- unique(branchDf$seen_branch[which(branchDf$analysis_id == analysis_ids[o])])
  predClasses <- unique(predictedBranch$predictedBranch[which(predictedBranch$analysis_id ==analysis_ids[o])])
  if(length(predClasses)==0){
    next
  }
  for(i in 1:length(lev)){
    for(j in 1:length(lev)){
      tempTab[i,j,o] <- sum(lev[i] %in% seenClasses & lev[j] %in% predClasses)
    }
  }
}
branchPredictionsTab <- apply(tempTab, 1:2, sum, na.rm=T)
branchPredictionsTab

# write outcomes
sink(here::here("outputs/coding/ConfusionMatrix_oroBranchPredictions.txt"))
print(caret::confusionMatrix(branchPredictionsTab))
sink()
```

## Explore changes in seen article datasets used between models


```{r Explore the changes in inclusions/exclusions between the screening and post coding datasets}

## Load data
# load screen results before coding as well
load(here::here("data","derived-data","screening","screened-records","screen_results_merged.RData"))
# retreive the analysis ids as well as look up tables for the different ids
all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"),
                                 create=FALSE)
sysrevIDLookup <- tbl(all_db, "sysrevid_2_analysisid_lookup") %>% collect()

RSQLite::dbDisconnect(all_db)



## Explore changes in article inclusions between two datasets
# after coding, 2060 exclusions, 956 inclusions
table(screens$include_screen)

# number of inclusions added from supplemental coding = 254
screens %>%
  group_by(include_screen, sample_screen) %>%
  summarise(n=n())

# which test list articles were excluded?
screens %>%
  filter(include_screen == 0 & sample_screen == "test list") %>%
  select(-c(reviewer, reviewer_2, abstract, keywords)) %>%
  print()

# At the initial screening stage, 886 included, 1796 excluded
# note that one test list article was excluded
table(screen_results_merged$include_screen)
screen_results_merged %>%
  group_by(include_screen, sample_screen) %>%
  summarise(n=n())

# combine inclusions from screening stage with the coding screens to see changes
# therefore the column include_screen represents the decisions to include at coding. 
# a 0 would indicate that it was included then excluded
changedScreens <- screen_results_merged %>%
  filter(include_screen == TRUE) %>%
  select(sysrev_id, title, abstract, screener, sample_screen) %>%
  left_join(screens %>% 
              select(sysrev_id, title, reviewer, reviewer_2, include_screen), 
            by = "sysrev_id") %>%
  left_join(sysrevIDLookup %>% select(sysrev_id, analysis_id, duplicate_id)) %>%
  mutate(include_screen = factor(include_screen))
summary(changedScreens)


# who made the most exclusions at the coding stage?
changedScreens %>%
  filter(include_screen == 0) %>%
  group_by(reviewer) %>%
  summarise(n=n())

# how were the changed screens sampled?
changedScreens %>%
  filter(include_screen == 0) %>%
  group_by(sample_screen) %>%
  summarise(n=n())

# number of articles that were excluded at the coding stage = 184
changedScreens %>%
  filter(include_screen == 0) %>%
  nrow()


```

```{r relevance predictions for changed screens}
## Explore relevance predictions for changed screens
# what are the relevance predictions for articles that were include after screening, then changed to exclude?
changedScreensRelevance <- changedScreens %>%
  filter(include_screen == 0) %>%
  select(sysrev_id, analysis_id, duplicate_id, title.x, sample_screen) %>%
  rename(title = title.x) %>%
  left_join(test %>% select(duplicate_id, relevance_mean2, relevance_mean1)) %>%
  reshape2::melt(measure.vars = c("relevance_mean2","relevance_mean1"),
                 variable.name = "iteration",value.name="relevance")%>%
  mutate(iteration = factor(iteration, 
                            levels = c("relevance_mean1","relevance_mean2"),
                            labels = c("screening","screen excl, coding incl"))) 


ggplot(changedScreensRelevance, aes(x=iteration, y=relevance)) +
  geom_violin()+
  labs(caption = "amongst the articles that were included at screening and excluded at coding")



```


# Summarise relevance predictions for best model

```{r Get tables of unique references and relevance predictions}

all_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                    here::here("data","raw-data","sql-databases", "all_tables_v1.sqlite"), 
                                create=FALSE)

predRel <- tbl(all_db, "predRel2") 
dedups <- tbl(all_db, "uniquerefs") %>% 
  select(analysis_id, duplicate_id, title, abstract, keywords, doi)
screens <- tbl(all_db, "allScreen_afterCoding") %>% 
  select(duplicate_id, sample_screen, include_screen)

predRelMeta <- dedups %>%
  inner_join(predRel, by = "analysis_id") %>%
  full_join(screens, by = "duplicate_id") %>%
  collect()

predRel <- predRel %>% collect()

nrow(predRelMeta)== nrow(predRel)# check number of rows is correct and none were lost

dbDisconnect(all_db)
```

Perhaps 0.5 is too conservative a threshold for inclusion -- where do our manually included articles rank on the predicted relevance?

```{r compare to our manual screen results}

require(ggplot2)

# tabulate the number of included and excluded screens fall on either side of the 0.5 threshold
predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  group_by(include_screen) %>%
  summarise(nGreater0.5 = sum(0.5 <= relevance_mean), nLess0.5 = sum(relevance_mean < 0.5)) 



# violin plot
predRelMeta %>%
  filter(!is.na(include_screen)) %>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), labels = c("include","exclude"))) %>%
  ggplot(aes(x= sample_screen, y=relevance_mean, fill = sample_screen)) +
  facet_wrap(vars(factor(include_screen)))+
  geom_violin(trim=TRUE)+
  scale_y_continuous(limits = c(0,1))+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )


# Histogram so I can see the actual counts
relTable <- predRelMeta %>%
  filter(include_screen==1) %>%
  summarise(nGreater0.5 = sum(0.5 <= relevance_mean), nLess0.5 = sum(relevance_mean < 0.5)) %>%
  reshape2::melt(value.name = "sum") %>%
  mutate(relevance_mean = c(0.75, 0.25), count = 1250)

ggplot(predRelMeta %>% filter(include_screen==1), 
       aes(x= relevance_mean, fill = sample_screen, col = sample_screen)) +
  stat_density(alpha = 0.25, position = "identity", aes(y=after_stat(count)))+
  #geom_histogram(position = "stack")+
  scale_x_continuous(limits = c(0,1))+
  geom_vline(xintercept = 0.5, col="red")+
  geom_text(data=relTable, aes(x= relevance_mean, y=count, label = sum), inherit.aes = FALSE, col="red")+
  ggtitle("relevance of included articles")+
  theme_bw()


# predicted relevance by ordered article number 
predRelMeta %>% 
  filter(!is.na(include_screen))%>%
  mutate(include_screen = factor(include_screen, levels = c(1,0), 
                                 labels = c("include","exclude"))) %>%
  arrange(relevance_mean) %>% 
  mutate(id = row_number()) %>%
  ggplot(aes(id, relevance_mean))+ 
  geom_ribbon(aes(ymin = relevance_lower, ymax = relevance_upper), alpha = 0.5, fill="pink")+
  geom_point(size=1, alpha = 0.3)+
  geom_hline(yintercept = 0.5, col="darkgrey")+
  facet_wrap(vars(include_screen))+
  ggtitle("relevance predictions for screened abstracts")+
  theme_bw()

```


```{r explore which articles that should have been included are excluded}

# get vector of stopwords
all_stopwords <- unique(c(litsearchr::get_stopwords("English"), revtools::revwords())) 


## for articles that are include but low predicted relevance
lowRel <- predRelMeta %>% 
  filter(include_screen == 1 & relevance_mean < 0.5) %>%
  as.data.frame()
# fit topic model to title, and keep all other defaults. Then on data tab click "exit app" to save
lowRelTopic <- revtools::screen_topics(lowRel,remove_words = all_stopwords)

# plot
lowRelPlot <- plotly::plot_ly(lowRelTopic$plot_ready$x, x=~Axis1, y=~Axis2, z=~Axis3, color = ~factor(topic), type="scatter3d", text =~caption, hoverinfo="text", size=15) 
lowRelPlot 

lowRelTopic$plot_ready$topic %>% 
  mutate(caption = factor(
    caption, 
    levels = lowRelTopic$plot_ready$topic$caption[order(lowRelTopic$plot_ready$topic$topic)])) %>%
  ggplot(aes(x=caption, y=n, fill=caption))+
  geom_col()+
  ggtitle("Relevance < 0.5")+
  theme(axis.text.x = element_blank())


## for articles that are include but high predicted relevance
highRel <- predRelMeta %>% 
  filter(include_screen == 1 & 0.5 <= relevance_mean) %>%
  as.data.frame()
# fit topic model to title, and keep all other defaults. Then on data tab click "exit app" to save
highRelTopic <- revtools::screen_topics(highRel,remove_words = all_stopwords)

# plot
highRelTopic$plot_ready$x$caption <- highRelTopic$grouped$text

highRelPlot <- plotly::plot_ly(highRelTopic$plot_ready$x, x=~Axis1, y=~Axis2, z=~Axis3, color = ~factor(topic), type="scatter3d", text =~caption, hoverinfo="text", size=15) 
highRelPlot 

highRelTopic$plot_ready$topic %>% 
  mutate(caption = factor(
    caption, 
    levels = highRelTopic$plot_ready$topic$caption[order(highRelTopic$plot_ready$topic$topic)])) %>%
  ggplot(aes(x=caption, y=n, fill=caption))+
  geom_col()+
  ggtitle("Relevance >= 0.5")+
  theme(axis.text.x = element_blank())
```


```{r summaries of numbers of articles}

## Summaries

# the number of pred relevance > 0.5
predRelMeta %>%
  summarise(
    mean = sum(0.5 <= relevance_mean),
    lower = sum(0.5 <= relevance_lower),
    upper = sum(0.5 <= relevance_upper)
  )


# the distribution of relevance predictions -- if I change the threshold how will this change the number of inclusions?
# perhaps since all test list articles are > 0.8, maybe I want to use a threshold of 0.7 because 0.5 might still include a lot of crap?
counts <- predRelMeta %>% 
  summarise(
    GreaterThan_0.5 = sum(0.5 <= relevance_mean),
    GreaterThan_0.7 = sum(0.7 <= relevance_mean),
    GreaterThan_0.8 = sum(0.8 <= relevance_mean),
    GreaterThan_0.9 = sum(0.9 <= relevance_mean),
  )

counts <- as.data.frame(t(counts))
colnames(counts) <- "count"
counts$relevance_mean <- c(0.5,0.7,0.8,0.9)

predRelMeta %>%
  ggplot(aes(x=relevance_mean))+
  geom_histogram()+
  geom_vline(data=counts, aes(xintercept =relevance_mean))+
  geom_text(data=counts, aes(x = relevance_mean, label= count), y=50000, angle=60, hjust=0)+
  scale_x_continuous(breaks=c(0,0.5,0.7,0.8,0.9,1))


```



# Extracting occurrence of key terms from articles

## Process text

```{r source functions}
functionsToSource <- c("clean_string.R", "screen.R","tokenization.R","utils.R")
for(i in 1:length(functionsToSource)){
  source(here::here("R", functionsToSource[i]))
}

```


```{r get corresponding title and abstract info for predicted relevance greater than 0.5}
require(dbplyr)

## Get tables from databases
dedup_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite"), 
                                 create=FALSE)
src_dbi(dedup_db)
dedups <- tbl(dedup_db, "uniquerefs")

predRel_db <- dbConnect(SQLite(), dbname = here::here("data","raw-data","sql-databases","relevance-predictions.sqlite"),create = FALSE)
predRel <- tbl(predRel_db, "predRel")



## Filter and join

# subset to the records I want to join (pred rel => 0.5)
relRefs <- predRel %>%
  select(analysis_id, relevance_mean) %>%
  filter(0.5 <= relevance_mean) 

# use join to retreive metadata for those records
my_text <- left_join(relRefs, dedups %>% select(analysis_id,title, abstract), by = "analysis_id", copy=TRUE)
my_text <- my_text %>%
  collect()


## Disconnect databases
dbDisconnect(dedup_db)
dbDisconnect(predRel_db)


## Write to file
#save(my_text, file = here::here("data","raw-data","relevance-predictions","include_text.RData"))

```


I assembled a list of keywords grouped by different descriptive factors we would like to extract information about. We then scanned the article title, abstract and keywords and counted the occurrences of matches to these terms (boolean response of "yes" for at least one match/article).

```{r clean text}

## Load spreadsheet of keywords to extract and clean the text
nlp_search_terms <- read.csv(here::here("data","derived-data","coding","keyword-matches","keyword-search-tokens.csv"))
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling

# separate out into single terms and expressions
single_words <- nlp_search_terms$Term[which(nlp_search_terms[,4] == "single")]
expressions <- nlp_search_terms$Term[which(nlp_search_terms[,4] == "expression")]
# name them by their corresponding group
names(single_words) <- nlp_search_terms$`Group name`[which(nlp_search_terms[,4] == "single")]
names(expressions) <- nlp_search_terms$`Group name`[which(nlp_search_terms[,4] == "expression")]



## Process the text to screen

# group title, abstract together
nlptxt <- my_text %>%
  mutate(text = paste(title, abstract))%>%
  select(duplicate_id, text)

# Lemmitization and clean string to remove extra spaces, numbers and punctuation
nlptxt$text <- clean_string(nlptxt$text)
nlptxt$text <- textstem::lemmatize_strings(nlptxt$text)

```


## Screen for keywords

These keyword occurrences are extracted using the code ross_keyword_extract.R. 


## Visualizing keyword search results

identify numbers of papers relevant to each topic. The tokens to search for are stored in /data/derived-data/coding/keyword-matches/keyword-search-tokens.csv. 

```{r lookup table of groups for different keywords}
nlp_search_terms <- read.csv(here::here("data","derived-data","coding","keyword-matches","keyword-search-tokens.csv"))


## Process terms so that they match the column names in the sqlite database
colnames(nlp_search_terms) <- c("Group","Group name", "Term","Term type")
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling
nlp_search_terms$Term <- gsub(" ","_", nlp_search_terms$Term)


## Change names of some groupings

# change groupings of mitigation keywords
nlp_search_terms$`Group name`[
  which(nlp_search_terms$`Group name` %in% c("removal","CO2 removal from seawater",
                                   "OIF","blue carbon","bioenergy"))
] <- "carbon removal or storage"


## Names for each of the papers
topicNames <- c(
  "R&D vs implementation of mitigation OROs",
  "Biodiv & NCP outcomes",
  "Restoration practices",
  "Coastal community adaptation portfolio"
)

```

```{r connect to database with the keyword matches}
## Connect to database
keywordCon <- RSQLite::dbConnect(RSQLite::SQLite(), 
                                 dbname = here::here("data","derived-data","coding","keyword-matches","keyword-matches.sqlite"))
src_dbi(keywordCon)

keywordMatches <- tbl(keywordCon, "keywordMatches")

```

Now visualize these results 
```{r create an object to store all the plots}
ggp_list <- list()
```


For paper 1, I just want a barchart, and then the total number with any occurrence
```{r paper 1}
## Identify which paper
PaperNumber=1
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper 1
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  #head() %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
nrow(paper_df)  

# by keyword group (Group name) tabulate how many hits there were
paper_tab <- paper_df %>%
  group_by(`Group name`) %>%
  summarise(n=n_distinct(analysis_id))

# how many articles retreived any hit
paper_total1 <- length(unique(paper_df$analysis_id))


## Display 

# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000


# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- paper_tab %>%
  mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = ymax+yoffset, label = paste("total papers = ", paper_total1)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]


```


For paper 2, they want to know papers that have biodiv, ES, or both
```{r paper 2}
## Identify which paper
PaperNumber=2
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
  

# # how man articles have either or both biodiv or ncp
counts = with(paper_df, table(tapply(`Group name`, analysis_id, function(x) paste(as.character(sort(unique(x))), collapse=' & '))))
counts = as.data.frame(t(counts))
colnames(counts) <- c("Topic", "Group name","n")
counts$Topic <- paste("Paper", PaperNumber)


# # by keyword group (Group name) tabulate how many hits there were
# paper_tab <- paper_df %>%
#   group_by(`Group name`) %>%
#   summarise(n=n_distinct(analysis_id))


# how many articles retreived any hit
paper_total <- length(unique(paper_df$analysis_id))



# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000

# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- counts %>%
  #mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = n, label=paste(n),group = `Group name`), position = position_stack(vjust=0.5))+
    #geom_text(aes(x=paste("Paper", PaperNumber), y = ymax+yoffset, label = paste("total papers = ", paper_total)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    #ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]

```


For paper 3, do the same as for paper 2
```{r paper 3}
## Identify which paper
PaperNumber=3
Paper = paste0("Paper", PaperNumber)
tempTitle <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)
  
  

# # how man articles have either or both biodiv or ncp
counts = with(paper_df, table(tapply(`Group name`, analysis_id, function(x) paste(as.character(sort(unique(x))), collapse=' & '))))
counts = as.data.frame(t(counts))
colnames(counts) <- c("Topic", "Group name","n")
counts$Topic <- paste("Paper", PaperNumber)


# # by keyword group (Group name) tabulate how many hits there were
# paper_tab <- paper_df %>%
#   group_by(`Group name`) %>%
#   summarise(n=n_distinct(analysis_id))


# how many articles retreived any hit
paper_total <- length(unique(paper_df$analysis_id))



# plot parameters
yaxis_title <- "N matches"
ymax = sum(paper_tab$n, na.rm=T)   
yoffset = 5000

# plot
require(ggplot2)

ggp_list[[PaperNumber]] <- counts %>%
  #mutate(Topic = paste("Paper", PaperNumber)) %>% 
  ggplot(aes(x=Topic, y=n, fill = `Group name`)) + 
    geom_col()+
    geom_text(aes(y = n, label=paste(n),group = `Group name`), position = position_stack(vjust=0.5))+
    #geom_text(aes(x=paste("Paper", PaperNumber), y = ymax+yoffset, label = paste("total papers = ", paper_total)))+
    labs(
      title = paste(strwrap(tempTitle, width = 40), collapse="\n"),
      y=yaxis_title
    )+
    #ylim(0, ymax+yoffset)+
    scale_fill_brewer(type="qual", palette = PaperNumber)+
    theme(
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
  
ggp_list[[PaperNumber]]

```


```{r paper 4}

## Identify which paper
PaperNumber=4
Paper = paste0("Paper", PaperNumber)
tempTitle1 <- paste0(letters[PaperNumber],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")
tempTitle2 <- paste0(letters[PaperNumber+1],". Paper ", PaperNumber, ": ",topicNames[PaperNumber], collapse="")



## Calculate tabulation summaries

paper_df <- keywordMatches %>%
  # select only columns pertaining to paper 
  select(analysis_id, nlp_search_terms$Term[which(nlp_search_terms$Group == Paper)]) %>%
  #head() %>%
  collect() %>%
  # format into long form with columns for id, Term and value (y/n)
  reshape2::melt(id.vars = "analysis_id", variable.name = "Term") %>%
  filter(value > 0) %>%
  # merge with grouping information for each term
  merge(nlp_search_terms, by="Term", all = FALSE)

# how many articles retreived any hit
paper_total4 <- length(unique(paper_df$analysis_id))


nrow(paper_df)


## Format to visualize as a heatmap
library(reshape2)
paper_tab <- paper_df[!duplicated(paper_df[,c("analysis_id","Group name")]),]


paper_tab_Intervention <- paper_tab[grep("Intervention",paper_tab$`Group name`),c("analysis_id","Group name")]
colnames(paper_tab_Intervention)[2] <- c("Intervention")
paper_tab_Intervention$Intervention <- gsub("Intervention: ","",paper_tab_Intervention$Intervention)

paper_tab_Population <- paper_tab[grep("Population",paper_tab$`Group name`),c("analysis_id","Group name")]
colnames(paper_tab_Population)[2] <- c("Population")
paper_tab_Population$Population <- gsub("Population: ","",paper_tab_Population$Population)

paper_tab_Threat <- paper_tab[grep("threat",paper_tab$`Group name`),c("analysis_id","Term")]
colnames(paper_tab_Threat)[2] <- c("Threat")


paperMerge <- merge(paper_tab_Intervention, paper_tab_Population, by="analysis_id")
paperMerge <- merge(paperMerge, paper_tab_Threat, by="analysis_id")


## Display
library(stringr)

# Visualize Population by intervention
ggp_list[[PaperNumber]] <- paperMerge %>%
  group_by(Intervention, Population) %>%
  summarise(n=n()) %>%
  ggplot(aes(Population, Intervention, fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()+
  scale_y_discrete(labels = function(x) str_wrap(x, width=10))+
  geom_text(aes(label = n), col="red")+
  labs(title = paste(strwrap(tempTitle1, width = 40), collapse="\n"),
       caption = paste("total articles = ", paper_total4))+
  theme(
      legend.position = "na",
      legend.title = element_blank(),
      #axis.text.x = element_blank(),
      #axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )


# Visualise threat by intervention
ggp_list[[PaperNumber+1]] <- paperMerge %>%
  group_by(Intervention, Threat) %>%
  summarise(n=n()) %>%
  ggplot(aes(Threat, Intervention, fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()+
  scale_y_discrete(labels = function(x) str_wrap(x, width=10))+
  geom_text(aes(label = n), col="red")+
  labs(title = paste(strwrap(tempTitle2, width = 40), collapse="\n"),
       caption = paste("total articles = ", paper_total4))+
  theme(
      legend.position = "na",
      legend.title = element_blank(),
      axis.text.x = element_text(angle=45,hjust=1),
      #axis.title.x = element_blank(),
      panel.background = element_rect(fill = "transparent"),
      panel.grid.major = element_line(colour = "grey"),
      panel.border = element_rect(colour = "black", fill="transparent")
    )
ggp_list[[PaperNumber+1]]
```

```{r create all plots together}

pdf(file = here::here("data","derived-data","coding","keyword-matches","keyword-matches-plots.pdf"),
    width = 10, height=15)
egg::ggarrange(
  plots = ggp_list,
  nrow=3,
  newpage=FALSE
)
dev.off()

```


```{r close databset connection}
## Close connection
RSQLite::dbDisconnect(keywordCon)

```



```{r clean environment}
rm(list=ls())
```


## Export metadata for review by teams

```{r source functions}
functionsToSource <- c("clean_string.R", "screen.R","tokenization.R","utils.R")
for(i in 1:length(functionsToSource)){
  source(here::here("R", functionsToSource[i]))
}

```


```{r lookup table of groups for different keywords}
nlp_search_terms <- read.csv(here::here("data","derived-data","coding","keyword-matches","keyword-search-tokens.csv"))


## Process terms so that they match the column names in the sqlite database
colnames(nlp_search_terms) <- c("Group","Group name", "Term","Term type")
nlp_search_terms <- na.omit(nlp_search_terms,nlp_search_terms) # remove empty spaces
nlp_search_terms$Term <- textstem::lemmatize_strings(nlp_search_terms$Term) # lemmitize
nlp_search_terms$Term <- clean_string(nlp_search_terms$Term) # remove punctuation and extra spaces
nlp_search_terms <- nlp_search_terms[!duplicated(nlp_search_terms$Term),] # remove any resulting duplicates
nlp_search_terms$Term <- uk2us::convert_uk2us(nlp_search_terms$Term) # transform everything to American spelling
nlp_search_terms$Term <- gsub(" ","_", nlp_search_terms$Term)


## Change names of some groupings

# change groupings of mitigation keywords
nlp_search_terms$`Group name`[
  which(nlp_search_terms$`Group name` %in% c("carbon","removal","CO2 removal from seawater",
                                   "OIF","blue carbon","bioenergy"))
] <- "carbon removal or storage"


## Names for each of the papers
topicNames <- c(
  "R&D vs implementation of mitigation OROs",
  "Biodiv & NCP outcomes",
  "Restoration practices",
  "Coastal community adaptation portfolio"
)

```

```{r connect to database with the keyword matches}
## Connect to database
keywordCon <- RSQLite::dbConnect(RSQLite::SQLite(), 
                                 dbname = here::here("data","derived-data","coding","keyword-matches","keyword-matches.sqlite"))
src_dbi(keywordCon)

keywordMatches <- tbl(keywordCon, "keywordMatches")
keywordMatches <- keywordMatches %>% collect

dbDisconnect(keywordCon)
```

```{r get associated metadata for the matches}

## Get tables from databases
dedup_db <- dbConnect(RSQLite::SQLite(),
                      here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite"), 
                                 create=FALSE)
src_dbi(dedup_db)
dedups <- tbl(dedup_db, "uniquerefs")



metadata <- dedups %>% 
  select(analysis_id, author, title, source_title, year, abstract, doi) %>%
  collect()

dbDisconnect(dedup_db)
```

```{r join metadata to keyword matches}

keywordMatchesMerge <- merge(keywordMatches, metadata, by="analysis_id", all.x = TRUE, all.y=FALSE)

# reorder columns so keyword matches come last
cols <- colnames(keywordMatchesMerge)
cols <- cols[-which(cols %in% c("analysis_id", "author","title","source_title","year","abstract","doi"))]
keywordMatchesMerge <- keywordMatchesMerge[,c("analysis_id", "author","title","source_title","year","abstract","doi", cols)]
  
```



```{r For each paper filter to relevant articles and export as a csv}
## Inputs
out_dir <- here::here("data","derived-data","coding","keyword-matches","keyword-matches-and-metadata-csv")


papers <- unique(nlp_search_terms$Group)

# columns that contain the metadata variables -- the keep
metadata_cols <- c("analysis_id", "author","title","source_title","year","abstract","doi")




for(p in 1:length(papers)){
  
  # columns that contain the keywords relevant to that paper
  paper_cols <- nlp_search_terms$Term[which(nlp_search_terms$Group == papers[p])]
  
  # subset the dataframe to only the metadata columns and the keywords relevant to the paper
  tempDf <- keywordMatchesMerge[,c(metadata_cols, paper_cols)]
  
  
  # compress all keyword columns into one column containing all the matches in one string
  
  # replace binary indicator with keyword name
  for(j in 1:length(paper_cols)){
    tempDf[,paper_cols[j]] <- ifelse(tempDf[,paper_cols[j]] > 0, paper_cols[j], NA)
  }
  
  # make one column that joins all the keywords in that row
  matches <- apply(tempDf[,paper_cols], 1, function(x){
    x <- x[!is.na(x)]
    if(length(x)==0){
      x <- NA
    }else{
      x <- paste(x, collapse = ", ")
    }
    return(x)
  })
  
  tempDf <- cbind(tempDf[,metadata_cols], data.frame(keyword_matches = matches))
  tempDf <- tempDf %>%
    filter(!is.na(abstract), !is.na(keyword_matches))
  
  
  ## If it is paper 2, make another column if there is a doi match to Galparoso et al
  if(papers[p] == "Paper2"){
    # read in
    galRef <- revtools::read_bibliography(here::here("data","raw-data","citation-chasing","Galparsoro_etal_2002_references.ris"))
    galRef <- subset(galRef, !is.na(doi))
    galmatch <- tempDf$doi %in% galRef$doi 
    ngalmatch <- sum(galmatch)
    
    tempDf <- cbind(tempDf, galmatch)
    
    colnames(tempDf)[ncol(tempDf)] <- paste0("Galparsoro_match_total_",ngalmatch)
  }

  
  ## Write to csv
  write.csv(tempDf, file = file.path(out_dir, paste(papers[p],"keyword-matches.csv", sep="-")))
}

```





# Geoparsing text


Repeat the set up chunk from the beginning of the doc to load files
```{r set up dataset}
## inputs (need to change)
sqlite.fp <- here::here("data","raw-data","sql-databases", "unique-refs_v2.sqlite")
tbl.name <- "uniquerefs"


## Get records from database
dbcon <- dbConnect(RSQLite::SQLite(), sqlite.fp, create = FALSE)
recs <- tbl(dbcon, tbl.name)


## perhaps just to on a subsample
my_text <- recs %>%
  select(analysis_id, duplicate_id, title, abstract) %>%
  slice_sample(n=100) %>%
  collect()

dim(my_text)
head(my_text)

## save
write.csv(my_text,here::here("data","derived-data","coding","nlp-extracted","geocode-text.csv"))

## Clost database connection
DBI::dbDisconnect(dbcon)
```

```{r combine text for geoparsing}
my_text <- read.csv(here::here("data","derived-data","coding","nlp-extracted","geocode-text.csv"))

vec2geocode <- paste(my_text$title, my_text$abstract)
vec2geocode_duplicate_id <- my_text$duplicate_id

rm(my_text)

```



```{r clean text and remove stopwords}
#vec2geocode <- c('I went to Ottawa and then London.','Pacific northwest')


# # clean 
# vec2geocode <- clean_string(vec2geocode[1:2])
# 
# # remove stopwords
# 
# # identify stopwords
# my_stopwords <- unique(c(quanteda::stopwords(),revtools::revwords(), litsearchr::get_stopwords(), "sea", "ocean", "marine", "coast", "south", "part"))
# 
# # remove
# for(i in 1:length(vec2geocode)){
#   vec2geocode[i] <- paste(quanteda::tokens_remove(quanteda::tokens(vec2geocode[i]),
#                                                   my_stopwords),collapse = " ")
# }


```



```{r}
use_virtualenv(here::here("spaCy_env"))
#use_virtualenv(here::here("localGeocode_env"))
#use_python("C:\\Users\\deviv\\AppData\\Local\\Programs\\Python\\PYTHON~1\\python.exe")
```

Use spacy to extract components of sentences that contain locations.

Types of entity labels available:
PERSON:      People, including fictional.
NORP:        Nationalities or religious or political groups.
FAC:         Buildings, airports, highways, bridges, etc.
ORG:         Companies, agencies, institutions, etc.
GPE:         Countries, cities, states.
LOC:         Non-GPE locations, mountain ranges, bodies of water.
PRODUCT:     Objects, vehicles, foods, etc. (Not services.)
EVENT:       Named hurricanes, battles, wars, sports events, etc.
WORK_OF_ART: Titles of books, songs, etc.
LAW:         Named documents made into laws.
LANGUAGE:    Any named language.
DATE:        Absolute or relative dates or periods.
TIME:        Times smaller than a day.
PERCENT:     Percentage, including %.
MONEY:       Monetary values, including unit.
QUANTITY:    Measurements, as of weight or distance.
ORDINAL:     first, second, etc.
CARDINAL:    Numerals that do not fall under another type.

attributes can be displayed using dir(doc)

First extract the entities from the text. If GPE or LOC, store seperatelyfrom dates
```{python}

import spacy
nlp = spacy.load("en_core_web_sm")


Entarr = [] ;
Datearr = [] ;


for i in range(len(r.vec2geocode)):
  doc = nlp(r.vec2geocode[i])
  
  for entity in doc.ents:
    
    if entity.label_ in ('GPE' 'LOC'):
      ents = [r.vec2geocode_duplicate_id[i], entity.label_, entity.text]
      Entarr.append(ents)
      
    if entity.label_ == 'DATE':
      ents = [r.vec2geocode_duplicate_id[i], entity.label_, entity.text]
      Datearr.append(ents)


print(Entarr[0])
print(Datearr[0])



import pickle
import os

pickle.dump(Entarr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "wb"))
pickle.dump(Datearr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Dates.pickle"]), "wb"))

del Datearr
del Entarr


```

note there will likely be duplicates in place names within a duplicate_id, so make sure to clean afterwards. Columns are: duplicate_id, entity_label, entity_text


call the output from location tagging in spacy directly to retreive metadata about the location
```{python}
from geocode.geocode import Geocode
gc = Geocode()
gc.load()

import numpy as np


Entarr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "rb"))
Entarr_numpy = np.array(Entarr)

Geocodearr = [] ;

for i in range(len(Entarr)):
  location = gc.decode(Entarr_numpy[i,2])
  Geocodearr.append(location)
  

pickle.dump(Geocodearr, file= open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Geocode-Metadata.pickle"]), "wb"))



del Geocodearr
del Entarr


```

Note also the option of gc.decode_parallel in https://github.com/mar-muel/local-geocode



Map the distribution of locations mentioned

```{python}
import pickle
import os

Entarr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"Entities.pickle"]), "rb"))
Geocodearr = pickle.load(open("\\".join([os.getcwd(),'data','derived-data','coding','nlp-extracted',"geocode-metadata.pickle"]), "rb"))

quit
```

Use r to format data frame
```{r}
require(tidyverse)

# add the duplicate id to each entry
geoarr <- list()

for(i in 1:length(py$Entarr)){
  # append each element of the list with the corresponding duplicate id -- sometimes multiple entries for each entity because returns
  # multiple matches
  tmp <- lapply(py$Geocodearr[[i]], function(x) append(x, py$Entarr[[i]]))
  # Add names to these new columns
  tmp <- lapply(tmp, function(x) {names(x)[9:11] <- c("duplicate_id","entity_label","entity_text"); x})
  geoarr[[i]] <- tmp
}

locationsDf <- flatten(geoarr)
rm(geoarr)

locationsDf <- lapply(locationsDf, function(x) as.data.frame(x))

locationsDf <- do.call(rbind.data.frame, locationsDf)


# remove any duplicated identifications within each article
# sometimes there will also be multiple values for an entity label -- in this case take the first? ****
locationsDf <- locationsDf %>%
  distinct(geoname_id, duplicate_id, entity_text, .keep_all=TRUE) 


## Format the data frame
locationsDf <- locationsDf %>%
  mutate(
    entity_label = factor(entity_label, levels=c("LOC","GPE"))
  )

## BUT geolocating only really works on GPE, not LOC types, so maybe seperate as several columns in the metadata
```


Work on coding the location types
```{r}
library(mregions)


# get records
types <- mr_place_types()

mrec <- mr_records_by_type(type = types$type[1])



# identify which records correspond to each entry
mreg_text <- locationsDf %>%
  filter(entity_label == "LOC")

mreg_code <- mregions::mr_geo_code(mreg_text$entity_text[2], fuzzy=TRUE)
mreg_code <- mreg_code[1,]


# get the area of the bounding box

library(geosphere)

xlim <- c(mreg_code$minLongitude, mreg_code$maxLongitude)
ylim <- c(mreg_code$minLatitude, mreg_code$maxLatitude)
l.out = 50

bb = rbind(cbind(xlim[1], seq(ylim[1],ylim[2],length.out = l.out)),
            cbind(seq(xlim[1],xlim[2],length.out = l.out),ylim[2]),
            cbind(xlim[2],seq(ylim[2],ylim[1],length.out = l.out)),
            cbind(seq(xlim[2],xlim[1],length.out = l.out),ylim[1]))


area <- geosphere::areaPolygon(bb)

area # in m2


# extract shp file?





save(locationsDf, file = here::here("data","derived-data","coding","nlp-extracted","geocode-metadata.RData"))
```

```{r}
library(maps)

load(here::here("data","derived-data","coding","nlp-extracted","geocode-metadata.RData"))

world_map <- map_data("world")

# # add the sizes of the different location_types
# location_size_lookup <- data.frame(
#   location_type = c('city', 'place', 'country', 'admin1', 'admin2', 'admin3', 'admin4', 'admin5', 'admin6', 'admin_other', 'continent', 'region'),
#   area  = c()
# )

ggplot()+
  geom_polygon(data = world_map, aes(x=long, y=lat, group=group), fill="grey")+

  geom_point(data=locationsDf %>% filter(entity_label == "GPE") %>% arrange(population), 
             aes(longitude, latitude, size = log(population), color = log(population)), alpha=0.3)+
  
  coord_sf()+
  scale_size_continuous()+
  scale_color_viridis_c(option = "magma")+
  theme_void()+
  guides(color = guide_legend())+
  theme(
    legend.position = c(0.1, 0.5),
      plot.background = element_rect(fill = "#f5f5f2", color = NA), 
      panel.background = element_rect(fill = "#f5f5f2", color = NA), 
      legend.background = element_rect(fill = "#f5f5f2", color = NA)
  )
  

  
```


# Junk code

Unfortunately couldn't get spacyr to work -- got stuck on spcay_install()

using: https://spacyr.quanteda.io/index.html
```{r install spaCy, eval=FALSE}
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)

library("spacyr")
spacy_install()

```


```{r use spacy to parse text and identify nouns}

library("spacyr")
spacy_initialize(model = "en_core_web_sm")

entities <- spacy_extract_entity(vec2geocode, output = "data.frame", type = "GPE")

entities

# finish spacy session
spacy_finalize()
```

